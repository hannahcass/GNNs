{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv, BatchNorm\n",
    "from torch_geometric.data import Data\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.load('data/processed_data/graph_data.pt').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = data.num_features\n",
    "output_dim = len(torch.unique(data.y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = data.num_nodes\n",
    "num_train = int(num_nodes * 0.8)\n",
    "num_val = int(num_nodes * 0.1)\n",
    "num_test = num_nodes - (num_train + num_val)\n",
    "\n",
    "indices = torch.randperm(num_nodes)\n",
    "\n",
    "train_mask = indices[:num_train]\n",
    "val_mask = indices[num_train:num_train + num_val]\n",
    "test_mask = indices[num_train + num_val:]\n",
    "\n",
    "data.train_mask = torch.zeros(num_nodes, dtype=torch.bool).scatter_(0, train_mask, True)\n",
    "data.val_mask = torch.zeros(num_nodes, dtype=torch.bool).scatter_(0, val_mask, True)\n",
    "data.test_mask = torch.zeros(num_nodes, dtype=torch.bool).scatter_(0, test_mask, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "unique_labels = data.y.unique()\n",
    "print(unique_labels)  \n",
    "output_dim = len(unique_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.x = (data.x - data.x.mean(dim=0)) / data.x.std(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(297746) tensor(37218) tensor(37219)\n",
      "tensor(0, device='cuda:0') tensor(8, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(data.train_mask.sum(), data.val_mask.sum(), data.test_mask.sum())\n",
    "print(data.y.min(), data.y.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert data.y.min() >= 0 and data.y.max() < output_dim\n",
    "assert torch.isfinite(data.x).all()\n",
    "assert torch.isfinite(data.y).all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[372183, 102], edge_index=[2, 1199750], y=[372183], train_mask=[372183], val_mask=[372183], test_mask=[372183])\n",
      "torch.Size([2, 1199750])\n",
      "torch.Size([372183, 102])\n",
      "torch.Size([372183])\n"
     ]
    }
   ],
   "source": [
    "print(data)\n",
    "print(data.edge_index.size())\n",
    "print(data.x.size())\n",
    "print(data.y.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: 1 instances\n",
      "Class 1: 273001 instances\n",
      "Class 2: 17918 instances\n",
      "Class 3: 63084 instances\n",
      "Class 4: 3 instances\n",
      "Class 5: 1 instances\n",
      "Class 6: 7 instances\n",
      "Class 7: 18167 instances\n",
      "Class 8: 1 instances\n"
     ]
    }
   ],
   "source": [
    "for i in range(output_dim):\n",
    "    print(f\"Class {i}: {(data.y == i).sum().item()} instances\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim, dropout_rate):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim1)\n",
    "        self.bn1 = BatchNorm(hidden_dim1)\n",
    "        self.conv2 = GCNConv(hidden_dim1, hidden_dim2)\n",
    "        self.bn2 = BatchNorm(hidden_dim2)\n",
    "        self.conv3 = GCNConv(hidden_dim2, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=dropout_rate, training=self.training)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=dropout_rate, training=self.training)\n",
    "\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim1 = 32\n",
    "hidden_dim2 = 16\n",
    "\n",
    "dropout_rate = 0.5\n",
    "lr=0.1\n",
    "weight_decay=5e-3\n",
    "step_size=50\n",
    "gamma=0.5\n",
    "\n",
    "\n",
    "class_weights = torch.tensor([1, 0.1, 10, 5, 20, 30, 15, 10, 30], device=device)\n",
    "\n",
    "def weights_init(m):\n",
    "    if hasattr(m, 'weight') and m.weight is not None:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "    if hasattr(m, 'bias') and m.bias is not None:\n",
    "        torch.nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN(input_dim=input_dim, hidden_dim1=hidden_dim1, hidden_dim2=hidden_dim2, output_dim=output_dim, dropout_rate=dropout_rate).to(device)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    if hasattr(m, 'weight') and m.weight is not None:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "    if hasattr(m, 'bias') and m.bias is not None:\n",
    "        torch.nn.init.zeros_(m.bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training & Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    \n",
    "    loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "    loss = loss_fn(out[data.train_mask], data.y[data.train_mask])\n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            print(f\"{name}, gradient norm: {param.grad.norm().item()}\")\n",
    "        else:\n",
    "            print(f\"{name} has no gradient\")\n",
    "\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    logits = model(data)\n",
    "    loss_fn = torch.nn.NLLLoss()\n",
    "    \n",
    "    accs = []\n",
    "    losses = []\n",
    "    \n",
    "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
    "        mask_logits = logits[mask]\n",
    "        mask_labels = data.y[mask]\n",
    "        loss = loss_fn(mask_logits, mask_labels)\n",
    "        pred = mask_logits.max(1)[1]\n",
    "        acc = pred.eq(mask_labels).sum().item() / mask.sum().item()\n",
    "        accs.append(acc)\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    return accs[0], accs[1], accs[2], losses[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias, gradient norm: 8.232212245218307e-09\n",
      "conv1.lin.weight, gradient norm: 2.318267822265625\n",
      "bn1.module.weight, gradient norm: 0.21683312952518463\n",
      "bn1.module.bias, gradient norm: 0.1841115802526474\n",
      "conv2.bias, gradient norm: 9.956223578910794e-08\n",
      "conv2.lin.weight, gradient norm: 1.1698213815689087\n",
      "bn2.module.weight, gradient norm: 0.5124688744544983\n",
      "bn2.module.bias, gradient norm: 0.6276944279670715\n",
      "conv3.bias, gradient norm: 0.5349704623222351\n",
      "conv3.lin.weight, gradient norm: 1.1349483728408813\n",
      "Epoch: 1, Training Loss: 3.2093, Validation Loss: 2.4981, Train Acc: 0.1603, Val Acc: 0.1625, Test Acc: 0.1578\n",
      "conv1.bias, gradient norm: 3.0504065939851444e-09\n",
      "conv1.lin.weight, gradient norm: 0.37072664499282837\n",
      "bn1.module.weight, gradient norm: 0.14312463998794556\n",
      "bn1.module.bias, gradient norm: 0.14479804039001465\n",
      "conv2.bias, gradient norm: 2.9778274068803512e-08\n",
      "conv2.lin.weight, gradient norm: 0.4813706576824188\n",
      "bn2.module.weight, gradient norm: 0.13805237412452698\n",
      "bn2.module.bias, gradient norm: 0.19090233743190765\n",
      "conv3.bias, gradient norm: 0.38856080174446106\n",
      "conv3.lin.weight, gradient norm: 0.5945250988006592\n",
      "Epoch: 2, Training Loss: 1.5788, Validation Loss: 3.5694, Train Acc: 0.2581, Val Acc: 0.2587, Test Acc: 0.2591\n",
      "conv1.bias, gradient norm: 5.953567083771816e-10\n",
      "conv1.lin.weight, gradient norm: 0.1324770301580429\n",
      "bn1.module.weight, gradient norm: 0.04979377239942551\n",
      "bn1.module.bias, gradient norm: 0.05141748860478401\n",
      "conv2.bias, gradient norm: 8.858438427239435e-09\n",
      "conv2.lin.weight, gradient norm: 0.23228909075260162\n",
      "bn2.module.weight, gradient norm: 0.12600164115428925\n",
      "bn2.module.bias, gradient norm: 0.1250070482492447\n",
      "conv3.bias, gradient norm: 0.23769469559192657\n",
      "conv3.lin.weight, gradient norm: 0.592051088809967\n",
      "Epoch: 3, Training Loss: 1.0391, Validation Loss: 3.3857, Train Acc: 0.2271, Val Acc: 0.2297, Test Acc: 0.2282\n",
      "conv1.bias, gradient norm: 4.094635464024776e-10\n",
      "conv1.lin.weight, gradient norm: 0.07991346716880798\n",
      "bn1.module.weight, gradient norm: 0.03438320383429527\n",
      "bn1.module.bias, gradient norm: 0.037509214133024216\n",
      "conv2.bias, gradient norm: 4.490761984499159e-09\n",
      "conv2.lin.weight, gradient norm: 0.12770184874534607\n",
      "bn2.module.weight, gradient norm: 0.10297942161560059\n",
      "bn2.module.bias, gradient norm: 0.08801157027482986\n",
      "conv3.bias, gradient norm: 0.12238571047782898\n",
      "conv3.lin.weight, gradient norm: 0.28262171149253845\n",
      "Epoch: 4, Training Loss: 0.7811, Validation Loss: 2.6652, Train Acc: 0.2179, Val Acc: 0.2216, Test Acc: 0.2193\n",
      "conv1.bias, gradient norm: 4.0991487981756336e-10\n",
      "conv1.lin.weight, gradient norm: 0.05700673907995224\n",
      "bn1.module.weight, gradient norm: 0.030977288261055946\n",
      "bn1.module.bias, gradient norm: 0.03457443788647652\n",
      "conv2.bias, gradient norm: 2.5300597172872585e-09\n",
      "conv2.lin.weight, gradient norm: 0.09576808661222458\n",
      "bn2.module.weight, gradient norm: 0.07465644180774689\n",
      "bn2.module.bias, gradient norm: 0.061750732362270355\n",
      "conv3.bias, gradient norm: 0.07933788001537323\n",
      "conv3.lin.weight, gradient norm: 0.17954915761947632\n",
      "Epoch: 5, Training Loss: 0.6588, Validation Loss: 2.1135, Train Acc: 0.2174, Val Acc: 0.2206, Test Acc: 0.2193\n",
      "conv1.bias, gradient norm: 2.716197267105258e-10\n",
      "conv1.lin.weight, gradient norm: 0.04375659301877022\n",
      "bn1.module.weight, gradient norm: 0.033300988376140594\n",
      "bn1.module.bias, gradient norm: 0.03320290893316269\n",
      "conv2.bias, gradient norm: 2.9199722639816628e-09\n",
      "conv2.lin.weight, gradient norm: 0.09131383895874023\n",
      "bn2.module.weight, gradient norm: 0.07333080470561981\n",
      "bn2.module.bias, gradient norm: 0.06379730999469757\n",
      "conv3.bias, gradient norm: 0.06552740931510925\n",
      "conv3.lin.weight, gradient norm: 0.16308024525642395\n",
      "Epoch: 6, Training Loss: 0.5898, Validation Loss: 1.8980, Train Acc: 0.2182, Val Acc: 0.2211, Test Acc: 0.2199\n",
      "conv1.bias, gradient norm: 2.756423700400745e-10\n",
      "conv1.lin.weight, gradient norm: 0.036217886954545975\n",
      "bn1.module.weight, gradient norm: 0.03270517289638519\n",
      "bn1.module.bias, gradient norm: 0.028384100645780563\n",
      "conv2.bias, gradient norm: 1.9599730727293263e-09\n",
      "conv2.lin.weight, gradient norm: 0.07513999193906784\n",
      "bn2.module.weight, gradient norm: 0.058853454887866974\n",
      "bn2.module.bias, gradient norm: 0.059240348637104034\n",
      "conv3.bias, gradient norm: 0.07268006354570389\n",
      "conv3.lin.weight, gradient norm: 0.12289764732122421\n",
      "Epoch: 7, Training Loss: 0.5417, Validation Loss: 1.8976, Train Acc: 0.2205, Val Acc: 0.2239, Test Acc: 0.2221\n",
      "conv1.bias, gradient norm: 1.9180031174403922e-10\n",
      "conv1.lin.weight, gradient norm: 0.0323781780898571\n",
      "bn1.module.weight, gradient norm: 0.0335363894701004\n",
      "bn1.module.bias, gradient norm: 0.025926606729626656\n",
      "conv2.bias, gradient norm: 2.191600456669107e-09\n",
      "conv2.lin.weight, gradient norm: 0.06377646327018738\n",
      "bn2.module.weight, gradient norm: 0.046485040336847305\n",
      "bn2.module.bias, gradient norm: 0.054989445954561234\n",
      "conv3.bias, gradient norm: 0.07135722786188126\n",
      "conv3.lin.weight, gradient norm: 0.10408883541822433\n",
      "Epoch: 8, Training Loss: 0.4985, Validation Loss: 2.0278, Train Acc: 0.2278, Val Acc: 0.2312, Test Acc: 0.2296\n",
      "conv1.bias, gradient norm: 1.833066198830835e-10\n",
      "conv1.lin.weight, gradient norm: 0.02605685591697693\n",
      "bn1.module.weight, gradient norm: 0.03239867836236954\n",
      "bn1.module.bias, gradient norm: 0.025395160540938377\n",
      "conv2.bias, gradient norm: 2.3541064653187505e-09\n",
      "conv2.lin.weight, gradient norm: 0.05733690783381462\n",
      "bn2.module.weight, gradient norm: 0.044632188975811005\n",
      "bn2.module.bias, gradient norm: 0.05456388369202614\n",
      "conv3.bias, gradient norm: 0.05210107937455177\n",
      "conv3.lin.weight, gradient norm: 0.13397552073001862\n",
      "Epoch: 9, Training Loss: 0.4646, Validation Loss: 2.1919, Train Acc: 0.2286, Val Acc: 0.2317, Test Acc: 0.2302\n",
      "conv1.bias, gradient norm: 3.4312583330198265e-10\n",
      "conv1.lin.weight, gradient norm: 0.025337031111121178\n",
      "bn1.module.weight, gradient norm: 0.029649559408426285\n",
      "bn1.module.bias, gradient norm: 0.021972719579935074\n",
      "conv2.bias, gradient norm: 2.1848702846938295e-09\n",
      "conv2.lin.weight, gradient norm: 0.050359368324279785\n",
      "bn2.module.weight, gradient norm: 0.03627369925379753\n",
      "bn2.module.bias, gradient norm: 0.04794248938560486\n",
      "conv3.bias, gradient norm: 0.057809848338365555\n",
      "conv3.lin.weight, gradient norm: 0.06947138160467148\n",
      "Epoch: 10, Training Loss: 0.4292, Validation Loss: 2.2897, Train Acc: 0.2351, Val Acc: 0.2390, Test Acc: 0.2365\n",
      "conv1.bias, gradient norm: 1.6510129374758264e-10\n",
      "conv1.lin.weight, gradient norm: 0.019232887774705887\n",
      "bn1.module.weight, gradient norm: 0.02484980784356594\n",
      "bn1.module.bias, gradient norm: 0.016814444214105606\n",
      "conv2.bias, gradient norm: 1.6824881488020083e-09\n",
      "conv2.lin.weight, gradient norm: 0.03878471255302429\n",
      "bn2.module.weight, gradient norm: 0.02936258539557457\n",
      "bn2.module.bias, gradient norm: 0.03825094923377037\n",
      "conv3.bias, gradient norm: 0.049078069627285004\n",
      "conv3.lin.weight, gradient norm: 0.05805733799934387\n",
      "Epoch: 11, Training Loss: 0.4057, Validation Loss: 2.3111, Train Acc: 0.2499, Val Acc: 0.2545, Test Acc: 0.2508\n",
      "conv1.bias, gradient norm: 1.32266711472262e-10\n",
      "conv1.lin.weight, gradient norm: 0.015265942551195621\n",
      "bn1.module.weight, gradient norm: 0.018138516694307327\n",
      "bn1.module.bias, gradient norm: 0.014726735651493073\n",
      "conv2.bias, gradient norm: 1.2342964428313508e-09\n",
      "conv2.lin.weight, gradient norm: 0.031085791066288948\n",
      "bn2.module.weight, gradient norm: 0.02623877115547657\n",
      "bn2.module.bias, gradient norm: 0.03200695291161537\n",
      "conv3.bias, gradient norm: 0.036726631224155426\n",
      "conv3.lin.weight, gradient norm: 0.05340723693370819\n",
      "Epoch: 12, Training Loss: 0.3840, Validation Loss: 2.2739, Train Acc: 0.2544, Val Acc: 0.2590, Test Acc: 0.2558\n",
      "conv1.bias, gradient norm: 1.3913797891618174e-10\n",
      "conv1.lin.weight, gradient norm: 0.015610075555741787\n",
      "bn1.module.weight, gradient norm: 0.017985424026846886\n",
      "bn1.module.bias, gradient norm: 0.016073448583483696\n",
      "conv2.bias, gradient norm: 8.788760275102447e-10\n",
      "conv2.lin.weight, gradient norm: 0.03251130133867264\n",
      "bn2.module.weight, gradient norm: 0.022037185728549957\n",
      "bn2.module.bias, gradient norm: 0.027340605854988098\n",
      "conv3.bias, gradient norm: 0.03345915302634239\n",
      "conv3.lin.weight, gradient norm: 0.04479387402534485\n",
      "Epoch: 13, Training Loss: 0.3708, Validation Loss: 2.1698, Train Acc: 0.2565, Val Acc: 0.2607, Test Acc: 0.2579\n",
      "conv1.bias, gradient norm: 1.2924224190857814e-10\n",
      "conv1.lin.weight, gradient norm: 0.015403088182210922\n",
      "bn1.module.weight, gradient norm: 0.014223548583686352\n",
      "bn1.module.bias, gradient norm: 0.01510019600391388\n",
      "conv2.bias, gradient norm: 4.0499611997368845e-10\n",
      "conv2.lin.weight, gradient norm: 0.027378220111131668\n",
      "bn2.module.weight, gradient norm: 0.017216429114341736\n",
      "bn2.module.bias, gradient norm: 0.020790470764040947\n",
      "conv3.bias, gradient norm: 0.03544849902391434\n",
      "conv3.lin.weight, gradient norm: 0.05927367880940437\n",
      "Epoch: 14, Training Loss: 0.3522, Validation Loss: 2.0035, Train Acc: 0.2594, Val Acc: 0.2639, Test Acc: 0.2605\n",
      "conv1.bias, gradient norm: 9.248561638752761e-11\n",
      "conv1.lin.weight, gradient norm: 0.01591666415333748\n",
      "bn1.module.weight, gradient norm: 0.014049236662685871\n",
      "bn1.module.bias, gradient norm: 0.014043407514691353\n",
      "conv2.bias, gradient norm: 5.595722774032197e-10\n",
      "conv2.lin.weight, gradient norm: 0.02934378571808338\n",
      "bn2.module.weight, gradient norm: 0.01642639935016632\n",
      "bn2.module.bias, gradient norm: 0.017916036769747734\n",
      "conv3.bias, gradient norm: 0.031331438571214676\n",
      "conv3.lin.weight, gradient norm: 0.05742930993437767\n",
      "Epoch: 15, Training Loss: 0.3435, Validation Loss: 1.8074, Train Acc: 0.2608, Val Acc: 0.2653, Test Acc: 0.2615\n",
      "conv1.bias, gradient norm: 6.678888692102092e-11\n",
      "conv1.lin.weight, gradient norm: 0.016041424125432968\n",
      "bn1.module.weight, gradient norm: 0.014016799628734589\n",
      "bn1.module.bias, gradient norm: 0.01250801607966423\n",
      "conv2.bias, gradient norm: 6.441741584595206e-10\n",
      "conv2.lin.weight, gradient norm: 0.02798963524401188\n",
      "bn2.module.weight, gradient norm: 0.01919977180659771\n",
      "bn2.module.bias, gradient norm: 0.01924242451786995\n",
      "conv3.bias, gradient norm: 0.022352444007992744\n",
      "conv3.lin.weight, gradient norm: 0.03955747187137604\n",
      "Epoch: 16, Training Loss: 0.3360, Validation Loss: 1.6259, Train Acc: 0.2608, Val Acc: 0.2652, Test Acc: 0.2615\n",
      "conv1.bias, gradient norm: 1.460986193135838e-10\n",
      "conv1.lin.weight, gradient norm: 0.015414995141327381\n",
      "bn1.module.weight, gradient norm: 0.01511072926223278\n",
      "bn1.module.bias, gradient norm: 0.011396360583603382\n",
      "conv2.bias, gradient norm: 4.807714226728876e-10\n",
      "conv2.lin.weight, gradient norm: 0.026554729789495468\n",
      "bn2.module.weight, gradient norm: 0.020865634083747864\n",
      "bn2.module.bias, gradient norm: 0.020377065986394882\n",
      "conv3.bias, gradient norm: 0.018058281391859055\n",
      "conv3.lin.weight, gradient norm: 0.05890275165438652\n",
      "Epoch: 17, Training Loss: 0.3264, Validation Loss: 1.4847, Train Acc: 0.2605, Val Acc: 0.2650, Test Acc: 0.2611\n",
      "conv1.bias, gradient norm: 7.230432919058671e-11\n",
      "conv1.lin.weight, gradient norm: 0.015514620579779148\n",
      "bn1.module.weight, gradient norm: 0.014343173243105412\n",
      "bn1.module.bias, gradient norm: 0.011740142479538918\n",
      "conv2.bias, gradient norm: 1.0677831951966255e-09\n",
      "conv2.lin.weight, gradient norm: 0.024966755881905556\n",
      "bn2.module.weight, gradient norm: 0.018440961837768555\n",
      "bn2.module.bias, gradient norm: 0.017777306959033012\n",
      "conv3.bias, gradient norm: 0.01874920167028904\n",
      "conv3.lin.weight, gradient norm: 0.0344771109521389\n",
      "Epoch: 18, Training Loss: 0.3171, Validation Loss: 1.3852, Train Acc: 0.2603, Val Acc: 0.2648, Test Acc: 0.2609\n",
      "conv1.bias, gradient norm: 6.998776558297948e-11\n",
      "conv1.lin.weight, gradient norm: 0.01609385572373867\n",
      "bn1.module.weight, gradient norm: 0.0127024557441473\n",
      "bn1.module.bias, gradient norm: 0.011979647912085056\n",
      "conv2.bias, gradient norm: 1.4867352904346376e-09\n",
      "conv2.lin.weight, gradient norm: 0.02580748312175274\n",
      "bn2.module.weight, gradient norm: 0.017273319885134697\n",
      "bn2.module.bias, gradient norm: 0.01442743930965662\n",
      "conv3.bias, gradient norm: 0.022489141672849655\n",
      "conv3.lin.weight, gradient norm: 0.04303015395998955\n",
      "Epoch: 19, Training Loss: 0.3054, Validation Loss: 1.3176, Train Acc: 0.2597, Val Acc: 0.2640, Test Acc: 0.2602\n",
      "conv1.bias, gradient norm: 2.845371438464639e-10\n",
      "conv1.lin.weight, gradient norm: 0.02611534669995308\n",
      "bn1.module.weight, gradient norm: 0.012154536321759224\n",
      "bn1.module.bias, gradient norm: 0.013938399963080883\n",
      "conv2.bias, gradient norm: 1.175876507275575e-09\n",
      "conv2.lin.weight, gradient norm: 0.02200380526483059\n",
      "bn2.module.weight, gradient norm: 0.016347568482160568\n",
      "bn2.module.bias, gradient norm: 0.01503668911755085\n",
      "conv3.bias, gradient norm: 0.02349082939326763\n",
      "conv3.lin.weight, gradient norm: 0.0467393659055233\n",
      "Epoch: 20, Training Loss: 0.2989, Validation Loss: 1.2685, Train Acc: 0.2580, Val Acc: 0.2623, Test Acc: 0.2585\n",
      "conv1.bias, gradient norm: 1.9679284590790047e-10\n",
      "conv1.lin.weight, gradient norm: 0.01811891235411167\n",
      "bn1.module.weight, gradient norm: 0.015421465039253235\n",
      "bn1.module.bias, gradient norm: 0.012878826819360256\n",
      "conv2.bias, gradient norm: 1.2821779193927796e-09\n",
      "conv2.lin.weight, gradient norm: 0.030556708574295044\n",
      "bn2.module.weight, gradient norm: 0.015794042497873306\n",
      "bn2.module.bias, gradient norm: 0.02384735457599163\n",
      "conv3.bias, gradient norm: 0.018889646977186203\n",
      "conv3.lin.weight, gradient norm: 0.03925035148859024\n",
      "Epoch: 21, Training Loss: 0.2968, Validation Loss: 1.2275, Train Acc: 0.2569, Val Acc: 0.2613, Test Acc: 0.2574\n",
      "conv1.bias, gradient norm: 8.853850291812293e-11\n",
      "conv1.lin.weight, gradient norm: 0.016500983387231827\n",
      "bn1.module.weight, gradient norm: 0.015258070081472397\n",
      "bn1.module.bias, gradient norm: 0.012425431981682777\n",
      "conv2.bias, gradient norm: 8.589035038752968e-10\n",
      "conv2.lin.weight, gradient norm: 0.03216622397303581\n",
      "bn2.module.weight, gradient norm: 0.013830085285007954\n",
      "bn2.module.bias, gradient norm: 0.02538927085697651\n",
      "conv3.bias, gradient norm: 0.019387608394026756\n",
      "conv3.lin.weight, gradient norm: 0.03958359733223915\n",
      "Epoch: 22, Training Loss: 0.2916, Validation Loss: 1.1917, Train Acc: 0.2561, Val Acc: 0.2604, Test Acc: 0.2565\n",
      "conv1.bias, gradient norm: 2.565771761720015e-10\n",
      "conv1.lin.weight, gradient norm: 0.04892902076244354\n",
      "bn1.module.weight, gradient norm: 0.017275139689445496\n",
      "bn1.module.bias, gradient norm: 0.023361189290881157\n",
      "conv2.bias, gradient norm: 1.5129034691696575e-09\n",
      "conv2.lin.weight, gradient norm: 0.05529895797371864\n",
      "bn2.module.weight, gradient norm: 0.014225035905838013\n",
      "bn2.module.bias, gradient norm: 0.028885817155241966\n",
      "conv3.bias, gradient norm: 0.024060355499386787\n",
      "conv3.lin.weight, gradient norm: 0.06202143430709839\n",
      "Epoch: 23, Training Loss: 0.2895, Validation Loss: 1.1711, Train Acc: 0.2526, Val Acc: 0.2569, Test Acc: 0.2532\n",
      "conv1.bias, gradient norm: 2.305622082143799e-10\n",
      "conv1.lin.weight, gradient norm: 0.020846940577030182\n",
      "bn1.module.weight, gradient norm: 0.014325921423733234\n",
      "bn1.module.bias, gradient norm: 0.010031327605247498\n",
      "conv2.bias, gradient norm: 9.488618779585067e-10\n",
      "conv2.lin.weight, gradient norm: 0.028232311829924583\n",
      "bn2.module.weight, gradient norm: 0.020213285461068153\n",
      "bn2.module.bias, gradient norm: 0.029277591034770012\n",
      "conv3.bias, gradient norm: 0.019612155854701996\n",
      "conv3.lin.weight, gradient norm: 0.04843337833881378\n",
      "Epoch: 24, Training Loss: 0.2800, Validation Loss: 1.1558, Train Acc: 0.2499, Val Acc: 0.2540, Test Acc: 0.2506\n",
      "conv1.bias, gradient norm: 1.0215905899002564e-10\n",
      "conv1.lin.weight, gradient norm: 0.020710399374365807\n",
      "bn1.module.weight, gradient norm: 0.014176249504089355\n",
      "bn1.module.bias, gradient norm: 0.009657735005021095\n",
      "conv2.bias, gradient norm: 1.472366339960729e-09\n",
      "conv2.lin.weight, gradient norm: 0.021372593939304352\n",
      "bn2.module.weight, gradient norm: 0.01979677937924862\n",
      "bn2.module.bias, gradient norm: 0.027530133724212646\n",
      "conv3.bias, gradient norm: 0.01907338947057724\n",
      "conv3.lin.weight, gradient norm: 0.05451739951968193\n",
      "Epoch: 25, Training Loss: 0.2734, Validation Loss: 1.1408, Train Acc: 0.2488, Val Acc: 0.2531, Test Acc: 0.2495\n",
      "conv1.bias, gradient norm: 1.2995600429110965e-10\n",
      "conv1.lin.weight, gradient norm: 0.025598840788006783\n",
      "bn1.module.weight, gradient norm: 0.012428159825503826\n",
      "bn1.module.bias, gradient norm: 0.01475653238594532\n",
      "conv2.bias, gradient norm: 1.3553678179789586e-09\n",
      "conv2.lin.weight, gradient norm: 0.05240645632147789\n",
      "bn2.module.weight, gradient norm: 0.012826538644731045\n",
      "bn2.module.bias, gradient norm: 0.02709261141717434\n",
      "conv3.bias, gradient norm: 0.020008666440844536\n",
      "conv3.lin.weight, gradient norm: 0.0396256148815155\n",
      "Epoch: 26, Training Loss: 0.2711, Validation Loss: 1.1327, Train Acc: 0.2425, Val Acc: 0.2460, Test Acc: 0.2430\n",
      "conv1.bias, gradient norm: 1.9225372682729613e-10\n",
      "conv1.lin.weight, gradient norm: 0.021450631320476532\n",
      "bn1.module.weight, gradient norm: 0.01142946071922779\n",
      "bn1.module.bias, gradient norm: 0.01069109607487917\n",
      "conv2.bias, gradient norm: 1.7309335076376442e-09\n",
      "conv2.lin.weight, gradient norm: 0.020435286685824394\n",
      "bn2.module.weight, gradient norm: 0.015187785029411316\n",
      "bn2.module.bias, gradient norm: 0.0228439811617136\n",
      "conv3.bias, gradient norm: 0.01633668877184391\n",
      "conv3.lin.weight, gradient norm: 0.024575578048825264\n",
      "Epoch: 27, Training Loss: 0.2662, Validation Loss: 1.1171, Train Acc: 0.2376, Val Acc: 0.2414, Test Acc: 0.2388\n",
      "conv1.bias, gradient norm: 1.8624976561021356e-10\n",
      "conv1.lin.weight, gradient norm: 0.023398999124765396\n",
      "bn1.module.weight, gradient norm: 0.010828336700797081\n",
      "bn1.module.bias, gradient norm: 0.012078125961124897\n",
      "conv2.bias, gradient norm: 9.185080473983476e-10\n",
      "conv2.lin.weight, gradient norm: 0.0326429158449173\n",
      "bn2.module.weight, gradient norm: 0.014320146292448044\n",
      "bn2.module.bias, gradient norm: 0.023964138701558113\n",
      "conv3.bias, gradient norm: 0.015332893468439579\n",
      "conv3.lin.weight, gradient norm: 0.025898274034261703\n",
      "Epoch: 28, Training Loss: 0.2632, Validation Loss: 1.0914, Train Acc: 0.2361, Val Acc: 0.2400, Test Acc: 0.2377\n",
      "conv1.bias, gradient norm: 1.6601772734325948e-10\n",
      "conv1.lin.weight, gradient norm: 0.039487965404987335\n",
      "bn1.module.weight, gradient norm: 0.01387376245111227\n",
      "bn1.module.bias, gradient norm: 0.012424490414559841\n",
      "conv2.bias, gradient norm: 1.7642141081353202e-09\n",
      "conv2.lin.weight, gradient norm: 0.03424699977040291\n",
      "bn2.module.weight, gradient norm: 0.014386139810085297\n",
      "bn2.module.bias, gradient norm: 0.01641453430056572\n",
      "conv3.bias, gradient norm: 0.020045539364218712\n",
      "conv3.lin.weight, gradient norm: 0.05852064490318298\n",
      "Epoch: 29, Training Loss: 0.2629, Validation Loss: 1.0707, Train Acc: 0.2278, Val Acc: 0.2314, Test Acc: 0.2290\n",
      "conv1.bias, gradient norm: 2.1828193419448638e-10\n",
      "conv1.lin.weight, gradient norm: 0.02161208912730217\n",
      "bn1.module.weight, gradient norm: 0.009911107830703259\n",
      "bn1.module.bias, gradient norm: 0.008997729048132896\n",
      "conv2.bias, gradient norm: 2.2327935056409842e-09\n",
      "conv2.lin.weight, gradient norm: 0.020684678107500076\n",
      "bn2.module.weight, gradient norm: 0.016627894714474678\n",
      "bn2.module.bias, gradient norm: 0.01372457854449749\n",
      "conv3.bias, gradient norm: 0.01451114658266306\n",
      "conv3.lin.weight, gradient norm: 0.024701377376914024\n",
      "Epoch: 30, Training Loss: 0.2558, Validation Loss: 1.0476, Train Acc: 0.2240, Val Acc: 0.2280, Test Acc: 0.2253\n",
      "conv1.bias, gradient norm: 1.171118202414334e-10\n",
      "conv1.lin.weight, gradient norm: 0.021814364939928055\n",
      "bn1.module.weight, gradient norm: 0.011507485993206501\n",
      "bn1.module.bias, gradient norm: 0.008006180636584759\n",
      "conv2.bias, gradient norm: 5.540501835099576e-09\n",
      "conv2.lin.weight, gradient norm: 0.02988639660179615\n",
      "bn2.module.weight, gradient norm: 0.026644835248589516\n",
      "bn2.module.bias, gradient norm: 0.021061033010482788\n",
      "conv3.bias, gradient norm: 0.013164633885025978\n",
      "conv3.lin.weight, gradient norm: 0.024386046454310417\n",
      "Epoch: 31, Training Loss: 0.2538, Validation Loss: 1.0221, Train Acc: 0.2223, Val Acc: 0.2261, Test Acc: 0.2235\n",
      "conv1.bias, gradient norm: 2.0641202636006994e-10\n",
      "conv1.lin.weight, gradient norm: 0.03269276022911072\n",
      "bn1.module.weight, gradient norm: 0.014169754460453987\n",
      "bn1.module.bias, gradient norm: 0.013358095660805702\n",
      "conv2.bias, gradient norm: 6.4976339864131205e-09\n",
      "conv2.lin.weight, gradient norm: 0.03646676614880562\n",
      "bn2.module.weight, gradient norm: 0.028332900255918503\n",
      "bn2.module.bias, gradient norm: 0.02002306655049324\n",
      "conv3.bias, gradient norm: 0.01384845469146967\n",
      "conv3.lin.weight, gradient norm: 0.020877039059996605\n",
      "Epoch: 32, Training Loss: 0.2528, Validation Loss: 1.0070, Train Acc: 0.2236, Val Acc: 0.2267, Test Acc: 0.2249\n",
      "conv1.bias, gradient norm: 1.5042894707661958e-10\n",
      "conv1.lin.weight, gradient norm: 0.04152408242225647\n",
      "bn1.module.weight, gradient norm: 0.018371790647506714\n",
      "bn1.module.bias, gradient norm: 0.011904273182153702\n",
      "conv2.bias, gradient norm: 3.4798386394641057e-09\n",
      "conv2.lin.weight, gradient norm: 0.05986800789833069\n",
      "bn2.module.weight, gradient norm: 0.02460537850856781\n",
      "bn2.module.bias, gradient norm: 0.025761714205145836\n",
      "conv3.bias, gradient norm: 0.011533377692103386\n",
      "conv3.lin.weight, gradient norm: 0.08202888071537018\n",
      "Epoch: 33, Training Loss: 0.2521, Validation Loss: 0.9810, Train Acc: 0.3012, Val Acc: 0.3061, Test Acc: 0.3041\n",
      "conv1.bias, gradient norm: 4.798711983333703e-10\n",
      "conv1.lin.weight, gradient norm: 0.16901639103889465\n",
      "bn1.module.weight, gradient norm: 0.05688396841287613\n",
      "bn1.module.bias, gradient norm: 0.05132252722978592\n",
      "conv2.bias, gradient norm: 1.2042790764610345e-08\n",
      "conv2.lin.weight, gradient norm: 0.21230193972587585\n",
      "bn2.module.weight, gradient norm: 0.045200422406196594\n",
      "bn2.module.bias, gradient norm: 0.08060646802186966\n",
      "conv3.bias, gradient norm: 0.041908781975507736\n",
      "conv3.lin.weight, gradient norm: 0.1570511907339096\n",
      "Epoch: 34, Training Loss: 0.2645, Validation Loss: 1.0013, Train Acc: 0.5966, Val Acc: 0.6012, Test Acc: 0.5997\n",
      "conv1.bias, gradient norm: 3.1641164688345214e-10\n",
      "conv1.lin.weight, gradient norm: 0.08256037533283234\n",
      "bn1.module.weight, gradient norm: 0.037235237658023834\n",
      "bn1.module.bias, gradient norm: 0.023051945492625237\n",
      "conv2.bias, gradient norm: 9.31158350425676e-09\n",
      "conv2.lin.weight, gradient norm: 0.13089556992053986\n",
      "bn2.module.weight, gradient norm: 0.04789825156331062\n",
      "bn2.module.bias, gradient norm: 0.05130136013031006\n",
      "conv3.bias, gradient norm: 0.03505079448223114\n",
      "conv3.lin.weight, gradient norm: 0.23952744901180267\n",
      "Epoch: 35, Training Loss: 0.2734, Validation Loss: 0.9858, Train Acc: 0.9358, Val Acc: 0.9354, Test Acc: 0.9363\n",
      "conv1.bias, gradient norm: 3.385575431114063e-10\n",
      "conv1.lin.weight, gradient norm: 0.03968524560332298\n",
      "bn1.module.weight, gradient norm: 0.015225325711071491\n",
      "bn1.module.bias, gradient norm: 0.01893923617899418\n",
      "conv2.bias, gradient norm: 5.297992711206234e-09\n",
      "conv2.lin.weight, gradient norm: 0.04925120994448662\n",
      "bn2.module.weight, gradient norm: 0.023793749511241913\n",
      "bn2.module.bias, gradient norm: 0.029754146933555603\n",
      "conv3.bias, gradient norm: 0.008804280310869217\n",
      "conv3.lin.weight, gradient norm: 0.0482405386865139\n",
      "Epoch: 36, Training Loss: 0.2567, Validation Loss: 0.9726, Train Acc: 0.9559, Val Acc: 0.9551, Test Acc: 0.9570\n",
      "conv1.bias, gradient norm: 5.054864304909756e-10\n",
      "conv1.lin.weight, gradient norm: 0.13721752166748047\n",
      "bn1.module.weight, gradient norm: 0.0582357682287693\n",
      "bn1.module.bias, gradient norm: 0.0667465552687645\n",
      "conv2.bias, gradient norm: 7.81633335833476e-09\n",
      "conv2.lin.weight, gradient norm: 0.09301555156707764\n",
      "bn2.module.weight, gradient norm: 0.026816658675670624\n",
      "bn2.module.bias, gradient norm: 0.035132650285959244\n",
      "conv3.bias, gradient norm: 0.046376362442970276\n",
      "conv3.lin.weight, gradient norm: 0.19139641523361206\n",
      "Epoch: 37, Training Loss: 0.2653, Validation Loss: 0.9702, Train Acc: 0.9575, Val Acc: 0.9570, Test Acc: 0.9583\n",
      "conv1.bias, gradient norm: 1.3783442442960592e-10\n",
      "conv1.lin.weight, gradient norm: 0.022684382274746895\n",
      "bn1.module.weight, gradient norm: 0.01646234653890133\n",
      "bn1.module.bias, gradient norm: 0.016469746828079224\n",
      "conv2.bias, gradient norm: 4.244483875481819e-09\n",
      "conv2.lin.weight, gradient norm: 0.029447264969348907\n",
      "bn2.module.weight, gradient norm: 0.016024285927414894\n",
      "bn2.module.bias, gradient norm: 0.020568955689668655\n",
      "conv3.bias, gradient norm: 0.025821160525083542\n",
      "conv3.lin.weight, gradient norm: 0.08767879754304886\n",
      "Epoch: 38, Training Loss: 0.2471, Validation Loss: 0.9689, Train Acc: 0.9561, Val Acc: 0.9554, Test Acc: 0.9567\n",
      "conv1.bias, gradient norm: 1.3389227226934253e-10\n",
      "conv1.lin.weight, gradient norm: 0.030318522825837135\n",
      "bn1.module.weight, gradient norm: 0.019412608817219734\n",
      "bn1.module.bias, gradient norm: 0.01971616968512535\n",
      "conv2.bias, gradient norm: 4.001156295174724e-09\n",
      "conv2.lin.weight, gradient norm: 0.03222554177045822\n",
      "bn2.module.weight, gradient norm: 0.022059494629502296\n",
      "bn2.module.bias, gradient norm: 0.028459934517741203\n",
      "conv3.bias, gradient norm: 0.010751981288194656\n",
      "conv3.lin.weight, gradient norm: 0.021941348910331726\n",
      "Epoch: 39, Training Loss: 0.2449, Validation Loss: 0.9633, Train Acc: 0.9554, Val Acc: 0.9551, Test Acc: 0.9564\n",
      "conv1.bias, gradient norm: 1.733029136863351e-10\n",
      "conv1.lin.weight, gradient norm: 0.030751336365938187\n",
      "bn1.module.weight, gradient norm: 0.021717824041843414\n",
      "bn1.module.bias, gradient norm: 0.0198337584733963\n",
      "conv2.bias, gradient norm: 3.511878121642553e-09\n",
      "conv2.lin.weight, gradient norm: 0.03173302859067917\n",
      "bn2.module.weight, gradient norm: 0.026031671091914177\n",
      "bn2.module.bias, gradient norm: 0.027522185817360878\n",
      "conv3.bias, gradient norm: 0.00855344533920288\n",
      "conv3.lin.weight, gradient norm: 0.05303385108709335\n",
      "Epoch: 40, Training Loss: 0.2408, Validation Loss: 0.9507, Train Acc: 0.9579, Val Acc: 0.9575, Test Acc: 0.9592\n",
      "conv1.bias, gradient norm: 2.6239432848740307e-10\n",
      "conv1.lin.weight, gradient norm: 0.09485521167516708\n",
      "bn1.module.weight, gradient norm: 0.047822266817092896\n",
      "bn1.module.bias, gradient norm: 0.04011988267302513\n",
      "conv2.bias, gradient norm: 7.596928419673077e-09\n",
      "conv2.lin.weight, gradient norm: 0.04752068594098091\n",
      "bn2.module.weight, gradient norm: 0.024017004296183586\n",
      "bn2.module.bias, gradient norm: 0.02166922204196453\n",
      "conv3.bias, gradient norm: 0.01795518584549427\n",
      "conv3.lin.weight, gradient norm: 0.04389273747801781\n",
      "Epoch: 41, Training Loss: 0.2443, Validation Loss: 0.9420, Train Acc: 0.9520, Val Acc: 0.9517, Test Acc: 0.9535\n",
      "conv1.bias, gradient norm: 1.2553949546578735e-10\n",
      "conv1.lin.weight, gradient norm: 0.023199424147605896\n",
      "bn1.module.weight, gradient norm: 0.018460769206285477\n",
      "bn1.module.bias, gradient norm: 0.01483911368995905\n",
      "conv2.bias, gradient norm: 4.10859168908928e-09\n",
      "conv2.lin.weight, gradient norm: 0.034244608134031296\n",
      "bn2.module.weight, gradient norm: 0.03526704013347626\n",
      "bn2.module.bias, gradient norm: 0.024624189361929893\n",
      "conv3.bias, gradient norm: 0.011615500785410404\n",
      "conv3.lin.weight, gradient norm: 0.08868332952260971\n",
      "Epoch: 42, Training Loss: 0.2343, Validation Loss: 0.9274, Train Acc: 0.9533, Val Acc: 0.9533, Test Acc: 0.9547\n",
      "conv1.bias, gradient norm: 2.672241594670055e-10\n",
      "conv1.lin.weight, gradient norm: 0.03726299852132797\n",
      "bn1.module.weight, gradient norm: 0.024635251611471176\n",
      "bn1.module.bias, gradient norm: 0.024212336167693138\n",
      "conv2.bias, gradient norm: 7.38565120173007e-09\n",
      "conv2.lin.weight, gradient norm: 0.029166530817747116\n",
      "bn2.module.weight, gradient norm: 0.024227172136306763\n",
      "bn2.module.bias, gradient norm: 0.010068416595458984\n",
      "conv3.bias, gradient norm: 0.0106685571372509\n",
      "conv3.lin.weight, gradient norm: 0.021781131625175476\n",
      "Epoch: 43, Training Loss: 0.2336, Validation Loss: 0.9145, Train Acc: 0.9533, Val Acc: 0.9528, Test Acc: 0.9549\n",
      "conv1.bias, gradient norm: 1.8192171380437827e-10\n",
      "conv1.lin.weight, gradient norm: 0.05672773718833923\n",
      "bn1.module.weight, gradient norm: 0.030268793925642967\n",
      "bn1.module.bias, gradient norm: 0.032196495682001114\n",
      "conv2.bias, gradient norm: 5.832761829083211e-09\n",
      "conv2.lin.weight, gradient norm: 0.04353543743491173\n",
      "bn2.module.weight, gradient norm: 0.01970098540186882\n",
      "bn2.module.bias, gradient norm: 0.015167555771768093\n",
      "conv3.bias, gradient norm: 0.01665269024670124\n",
      "conv3.lin.weight, gradient norm: 0.04903256520628929\n",
      "Epoch: 44, Training Loss: 0.2287, Validation Loss: 0.9060, Train Acc: 0.9529, Val Acc: 0.9525, Test Acc: 0.9544\n",
      "conv1.bias, gradient norm: 1.1352739581749205e-10\n",
      "conv1.lin.weight, gradient norm: 0.03107842616736889\n",
      "bn1.module.weight, gradient norm: 0.034848786890506744\n",
      "bn1.module.bias, gradient norm: 0.022152233868837357\n",
      "conv2.bias, gradient norm: 7.816749914013599e-09\n",
      "conv2.lin.weight, gradient norm: 0.041272249072790146\n",
      "bn2.module.weight, gradient norm: 0.030441099777817726\n",
      "bn2.module.bias, gradient norm: 0.027362514287233353\n",
      "conv3.bias, gradient norm: 0.01205239538103342\n",
      "conv3.lin.weight, gradient norm: 0.0807255432009697\n",
      "Epoch: 45, Training Loss: 0.2282, Validation Loss: 0.8887, Train Acc: 0.9551, Val Acc: 0.9548, Test Acc: 0.9561\n",
      "conv1.bias, gradient norm: 2.489045636266951e-10\n",
      "conv1.lin.weight, gradient norm: 0.022653238847851753\n",
      "bn1.module.weight, gradient norm: 0.020628392696380615\n",
      "bn1.module.bias, gradient norm: 0.017310960218310356\n",
      "conv2.bias, gradient norm: 5.933070035268884e-09\n",
      "conv2.lin.weight, gradient norm: 0.023019691929221153\n",
      "bn2.module.weight, gradient norm: 0.015100277028977871\n",
      "bn2.module.bias, gradient norm: 0.016557199880480766\n",
      "conv3.bias, gradient norm: 0.01351469848304987\n",
      "conv3.lin.weight, gradient norm: 0.0204642154276371\n",
      "Epoch: 46, Training Loss: 0.2216, Validation Loss: 0.8727, Train Acc: 0.9550, Val Acc: 0.9541, Test Acc: 0.9563\n",
      "conv1.bias, gradient norm: 2.200559040543837e-10\n",
      "conv1.lin.weight, gradient norm: 0.05565023422241211\n",
      "bn1.module.weight, gradient norm: 0.03734387829899788\n",
      "bn1.module.bias, gradient norm: 0.03898611664772034\n",
      "conv2.bias, gradient norm: 8.201157974951911e-09\n",
      "conv2.lin.weight, gradient norm: 0.04984194040298462\n",
      "bn2.module.weight, gradient norm: 0.018490485846996307\n",
      "bn2.module.bias, gradient norm: 0.02240537479519844\n",
      "conv3.bias, gradient norm: 0.02655981108546257\n",
      "conv3.lin.weight, gradient norm: 0.08900818973779678\n",
      "Epoch: 47, Training Loss: 0.2226, Validation Loss: 0.8632, Train Acc: 0.9475, Val Acc: 0.9470, Test Acc: 0.9492\n",
      "conv1.bias, gradient norm: 2.0255383481604383e-10\n",
      "conv1.lin.weight, gradient norm: 0.041739050298929214\n",
      "bn1.module.weight, gradient norm: 0.05200354382395744\n",
      "bn1.module.bias, gradient norm: 0.039367470890283585\n",
      "conv2.bias, gradient norm: 8.32994295763001e-09\n",
      "conv2.lin.weight, gradient norm: 0.044613875448703766\n",
      "bn2.module.weight, gradient norm: 0.018133778125047684\n",
      "bn2.module.bias, gradient norm: 0.023527590557932854\n",
      "conv3.bias, gradient norm: 0.008648752234876156\n",
      "conv3.lin.weight, gradient norm: 0.05231526121497154\n",
      "Epoch: 48, Training Loss: 0.2222, Validation Loss: 0.8376, Train Acc: 0.9243, Val Acc: 0.9234, Test Acc: 0.9263\n",
      "conv1.bias, gradient norm: 2.2709394087438994e-10\n",
      "conv1.lin.weight, gradient norm: 0.02659989893436432\n",
      "bn1.module.weight, gradient norm: 0.02900172770023346\n",
      "bn1.module.bias, gradient norm: 0.023689597845077515\n",
      "conv2.bias, gradient norm: 6.942836972001487e-09\n",
      "conv2.lin.weight, gradient norm: 0.03085976280272007\n",
      "bn2.module.weight, gradient norm: 0.01751878298819065\n",
      "bn2.module.bias, gradient norm: 0.015836114063858986\n",
      "conv3.bias, gradient norm: 0.011471297591924667\n",
      "conv3.lin.weight, gradient norm: 0.024862030521035194\n",
      "Epoch: 49, Training Loss: 0.2138, Validation Loss: 0.8026, Train Acc: 0.9193, Val Acc: 0.9182, Test Acc: 0.9219\n",
      "conv1.bias, gradient norm: 9.194618955099543e-10\n",
      "conv1.lin.weight, gradient norm: 0.1957823634147644\n",
      "bn1.module.weight, gradient norm: 0.15079635381698608\n",
      "bn1.module.bias, gradient norm: 0.13040916621685028\n",
      "conv2.bias, gradient norm: 5.700591287904899e-08\n",
      "conv2.lin.weight, gradient norm: 0.14353305101394653\n",
      "bn2.module.weight, gradient norm: 0.048208095133304596\n",
      "bn2.module.bias, gradient norm: 0.0560881644487381\n",
      "conv3.bias, gradient norm: 0.041111163794994354\n",
      "conv3.lin.weight, gradient norm: 0.16436170041561127\n",
      "Epoch: 50, Training Loss: 0.2246, Validation Loss: 0.7792, Train Acc: 0.9264, Val Acc: 0.9256, Test Acc: 0.9283\n",
      "conv1.bias, gradient norm: 6.394598184300548e-10\n",
      "conv1.lin.weight, gradient norm: 0.0776674747467041\n",
      "bn1.module.weight, gradient norm: 0.14284555613994598\n",
      "bn1.module.bias, gradient norm: 0.08328340202569962\n",
      "conv2.bias, gradient norm: 1.924618331372585e-08\n",
      "conv2.lin.weight, gradient norm: 0.12117990851402283\n",
      "bn2.module.weight, gradient norm: 0.04962122440338135\n",
      "bn2.module.bias, gradient norm: 0.05154898390173912\n",
      "conv3.bias, gradient norm: 0.04107489436864853\n",
      "conv3.lin.weight, gradient norm: 0.3112863004207611\n",
      "Epoch: 51, Training Loss: 0.2495, Validation Loss: 0.7400, Train Acc: 0.9237, Val Acc: 0.9223, Test Acc: 0.9259\n",
      "conv1.bias, gradient norm: 4.0138409262979735e-10\n",
      "conv1.lin.weight, gradient norm: 0.05734558776021004\n",
      "bn1.module.weight, gradient norm: 0.09262877702713013\n",
      "bn1.module.bias, gradient norm: 0.061753369867801666\n",
      "conv2.bias, gradient norm: 1.7160399323756792e-08\n",
      "conv2.lin.weight, gradient norm: 0.0733913853764534\n",
      "bn2.module.weight, gradient norm: 0.030675837770104408\n",
      "bn2.module.bias, gradient norm: 0.03692827373743057\n",
      "conv3.bias, gradient norm: 0.021309418603777885\n",
      "conv3.lin.weight, gradient norm: 0.1786033809185028\n",
      "Epoch: 52, Training Loss: 0.2324, Validation Loss: 0.6974, Train Acc: 0.9372, Val Acc: 0.9364, Test Acc: 0.9399\n",
      "conv1.bias, gradient norm: 6.187846901539729e-10\n",
      "conv1.lin.weight, gradient norm: 0.06181452423334122\n",
      "bn1.module.weight, gradient norm: 0.04375297948718071\n",
      "bn1.module.bias, gradient norm: 0.03988410905003548\n",
      "conv2.bias, gradient norm: 1.678567684848531e-08\n",
      "conv2.lin.weight, gradient norm: 0.05173954367637634\n",
      "bn2.module.weight, gradient norm: 0.018944548442959785\n",
      "bn2.module.bias, gradient norm: 0.015328297391533852\n",
      "conv3.bias, gradient norm: 0.025789834558963776\n",
      "conv3.lin.weight, gradient norm: 0.08368349075317383\n",
      "Epoch: 53, Training Loss: 0.2216, Validation Loss: 0.6581, Train Acc: 0.9420, Val Acc: 0.9412, Test Acc: 0.9434\n",
      "conv1.bias, gradient norm: 8.774517223919531e-10\n",
      "conv1.lin.weight, gradient norm: 0.16462036967277527\n",
      "bn1.module.weight, gradient norm: 0.11839312314987183\n",
      "bn1.module.bias, gradient norm: 0.11140599846839905\n",
      "conv2.bias, gradient norm: 3.46026567399349e-08\n",
      "conv2.lin.weight, gradient norm: 0.12903012335300446\n",
      "bn2.module.weight, gradient norm: 0.04658284783363342\n",
      "bn2.module.bias, gradient norm: 0.058470457792282104\n",
      "conv3.bias, gradient norm: 0.055854614824056625\n",
      "conv3.lin.weight, gradient norm: 0.22368575632572174\n",
      "Epoch: 54, Training Loss: 0.2293, Validation Loss: 0.6202, Train Acc: 0.9413, Val Acc: 0.9404, Test Acc: 0.9431\n",
      "conv1.bias, gradient norm: 1.7461350421132948e-10\n",
      "conv1.lin.weight, gradient norm: 0.029898684471845627\n",
      "bn1.module.weight, gradient norm: 0.03724336996674538\n",
      "bn1.module.bias, gradient norm: 0.022653456777334213\n",
      "conv2.bias, gradient norm: 1.1177793801664393e-08\n",
      "conv2.lin.weight, gradient norm: 0.032043714076280594\n",
      "bn2.module.weight, gradient norm: 0.014249902218580246\n",
      "bn2.module.bias, gradient norm: 0.013412016443908215\n",
      "conv3.bias, gradient norm: 0.023953864350914955\n",
      "conv3.lin.weight, gradient norm: 0.05841212347149849\n",
      "Epoch: 55, Training Loss: 0.2110, Validation Loss: 0.5830, Train Acc: 0.9434, Val Acc: 0.9427, Test Acc: 0.9452\n",
      "conv1.bias, gradient norm: 2.580041180699766e-10\n",
      "conv1.lin.weight, gradient norm: 0.03828754276037216\n",
      "bn1.module.weight, gradient norm: 0.042883872985839844\n",
      "bn1.module.bias, gradient norm: 0.03352898731827736\n",
      "conv2.bias, gradient norm: 1.042665420669664e-08\n",
      "conv2.lin.weight, gradient norm: 0.04272250086069107\n",
      "bn2.module.weight, gradient norm: 0.016549065709114075\n",
      "bn2.module.bias, gradient norm: 0.025294842198491096\n",
      "conv3.bias, gradient norm: 0.011627284809947014\n",
      "conv3.lin.weight, gradient norm: 0.026171116158366203\n",
      "Epoch: 56, Training Loss: 0.2167, Validation Loss: 0.5480, Train Acc: 0.9683, Val Acc: 0.9681, Test Acc: 0.9699\n",
      "conv1.bias, gradient norm: 3.034179574257223e-10\n",
      "conv1.lin.weight, gradient norm: 0.04222298413515091\n",
      "bn1.module.weight, gradient norm: 0.036444950848817825\n",
      "bn1.module.bias, gradient norm: 0.03257526457309723\n",
      "conv2.bias, gradient norm: 8.549299934657029e-09\n",
      "conv2.lin.weight, gradient norm: 0.049537792801856995\n",
      "bn2.module.weight, gradient norm: 0.020263230428099632\n",
      "bn2.module.bias, gradient norm: 0.028150636702775955\n",
      "conv3.bias, gradient norm: 0.00828847847878933\n",
      "conv3.lin.weight, gradient norm: 0.048151060938835144\n",
      "Epoch: 57, Training Loss: 0.2124, Validation Loss: 0.5153, Train Acc: 0.9866, Val Acc: 0.9869, Test Acc: 0.9872\n",
      "conv1.bias, gradient norm: 2.3670110316231785e-10\n",
      "conv1.lin.weight, gradient norm: 0.030683116987347603\n",
      "bn1.module.weight, gradient norm: 0.023180773481726646\n",
      "bn1.module.bias, gradient norm: 0.03186911717057228\n",
      "conv2.bias, gradient norm: 9.5562295854279e-09\n",
      "conv2.lin.weight, gradient norm: 0.036855217069387436\n",
      "bn2.module.weight, gradient norm: 0.02026781067252159\n",
      "bn2.module.bias, gradient norm: 0.01742052100598812\n",
      "conv3.bias, gradient norm: 0.012871257029473782\n",
      "conv3.lin.weight, gradient norm: 0.028472037985920906\n",
      "Epoch: 58, Training Loss: 0.2091, Validation Loss: 0.4895, Train Acc: 0.9886, Val Acc: 0.9890, Test Acc: 0.9891\n",
      "conv1.bias, gradient norm: 5.228746879915036e-10\n",
      "conv1.lin.weight, gradient norm: 0.094574935734272\n",
      "bn1.module.weight, gradient norm: 0.058314986526966095\n",
      "bn1.module.bias, gradient norm: 0.076382115483284\n",
      "conv2.bias, gradient norm: 1.5560619459620284e-08\n",
      "conv2.lin.weight, gradient norm: 0.09238599240779877\n",
      "bn2.module.weight, gradient norm: 0.019215332344174385\n",
      "bn2.module.bias, gradient norm: 0.02877337671816349\n",
      "conv3.bias, gradient norm: 0.026126600801944733\n",
      "conv3.lin.weight, gradient norm: 0.11002589017152786\n",
      "Epoch: 59, Training Loss: 0.2124, Validation Loss: 0.4710, Train Acc: 0.9886, Val Acc: 0.9890, Test Acc: 0.9890\n",
      "conv1.bias, gradient norm: 1.5790033169871265e-10\n",
      "conv1.lin.weight, gradient norm: 0.021676750853657722\n",
      "bn1.module.weight, gradient norm: 0.01888846419751644\n",
      "bn1.module.bias, gradient norm: 0.018492789939045906\n",
      "conv2.bias, gradient norm: 7.888742104000812e-09\n",
      "conv2.lin.weight, gradient norm: 0.037902262061834335\n",
      "bn2.module.weight, gradient norm: 0.023893799632787704\n",
      "bn2.module.bias, gradient norm: 0.02260379120707512\n",
      "conv3.bias, gradient norm: 0.00821028184145689\n",
      "conv3.lin.weight, gradient norm: 0.05334394425153732\n",
      "Epoch: 60, Training Loss: 0.2091, Validation Loss: 0.4602, Train Acc: 0.9892, Val Acc: 0.9894, Test Acc: 0.9896\n",
      "conv1.bias, gradient norm: 3.2158387064384897e-10\n",
      "conv1.lin.weight, gradient norm: 0.03238759934902191\n",
      "bn1.module.weight, gradient norm: 0.025028668344020844\n",
      "bn1.module.bias, gradient norm: 0.02702805958688259\n",
      "conv2.bias, gradient norm: 9.051752236644006e-09\n",
      "conv2.lin.weight, gradient norm: 0.044966068118810654\n",
      "bn2.module.weight, gradient norm: 0.0239492729306221\n",
      "bn2.module.bias, gradient norm: 0.02714148350059986\n",
      "conv3.bias, gradient norm: 0.009836708195507526\n",
      "conv3.lin.weight, gradient norm: 0.07679746299982071\n",
      "Epoch: 61, Training Loss: 0.2074, Validation Loss: 0.4553, Train Acc: 0.9907, Val Acc: 0.9906, Test Acc: 0.9912\n",
      "conv1.bias, gradient norm: 2.990596659202538e-10\n",
      "conv1.lin.weight, gradient norm: 0.03385257348418236\n",
      "bn1.module.weight, gradient norm: 0.017131898552179337\n",
      "bn1.module.bias, gradient norm: 0.02269587479531765\n",
      "conv2.bias, gradient norm: 9.902445086140688e-09\n",
      "conv2.lin.weight, gradient norm: 0.031045563519001007\n",
      "bn2.module.weight, gradient norm: 0.01977039873600006\n",
      "bn2.module.bias, gradient norm: 0.011413320899009705\n",
      "conv3.bias, gradient norm: 0.009850517846643925\n",
      "conv3.lin.weight, gradient norm: 0.02409793809056282\n",
      "Epoch: 62, Training Loss: 0.2037, Validation Loss: 0.4513, Train Acc: 0.9915, Val Acc: 0.9914, Test Acc: 0.9920\n",
      "conv1.bias, gradient norm: 4.707937373282789e-10\n",
      "conv1.lin.weight, gradient norm: 0.07326102256774902\n",
      "bn1.module.weight, gradient norm: 0.03725375235080719\n",
      "bn1.module.bias, gradient norm: 0.03814849629998207\n",
      "conv2.bias, gradient norm: 1.3301402645993221e-08\n",
      "conv2.lin.weight, gradient norm: 0.0492250919342041\n",
      "bn2.module.weight, gradient norm: 0.018741976469755173\n",
      "bn2.module.bias, gradient norm: 0.00979046430438757\n",
      "conv3.bias, gradient norm: 0.018955545499920845\n",
      "conv3.lin.weight, gradient norm: 0.04536765068769455\n",
      "Epoch: 63, Training Loss: 0.1995, Validation Loss: 0.4439, Train Acc: 0.9906, Val Acc: 0.9905, Test Acc: 0.9911\n",
      "conv1.bias, gradient norm: 3.1515665077641586e-10\n",
      "conv1.lin.weight, gradient norm: 0.028775891289114952\n",
      "bn1.module.weight, gradient norm: 0.020196329802274704\n",
      "bn1.module.bias, gradient norm: 0.02016676776111126\n",
      "conv2.bias, gradient norm: 8.97023255674867e-09\n",
      "conv2.lin.weight, gradient norm: 0.033314820379018784\n",
      "bn2.module.weight, gradient norm: 0.017352188006043434\n",
      "bn2.module.bias, gradient norm: 0.016527816653251648\n",
      "conv3.bias, gradient norm: 0.009962739422917366\n",
      "conv3.lin.weight, gradient norm: 0.03293987363576889\n",
      "Epoch: 64, Training Loss: 0.2001, Validation Loss: 0.4357, Train Acc: 0.9903, Val Acc: 0.9901, Test Acc: 0.9908\n",
      "conv1.bias, gradient norm: 2.0030059555420365e-10\n",
      "conv1.lin.weight, gradient norm: 0.029205938801169395\n",
      "bn1.module.weight, gradient norm: 0.014753695577383041\n",
      "bn1.module.bias, gradient norm: 0.016550393775105476\n",
      "conv2.bias, gradient norm: 7.843548033292791e-09\n",
      "conv2.lin.weight, gradient norm: 0.031384605914354324\n",
      "bn2.module.weight, gradient norm: 0.017154483124613762\n",
      "bn2.module.bias, gradient norm: 0.016525661572813988\n",
      "conv3.bias, gradient norm: 0.008886887691915035\n",
      "conv3.lin.weight, gradient norm: 0.04094511270523071\n",
      "Epoch: 65, Training Loss: 0.2021, Validation Loss: 0.4282, Train Acc: 0.9905, Val Acc: 0.9901, Test Acc: 0.9910\n",
      "conv1.bias, gradient norm: 3.834094708388136e-10\n",
      "conv1.lin.weight, gradient norm: 0.02640875056385994\n",
      "bn1.module.weight, gradient norm: 0.02275247313082218\n",
      "bn1.module.bias, gradient norm: 0.023121746256947517\n",
      "conv2.bias, gradient norm: 1.1822722356669146e-08\n",
      "conv2.lin.weight, gradient norm: 0.037730034440755844\n",
      "bn2.module.weight, gradient norm: 0.017799289897084236\n",
      "bn2.module.bias, gradient norm: 0.007287886925041676\n",
      "conv3.bias, gradient norm: 0.015139533206820488\n",
      "conv3.lin.weight, gradient norm: 0.02363373339176178\n",
      "Epoch: 66, Training Loss: 0.1965, Validation Loss: 0.4224, Train Acc: 0.9904, Val Acc: 0.9901, Test Acc: 0.9910\n",
      "conv1.bias, gradient norm: 3.3242505970143554e-10\n",
      "conv1.lin.weight, gradient norm: 0.027707308530807495\n",
      "bn1.module.weight, gradient norm: 0.02433382347226143\n",
      "bn1.module.bias, gradient norm: 0.027495237067341805\n",
      "conv2.bias, gradient norm: 9.961933500335363e-09\n",
      "conv2.lin.weight, gradient norm: 0.0427117645740509\n",
      "bn2.module.weight, gradient norm: 0.017822526395320892\n",
      "bn2.module.bias, gradient norm: 0.008217761293053627\n",
      "conv3.bias, gradient norm: 0.015915559604763985\n",
      "conv3.lin.weight, gradient norm: 0.03287931904196739\n",
      "Epoch: 67, Training Loss: 0.1998, Validation Loss: 0.4217, Train Acc: 0.9899, Val Acc: 0.9898, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 3.2926997239890454e-10\n",
      "conv1.lin.weight, gradient norm: 0.02465079165995121\n",
      "bn1.module.weight, gradient norm: 0.017543625086545944\n",
      "bn1.module.bias, gradient norm: 0.01999572664499283\n",
      "conv2.bias, gradient norm: 7.791355116637533e-09\n",
      "conv2.lin.weight, gradient norm: 0.02782890386879444\n",
      "bn2.module.weight, gradient norm: 0.018055861815810204\n",
      "bn2.module.bias, gradient norm: 0.014512626454234123\n",
      "conv3.bias, gradient norm: 0.009722841903567314\n",
      "conv3.lin.weight, gradient norm: 0.027675975114107132\n",
      "Epoch: 68, Training Loss: 0.1973, Validation Loss: 0.4253, Train Acc: 0.9900, Val Acc: 0.9899, Test Acc: 0.9908\n",
      "conv1.bias, gradient norm: 2.5451921126240507e-10\n",
      "conv1.lin.weight, gradient norm: 0.02257094718515873\n",
      "bn1.module.weight, gradient norm: 0.015615127049386501\n",
      "bn1.module.bias, gradient norm: 0.019511833786964417\n",
      "conv2.bias, gradient norm: 6.926847095911626e-09\n",
      "conv2.lin.weight, gradient norm: 0.02876594103872776\n",
      "bn2.module.weight, gradient norm: 0.015356420539319515\n",
      "bn2.module.bias, gradient norm: 0.011940567754209042\n",
      "conv3.bias, gradient norm: 0.0117842061445117\n",
      "conv3.lin.weight, gradient norm: 0.01987801305949688\n",
      "Epoch: 69, Training Loss: 0.1962, Validation Loss: 0.4317, Train Acc: 0.9907, Val Acc: 0.9905, Test Acc: 0.9912\n",
      "conv1.bias, gradient norm: 5.31624966271238e-10\n",
      "conv1.lin.weight, gradient norm: 0.10051695257425308\n",
      "bn1.module.weight, gradient norm: 0.028829338029026985\n",
      "bn1.module.bias, gradient norm: 0.044637568295001984\n",
      "conv2.bias, gradient norm: 1.64443250127988e-08\n",
      "conv2.lin.weight, gradient norm: 0.06520312279462814\n",
      "bn2.module.weight, gradient norm: 0.016233274713158607\n",
      "bn2.module.bias, gradient norm: 0.0145602747797966\n",
      "conv3.bias, gradient norm: 0.019837310537695885\n",
      "conv3.lin.weight, gradient norm: 0.061831891536712646\n",
      "Epoch: 70, Training Loss: 0.2001, Validation Loss: 0.4465, Train Acc: 0.9894, Val Acc: 0.9894, Test Acc: 0.9901\n",
      "conv1.bias, gradient norm: 3.8096784060748234e-10\n",
      "conv1.lin.weight, gradient norm: 0.04323481768369675\n",
      "bn1.module.weight, gradient norm: 0.029361039400100708\n",
      "bn1.module.bias, gradient norm: 0.0346779003739357\n",
      "conv2.bias, gradient norm: 9.05748454016475e-09\n",
      "conv2.lin.weight, gradient norm: 0.05915430560708046\n",
      "bn2.module.weight, gradient norm: 0.015970692038536072\n",
      "bn2.module.bias, gradient norm: 0.023913512006402016\n",
      "conv3.bias, gradient norm: 0.008838162757456303\n",
      "conv3.lin.weight, gradient norm: 0.06304709613323212\n",
      "Epoch: 71, Training Loss: 0.1980, Validation Loss: 0.4592, Train Acc: 0.9897, Val Acc: 0.9898, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 2.2830554113895118e-10\n",
      "conv1.lin.weight, gradient norm: 0.03702649474143982\n",
      "bn1.module.weight, gradient norm: 0.01823475956916809\n",
      "bn1.module.bias, gradient norm: 0.01863064058125019\n",
      "conv2.bias, gradient norm: 1.1263412424966646e-08\n",
      "conv2.lin.weight, gradient norm: 0.03626985847949982\n",
      "bn2.module.weight, gradient norm: 0.01484142616391182\n",
      "bn2.module.bias, gradient norm: 0.015770016238093376\n",
      "conv3.bias, gradient norm: 0.009482096880674362\n",
      "conv3.lin.weight, gradient norm: 0.02842540293931961\n",
      "Epoch: 72, Training Loss: 0.1949, Validation Loss: 0.4689, Train Acc: 0.9904, Val Acc: 0.9904, Test Acc: 0.9912\n",
      "conv1.bias, gradient norm: 5.287253412866733e-10\n",
      "conv1.lin.weight, gradient norm: 0.09088779240846634\n",
      "bn1.module.weight, gradient norm: 0.04672122746706009\n",
      "bn1.module.bias, gradient norm: 0.06254646927118301\n",
      "conv2.bias, gradient norm: 1.63401878694458e-08\n",
      "conv2.lin.weight, gradient norm: 0.092955581843853\n",
      "bn2.module.weight, gradient norm: 0.018840674310922623\n",
      "bn2.module.bias, gradient norm: 0.019654154777526855\n",
      "conv3.bias, gradient norm: 0.02189788781106472\n",
      "conv3.lin.weight, gradient norm: 0.06059162691235542\n",
      "Epoch: 73, Training Loss: 0.1980, Validation Loss: 0.4748, Train Acc: 0.9883, Val Acc: 0.9883, Test Acc: 0.9888\n",
      "conv1.bias, gradient norm: 3.9671571583355103e-10\n",
      "conv1.lin.weight, gradient norm: 0.05820945277810097\n",
      "bn1.module.weight, gradient norm: 0.03890283405780792\n",
      "bn1.module.bias, gradient norm: 0.028603382408618927\n",
      "conv2.bias, gradient norm: 8.00624544439188e-09\n",
      "conv2.lin.weight, gradient norm: 0.0627339631319046\n",
      "bn2.module.weight, gradient norm: 0.017805932089686394\n",
      "bn2.module.bias, gradient norm: 0.022411825135350227\n",
      "conv3.bias, gradient norm: 0.008064961060881615\n",
      "conv3.lin.weight, gradient norm: 0.07395114749670029\n",
      "Epoch: 74, Training Loss: 0.1947, Validation Loss: 0.4752, Train Acc: 0.9882, Val Acc: 0.9883, Test Acc: 0.9888\n",
      "conv1.bias, gradient norm: 2.4230187301022e-10\n",
      "conv1.lin.weight, gradient norm: 0.04203938692808151\n",
      "bn1.module.weight, gradient norm: 0.02914140745997429\n",
      "bn1.module.bias, gradient norm: 0.02419220097362995\n",
      "conv2.bias, gradient norm: 8.305987897472278e-09\n",
      "conv2.lin.weight, gradient norm: 0.04150397703051567\n",
      "bn2.module.weight, gradient norm: 0.01543143205344677\n",
      "bn2.module.bias, gradient norm: 0.0176593866199255\n",
      "conv3.bias, gradient norm: 0.008578537032008171\n",
      "conv3.lin.weight, gradient norm: 0.03572594374418259\n",
      "Epoch: 75, Training Loss: 0.1941, Validation Loss: 0.4711, Train Acc: 0.9900, Val Acc: 0.9900, Test Acc: 0.9908\n",
      "conv1.bias, gradient norm: 9.4428365127186e-10\n",
      "conv1.lin.weight, gradient norm: 0.20942458510398865\n",
      "bn1.module.weight, gradient norm: 0.09020790457725525\n",
      "bn1.module.bias, gradient norm: 0.08260482549667358\n",
      "conv2.bias, gradient norm: 2.7675628899714866e-08\n",
      "conv2.lin.weight, gradient norm: 0.18176214396953583\n",
      "bn2.module.weight, gradient norm: 0.02461378276348114\n",
      "bn2.module.bias, gradient norm: 0.04557648301124573\n",
      "conv3.bias, gradient norm: 0.03405774384737015\n",
      "conv3.lin.weight, gradient norm: 0.14275266230106354\n",
      "Epoch: 76, Training Loss: 0.2014, Validation Loss: 0.4711, Train Acc: 0.9868, Val Acc: 0.9868, Test Acc: 0.9874\n",
      "conv1.bias, gradient norm: 6.69112487639012e-10\n",
      "conv1.lin.weight, gradient norm: 0.06862352788448334\n",
      "bn1.module.weight, gradient norm: 0.04393777251243591\n",
      "bn1.module.bias, gradient norm: 0.046301521360874176\n",
      "conv2.bias, gradient norm: 1.6853430651053714e-08\n",
      "conv2.lin.weight, gradient norm: 0.10028762370347977\n",
      "bn2.module.weight, gradient norm: 0.02048342674970627\n",
      "bn2.module.bias, gradient norm: 0.033283911645412445\n",
      "conv3.bias, gradient norm: 0.01403291616588831\n",
      "conv3.lin.weight, gradient norm: 0.11775662004947662\n",
      "Epoch: 77, Training Loss: 0.2012, Validation Loss: 0.4768, Train Acc: 0.9872, Val Acc: 0.9873, Test Acc: 0.9877\n",
      "conv1.bias, gradient norm: 5.936083291580019e-10\n",
      "conv1.lin.weight, gradient norm: 0.0689091756939888\n",
      "bn1.module.weight, gradient norm: 0.03892532363533974\n",
      "bn1.module.bias, gradient norm: 0.044387079775333405\n",
      "conv2.bias, gradient norm: 1.3665133025142495e-08\n",
      "conv2.lin.weight, gradient norm: 0.08750881254673004\n",
      "bn2.module.weight, gradient norm: 0.01591319963335991\n",
      "bn2.module.bias, gradient norm: 0.031803544610738754\n",
      "conv3.bias, gradient norm: 0.011257062666118145\n",
      "conv3.lin.weight, gradient norm: 0.0934487134218216\n",
      "Epoch: 78, Training Loss: 0.2027, Validation Loss: 0.4875, Train Acc: 0.9892, Val Acc: 0.9895, Test Acc: 0.9900\n",
      "conv1.bias, gradient norm: 2.8216745606712834e-10\n",
      "conv1.lin.weight, gradient norm: 0.04010612517595291\n",
      "bn1.module.weight, gradient norm: 0.03909542039036751\n",
      "bn1.module.bias, gradient norm: 0.023108219727873802\n",
      "conv2.bias, gradient norm: 1.8353475184085255e-08\n",
      "conv2.lin.weight, gradient norm: 0.056493502110242844\n",
      "bn2.module.weight, gradient norm: 0.015392917208373547\n",
      "bn2.module.bias, gradient norm: 0.012079150415956974\n",
      "conv3.bias, gradient norm: 0.017559437081217766\n",
      "conv3.lin.weight, gradient norm: 0.04408377408981323\n",
      "Epoch: 79, Training Loss: 0.1966, Validation Loss: 0.4985, Train Acc: 0.9905, Val Acc: 0.9907, Test Acc: 0.9912\n",
      "conv1.bias, gradient norm: 2.4402393439260095e-09\n",
      "conv1.lin.weight, gradient norm: 0.3712460696697235\n",
      "bn1.module.weight, gradient norm: 0.12501148879528046\n",
      "bn1.module.bias, gradient norm: 0.19343449175357819\n",
      "conv2.bias, gradient norm: 5.495365940078045e-08\n",
      "conv2.lin.weight, gradient norm: 0.268049955368042\n",
      "bn2.module.weight, gradient norm: 0.032556962221860886\n",
      "bn2.module.bias, gradient norm: 0.07797618955373764\n",
      "conv3.bias, gradient norm: 0.05048656091094017\n",
      "conv3.lin.weight, gradient norm: 0.21453656256198883\n",
      "Epoch: 80, Training Loss: 0.2150, Validation Loss: 0.5141, Train Acc: 0.9840, Val Acc: 0.9840, Test Acc: 0.9846\n",
      "conv1.bias, gradient norm: 4.6161766076302513e-10\n",
      "conv1.lin.weight, gradient norm: 0.0655713826417923\n",
      "bn1.module.weight, gradient norm: 0.04151511192321777\n",
      "bn1.module.bias, gradient norm: 0.04764813929796219\n",
      "conv2.bias, gradient norm: 1.7905707139220794e-08\n",
      "conv2.lin.weight, gradient norm: 0.10148366540670395\n",
      "bn2.module.weight, gradient norm: 0.0201862845569849\n",
      "bn2.module.bias, gradient norm: 0.03455090895295143\n",
      "conv3.bias, gradient norm: 0.014847183600068092\n",
      "conv3.lin.weight, gradient norm: 0.12473903596401215\n",
      "Epoch: 81, Training Loss: 0.2084, Validation Loss: 0.5377, Train Acc: 0.9816, Val Acc: 0.9819, Test Acc: 0.9826\n",
      "conv1.bias, gradient norm: 9.578848825242403e-10\n",
      "conv1.lin.weight, gradient norm: 0.07411538064479828\n",
      "bn1.module.weight, gradient norm: 0.06167010962963104\n",
      "bn1.module.bias, gradient norm: 0.0573684386909008\n",
      "conv2.bias, gradient norm: 2.2543453326306917e-08\n",
      "conv2.lin.weight, gradient norm: 0.1207527220249176\n",
      "bn2.module.weight, gradient norm: 0.02639836259186268\n",
      "bn2.module.bias, gradient norm: 0.041229259222745895\n",
      "conv3.bias, gradient norm: 0.021897435188293457\n",
      "conv3.lin.weight, gradient norm: 0.16590657830238342\n",
      "Epoch: 82, Training Loss: 0.2170, Validation Loss: 0.5493, Train Acc: 0.9841, Val Acc: 0.9844, Test Acc: 0.9853\n",
      "conv1.bias, gradient norm: 5.8652765977385e-10\n",
      "conv1.lin.weight, gradient norm: 0.036372024565935135\n",
      "bn1.module.weight, gradient norm: 0.05071040987968445\n",
      "bn1.module.bias, gradient norm: 0.04189613088965416\n",
      "conv2.bias, gradient norm: 1.6532887059383938e-08\n",
      "conv2.lin.weight, gradient norm: 0.06721152365207672\n",
      "bn2.module.weight, gradient norm: 0.015185127034783363\n",
      "bn2.module.bias, gradient norm: 0.019263960421085358\n",
      "conv3.bias, gradient norm: 0.014263827353715897\n",
      "conv3.lin.weight, gradient norm: 0.03215738385915756\n",
      "Epoch: 83, Training Loss: 0.2051, Validation Loss: 0.5297, Train Acc: 0.9864, Val Acc: 0.9866, Test Acc: 0.9874\n",
      "conv1.bias, gradient norm: 7.453246908539768e-10\n",
      "conv1.lin.weight, gradient norm: 0.23538881540298462\n",
      "bn1.module.weight, gradient norm: 0.1371164470911026\n",
      "bn1.module.bias, gradient norm: 0.13517870008945465\n",
      "conv2.bias, gradient norm: 2.2726441173404055e-08\n",
      "conv2.lin.weight, gradient norm: 0.1919262558221817\n",
      "bn2.module.weight, gradient norm: 0.027067508548498154\n",
      "bn2.module.bias, gradient norm: 0.04820459336042404\n",
      "conv3.bias, gradient norm: 0.04498683288693428\n",
      "conv3.lin.weight, gradient norm: 0.19567734003067017\n",
      "Epoch: 84, Training Loss: 0.2182, Validation Loss: 0.4668, Train Acc: 0.9858, Val Acc: 0.9862, Test Acc: 0.9870\n",
      "conv1.bias, gradient norm: 3.676857429635305e-10\n",
      "conv1.lin.weight, gradient norm: 0.048515159636735916\n",
      "bn1.module.weight, gradient norm: 0.031885456293821335\n",
      "bn1.module.bias, gradient norm: 0.03600219637155533\n",
      "conv2.bias, gradient norm: 1.564264096032275e-08\n",
      "conv2.lin.weight, gradient norm: 0.06513968855142593\n",
      "bn2.module.weight, gradient norm: 0.016984006389975548\n",
      "bn2.module.bias, gradient norm: 0.02603459358215332\n",
      "conv3.bias, gradient norm: 0.01082330197095871\n",
      "conv3.lin.weight, gradient norm: 0.029537837952375412\n",
      "Epoch: 85, Training Loss: 0.2048, Validation Loss: 0.4016, Train Acc: 0.9867, Val Acc: 0.9870, Test Acc: 0.9875\n",
      "conv1.bias, gradient norm: 2.662094156224981e-10\n",
      "conv1.lin.weight, gradient norm: 0.06544587761163712\n",
      "bn1.module.weight, gradient norm: 0.0366557240486145\n",
      "bn1.module.bias, gradient norm: 0.04333766922354698\n",
      "conv2.bias, gradient norm: 8.766740222654335e-09\n",
      "conv2.lin.weight, gradient norm: 0.06594528257846832\n",
      "bn2.module.weight, gradient norm: 0.020487315952777863\n",
      "bn2.module.bias, gradient norm: 0.028811821714043617\n",
      "conv3.bias, gradient norm: 0.00855281576514244\n",
      "conv3.lin.weight, gradient norm: 0.06979591399431229\n",
      "Epoch: 86, Training Loss: 0.2042, Validation Loss: 0.3491, Train Acc: 0.9894, Val Acc: 0.9894, Test Acc: 0.9903\n",
      "conv1.bias, gradient norm: 5.017312121324835e-10\n",
      "conv1.lin.weight, gradient norm: 0.13672927021980286\n",
      "bn1.module.weight, gradient norm: 0.07399702817201614\n",
      "bn1.module.bias, gradient norm: 0.07537075877189636\n",
      "conv2.bias, gradient norm: 1.4933160485952612e-08\n",
      "conv2.lin.weight, gradient norm: 0.0908215194940567\n",
      "bn2.module.weight, gradient norm: 0.017089491710066795\n",
      "bn2.module.bias, gradient norm: 0.025023631751537323\n",
      "conv3.bias, gradient norm: 0.019017869606614113\n",
      "conv3.lin.weight, gradient norm: 0.06528105586767197\n",
      "Epoch: 87, Training Loss: 0.2047, Validation Loss: 0.3225, Train Acc: 0.9891, Val Acc: 0.9891, Test Acc: 0.9896\n",
      "conv1.bias, gradient norm: 2.5235363798614685e-10\n",
      "conv1.lin.weight, gradient norm: 0.029742784798145294\n",
      "bn1.module.weight, gradient norm: 0.03561221808195114\n",
      "bn1.module.bias, gradient norm: 0.02437804453074932\n",
      "conv2.bias, gradient norm: 1.2769099555498542e-08\n",
      "conv2.lin.weight, gradient norm: 0.05352884903550148\n",
      "bn2.module.weight, gradient norm: 0.02016081102192402\n",
      "bn2.module.bias, gradient norm: 0.025868233293294907\n",
      "conv3.bias, gradient norm: 0.00885099172592163\n",
      "conv3.lin.weight, gradient norm: 0.059831924736499786\n",
      "Epoch: 88, Training Loss: 0.2031, Validation Loss: 0.3230, Train Acc: 0.9898, Val Acc: 0.9900, Test Acc: 0.9903\n",
      "conv1.bias, gradient norm: 2.360752149321854e-10\n",
      "conv1.lin.weight, gradient norm: 0.03476540744304657\n",
      "bn1.module.weight, gradient norm: 0.032876234501600266\n",
      "bn1.module.bias, gradient norm: 0.0221035685390234\n",
      "conv2.bias, gradient norm: 1.075734079591939e-08\n",
      "conv2.lin.weight, gradient norm: 0.04459887370467186\n",
      "bn2.module.weight, gradient norm: 0.017477527260780334\n",
      "bn2.module.bias, gradient norm: 0.022855406627058983\n",
      "conv3.bias, gradient norm: 0.008068693801760674\n",
      "conv3.lin.weight, gradient norm: 0.047610633075237274\n",
      "Epoch: 89, Training Loss: 0.1982, Validation Loss: 0.3375, Train Acc: 0.9904, Val Acc: 0.9907, Test Acc: 0.9910\n",
      "conv1.bias, gradient norm: 2.535963383731854e-10\n",
      "conv1.lin.weight, gradient norm: 0.038811177015304565\n",
      "bn1.module.weight, gradient norm: 0.031540948897600174\n",
      "bn1.module.bias, gradient norm: 0.02935147099196911\n",
      "conv2.bias, gradient norm: 9.20285359029549e-09\n",
      "conv2.lin.weight, gradient norm: 0.03486127406358719\n",
      "bn2.module.weight, gradient norm: 0.01683497242629528\n",
      "bn2.module.bias, gradient norm: 0.012060540728271008\n",
      "conv3.bias, gradient norm: 0.016496174037456512\n",
      "conv3.lin.weight, gradient norm: 0.037847261875867844\n",
      "Epoch: 90, Training Loss: 0.1974, Validation Loss: 0.3484, Train Acc: 0.9904, Val Acc: 0.9905, Test Acc: 0.9910\n",
      "conv1.bias, gradient norm: 3.3752459160929504e-10\n",
      "conv1.lin.weight, gradient norm: 0.06099676340818405\n",
      "bn1.module.weight, gradient norm: 0.055415503680706024\n",
      "bn1.module.bias, gradient norm: 0.048049960285425186\n",
      "conv2.bias, gradient norm: 1.1347227157898487e-08\n",
      "conv2.lin.weight, gradient norm: 0.05901764705777168\n",
      "bn2.module.weight, gradient norm: 0.01788671500980854\n",
      "bn2.module.bias, gradient norm: 0.016917306929826736\n",
      "conv3.bias, gradient norm: 0.022988134995102882\n",
      "conv3.lin.weight, gradient norm: 0.05957527458667755\n",
      "Epoch: 91, Training Loss: 0.1985, Validation Loss: 0.3473, Train Acc: 0.9894, Val Acc: 0.9896, Test Acc: 0.9899\n",
      "conv1.bias, gradient norm: 2.67708438750347e-10\n",
      "conv1.lin.weight, gradient norm: 0.03798501938581467\n",
      "bn1.module.weight, gradient norm: 0.030528314411640167\n",
      "bn1.module.bias, gradient norm: 0.018124207854270935\n",
      "conv2.bias, gradient norm: 6.846800015836152e-09\n",
      "conv2.lin.weight, gradient norm: 0.03855784609913826\n",
      "bn2.module.weight, gradient norm: 0.017019985243678093\n",
      "bn2.module.bias, gradient norm: 0.015205754898488522\n",
      "conv3.bias, gradient norm: 0.010210761800408363\n",
      "conv3.lin.weight, gradient norm: 0.0273390281945467\n",
      "Epoch: 92, Training Loss: 0.1937, Validation Loss: 0.3398, Train Acc: 0.9892, Val Acc: 0.9895, Test Acc: 0.9896\n",
      "conv1.bias, gradient norm: 3.1522096044511727e-10\n",
      "conv1.lin.weight, gradient norm: 0.04304661229252815\n",
      "bn1.module.weight, gradient norm: 0.02862352691590786\n",
      "bn1.module.bias, gradient norm: 0.017466701567173004\n",
      "conv2.bias, gradient norm: 8.551629626651902e-09\n",
      "conv2.lin.weight, gradient norm: 0.03583141416311264\n",
      "bn2.module.weight, gradient norm: 0.017636308446526527\n",
      "bn2.module.bias, gradient norm: 0.014651237986981869\n",
      "conv3.bias, gradient norm: 0.00878110621124506\n",
      "conv3.lin.weight, gradient norm: 0.03258634731173515\n",
      "Epoch: 93, Training Loss: 0.1951, Validation Loss: 0.3345, Train Acc: 0.9900, Val Acc: 0.9902, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 3.362450040622633e-10\n",
      "conv1.lin.weight, gradient norm: 0.12547804415225983\n",
      "bn1.module.weight, gradient norm: 0.0511012077331543\n",
      "bn1.module.bias, gradient norm: 0.039858926087617874\n",
      "conv2.bias, gradient norm: 8.628979308866747e-09\n",
      "conv2.lin.weight, gradient norm: 0.06108122318983078\n",
      "bn2.module.weight, gradient norm: 0.021537598222494125\n",
      "bn2.module.bias, gradient norm: 0.019114989787340164\n",
      "conv3.bias, gradient norm: 0.022333357483148575\n",
      "conv3.lin.weight, gradient norm: 0.058813270181417465\n",
      "Epoch: 94, Training Loss: 0.1948, Validation Loss: 0.3307, Train Acc: 0.9893, Val Acc: 0.9896, Test Acc: 0.9898\n",
      "conv1.bias, gradient norm: 3.6015490589846877e-10\n",
      "conv1.lin.weight, gradient norm: 0.035752810537815094\n",
      "bn1.module.weight, gradient norm: 0.0292877946048975\n",
      "bn1.module.bias, gradient norm: 0.021687332540750504\n",
      "conv2.bias, gradient norm: 5.586817231062469e-09\n",
      "conv2.lin.weight, gradient norm: 0.04000953584909439\n",
      "bn2.module.weight, gradient norm: 0.015518218278884888\n",
      "bn2.module.bias, gradient norm: 0.014789491891860962\n",
      "conv3.bias, gradient norm: 0.008689473383128643\n",
      "conv3.lin.weight, gradient norm: 0.043299462646245956\n",
      "Epoch: 95, Training Loss: 0.1949, Validation Loss: 0.3320, Train Acc: 0.9896, Val Acc: 0.9898, Test Acc: 0.9902\n",
      "conv1.bias, gradient norm: 2.5040028384104573e-10\n",
      "conv1.lin.weight, gradient norm: 0.025770386680960655\n",
      "bn1.module.weight, gradient norm: 0.017886463552713394\n",
      "bn1.module.bias, gradient norm: 0.014390155673027039\n",
      "conv2.bias, gradient norm: 6.002375041447294e-09\n",
      "conv2.lin.weight, gradient norm: 0.031247565522789955\n",
      "bn2.module.weight, gradient norm: 0.014412987045943737\n",
      "bn2.module.bias, gradient norm: 0.010000113397836685\n",
      "conv3.bias, gradient norm: 0.011266510933637619\n",
      "conv3.lin.weight, gradient norm: 0.017427833750844002\n",
      "Epoch: 96, Training Loss: 0.1926, Validation Loss: 0.3383, Train Acc: 0.9902, Val Acc: 0.9902, Test Acc: 0.9908\n",
      "conv1.bias, gradient norm: 5.553174586836462e-10\n",
      "conv1.lin.weight, gradient norm: 0.12244682013988495\n",
      "bn1.module.weight, gradient norm: 0.0547536201775074\n",
      "bn1.module.bias, gradient norm: 0.04857520014047623\n",
      "conv2.bias, gradient norm: 1.2364504975437285e-08\n",
      "conv2.lin.weight, gradient norm: 0.07968980073928833\n",
      "bn2.module.weight, gradient norm: 0.01883043348789215\n",
      "bn2.module.bias, gradient norm: 0.0247145127505064\n",
      "conv3.bias, gradient norm: 0.022885940968990326\n",
      "conv3.lin.weight, gradient norm: 0.07395558059215546\n",
      "Epoch: 97, Training Loss: 0.1981, Validation Loss: 0.3497, Train Acc: 0.9876, Val Acc: 0.9876, Test Acc: 0.9883\n",
      "conv1.bias, gradient norm: 5.988800011458295e-10\n",
      "conv1.lin.weight, gradient norm: 0.06872435659170151\n",
      "bn1.module.weight, gradient norm: 0.05180326849222183\n",
      "bn1.module.bias, gradient norm: 0.03868561610579491\n",
      "conv2.bias, gradient norm: 8.959779584927219e-09\n",
      "conv2.lin.weight, gradient norm: 0.06706026941537857\n",
      "bn2.module.weight, gradient norm: 0.018943721428513527\n",
      "bn2.module.bias, gradient norm: 0.026440443471074104\n",
      "conv3.bias, gradient norm: 0.010288299061357975\n",
      "conv3.lin.weight, gradient norm: 0.09972219914197922\n",
      "Epoch: 98, Training Loss: 0.1962, Validation Loss: 0.3570, Train Acc: 0.9876, Val Acc: 0.9877, Test Acc: 0.9885\n",
      "conv1.bias, gradient norm: 6.042324418586986e-10\n",
      "conv1.lin.weight, gradient norm: 0.06490721553564072\n",
      "bn1.module.weight, gradient norm: 0.03644183650612831\n",
      "bn1.module.bias, gradient norm: 0.03133264556527138\n",
      "conv2.bias, gradient norm: 5.947828451979831e-09\n",
      "conv2.lin.weight, gradient norm: 0.04815010353922844\n",
      "bn2.module.weight, gradient norm: 0.018089408054947853\n",
      "bn2.module.bias, gradient norm: 0.021156078204512596\n",
      "conv3.bias, gradient norm: 0.007975456304848194\n",
      "conv3.lin.weight, gradient norm: 0.06601237505674362\n",
      "Epoch: 99, Training Loss: 0.1915, Validation Loss: 0.3590, Train Acc: 0.9895, Val Acc: 0.9893, Test Acc: 0.9901\n",
      "conv1.bias, gradient norm: 8.438591492243575e-10\n",
      "conv1.lin.weight, gradient norm: 0.18708960711956024\n",
      "bn1.module.weight, gradient norm: 0.11652138829231262\n",
      "bn1.module.bias, gradient norm: 0.0919303223490715\n",
      "conv2.bias, gradient norm: 2.2620683992613522e-08\n",
      "conv2.lin.weight, gradient norm: 0.11022629588842392\n",
      "bn2.module.weight, gradient norm: 0.024264680221676826\n",
      "bn2.module.bias, gradient norm: 0.03988739103078842\n",
      "conv3.bias, gradient norm: 0.030664127320051193\n",
      "conv3.lin.weight, gradient norm: 0.10907627642154694\n",
      "Epoch: 100, Training Loss: 0.1942, Validation Loss: 0.3563, Train Acc: 0.9870, Val Acc: 0.9870, Test Acc: 0.9875\n",
      "conv1.bias, gradient norm: 4.115665863668738e-10\n",
      "conv1.lin.weight, gradient norm: 0.05326925963163376\n",
      "bn1.module.weight, gradient norm: 0.04618507996201515\n",
      "bn1.module.bias, gradient norm: 0.030303552746772766\n",
      "conv2.bias, gradient norm: 7.768843346411813e-09\n",
      "conv2.lin.weight, gradient norm: 0.05340787023305893\n",
      "bn2.module.weight, gradient norm: 0.015657253563404083\n",
      "bn2.module.bias, gradient norm: 0.023003505542874336\n",
      "conv3.bias, gradient norm: 0.007942445576190948\n",
      "conv3.lin.weight, gradient norm: 0.06231942027807236\n",
      "Epoch: 101, Training Loss: 0.1895, Validation Loss: 0.3548, Train Acc: 0.9868, Val Acc: 0.9868, Test Acc: 0.9871\n",
      "conv1.bias, gradient norm: 4.879083248532368e-10\n",
      "conv1.lin.weight, gradient norm: 0.05338284373283386\n",
      "bn1.module.weight, gradient norm: 0.050378888845443726\n",
      "bn1.module.bias, gradient norm: 0.03176438435912132\n",
      "conv2.bias, gradient norm: 7.493798470648017e-09\n",
      "conv2.lin.weight, gradient norm: 0.056440893560647964\n",
      "bn2.module.weight, gradient norm: 0.014358701184391975\n",
      "bn2.module.bias, gradient norm: 0.02385651506483555\n",
      "conv3.bias, gradient norm: 0.008193056099116802\n",
      "conv3.lin.weight, gradient norm: 0.05765765160322189\n",
      "Epoch: 102, Training Loss: 0.1927, Validation Loss: 0.3527, Train Acc: 0.9873, Val Acc: 0.9873, Test Acc: 0.9878\n",
      "conv1.bias, gradient norm: 3.020042826928915e-10\n",
      "conv1.lin.weight, gradient norm: 0.04089292138814926\n",
      "bn1.module.weight, gradient norm: 0.03058980219066143\n",
      "bn1.module.bias, gradient norm: 0.017803778871893883\n",
      "conv2.bias, gradient norm: 4.976122180977427e-09\n",
      "conv2.lin.weight, gradient norm: 0.039206139743328094\n",
      "bn2.module.weight, gradient norm: 0.013543015345931053\n",
      "bn2.module.bias, gradient norm: 0.01730538345873356\n",
      "conv3.bias, gradient norm: 0.00900205783545971\n",
      "conv3.lin.weight, gradient norm: 0.0270970668643713\n",
      "Epoch: 103, Training Loss: 0.1893, Validation Loss: 0.3526, Train Acc: 0.9884, Val Acc: 0.9887, Test Acc: 0.9887\n",
      "conv1.bias, gradient norm: 5.26021226576745e-10\n",
      "conv1.lin.weight, gradient norm: 0.08859571069478989\n",
      "bn1.module.weight, gradient norm: 0.04387202113866806\n",
      "bn1.module.bias, gradient norm: 0.039427366107702255\n",
      "conv2.bias, gradient norm: 1.2118469783217733e-08\n",
      "conv2.lin.weight, gradient norm: 0.068115234375\n",
      "bn2.module.weight, gradient norm: 0.016936568543314934\n",
      "bn2.module.bias, gradient norm: 0.017551662400364876\n",
      "conv3.bias, gradient norm: 0.021959729492664337\n",
      "conv3.lin.weight, gradient norm: 0.07697032392024994\n",
      "Epoch: 104, Training Loss: 0.1938, Validation Loss: 0.3493, Train Acc: 0.9888, Val Acc: 0.9891, Test Acc: 0.9892\n",
      "conv1.bias, gradient norm: 3.0367000580788783e-10\n",
      "conv1.lin.weight, gradient norm: 0.06001972779631615\n",
      "bn1.module.weight, gradient norm: 0.043169744312763214\n",
      "bn1.module.bias, gradient norm: 0.0340447761118412\n",
      "conv2.bias, gradient norm: 1.0482421153312771e-08\n",
      "conv2.lin.weight, gradient norm: 0.054407998919487\n",
      "bn2.module.weight, gradient norm: 0.017368393018841743\n",
      "bn2.module.bias, gradient norm: 0.01280944887548685\n",
      "conv3.bias, gradient norm: 0.02092622220516205\n",
      "conv3.lin.weight, gradient norm: 0.06214887648820877\n",
      "Epoch: 105, Training Loss: 0.1895, Validation Loss: 0.3439, Train Acc: 0.9884, Val Acc: 0.9887, Test Acc: 0.9888\n",
      "conv1.bias, gradient norm: 1.6960124971099333e-10\n",
      "conv1.lin.weight, gradient norm: 0.032429106533527374\n",
      "bn1.module.weight, gradient norm: 0.02131781168282032\n",
      "bn1.module.bias, gradient norm: 0.013463535346090794\n",
      "conv2.bias, gradient norm: 7.3297248270876025e-09\n",
      "conv2.lin.weight, gradient norm: 0.02865021675825119\n",
      "bn2.module.weight, gradient norm: 0.015089545398950577\n",
      "bn2.module.bias, gradient norm: 0.013376371003687382\n",
      "conv3.bias, gradient norm: 0.010940661653876305\n",
      "conv3.lin.weight, gradient norm: 0.020406711846590042\n",
      "Epoch: 106, Training Loss: 0.1891, Validation Loss: 0.3399, Train Acc: 0.9881, Val Acc: 0.9884, Test Acc: 0.9885\n",
      "conv1.bias, gradient norm: 3.5389746688707646e-10\n",
      "conv1.lin.weight, gradient norm: 0.03967629373073578\n",
      "bn1.module.weight, gradient norm: 0.02714153192937374\n",
      "bn1.module.bias, gradient norm: 0.020835261791944504\n",
      "conv2.bias, gradient norm: 7.540463364819061e-09\n",
      "conv2.lin.weight, gradient norm: 0.0408082976937294\n",
      "bn2.module.weight, gradient norm: 0.0167223010212183\n",
      "bn2.module.bias, gradient norm: 0.018709199503064156\n",
      "conv3.bias, gradient norm: 0.008290860801935196\n",
      "conv3.lin.weight, gradient norm: 0.041459839791059494\n",
      "Epoch: 107, Training Loss: 0.1871, Validation Loss: 0.3383, Train Acc: 0.9883, Val Acc: 0.9885, Test Acc: 0.9887\n",
      "conv1.bias, gradient norm: 2.3520077552241503e-10\n",
      "conv1.lin.weight, gradient norm: 0.029102759435772896\n",
      "bn1.module.weight, gradient norm: 0.02705296501517296\n",
      "bn1.module.bias, gradient norm: 0.017118023708462715\n",
      "conv2.bias, gradient norm: 7.464921125688306e-09\n",
      "conv2.lin.weight, gradient norm: 0.03168070316314697\n",
      "bn2.module.weight, gradient norm: 0.017796600237488747\n",
      "bn2.module.bias, gradient norm: 0.015430664643645287\n",
      "conv3.bias, gradient norm: 0.009098799899220467\n",
      "conv3.lin.weight, gradient norm: 0.024957815185189247\n",
      "Epoch: 108, Training Loss: 0.1868, Validation Loss: 0.3393, Train Acc: 0.9888, Val Acc: 0.9890, Test Acc: 0.9894\n",
      "conv1.bias, gradient norm: 2.798563603079174e-10\n",
      "conv1.lin.weight, gradient norm: 0.05971807613968849\n",
      "bn1.module.weight, gradient norm: 0.03240349888801575\n",
      "bn1.module.bias, gradient norm: 0.0310893002897501\n",
      "conv2.bias, gradient norm: 1.2379850033994444e-08\n",
      "conv2.lin.weight, gradient norm: 0.04303857684135437\n",
      "bn2.module.weight, gradient norm: 0.018658939749002457\n",
      "bn2.module.bias, gradient norm: 0.011299182660877705\n",
      "conv3.bias, gradient norm: 0.01645033247768879\n",
      "conv3.lin.weight, gradient norm: 0.04761703684926033\n",
      "Epoch: 109, Training Loss: 0.1840, Validation Loss: 0.3422, Train Acc: 0.9888, Val Acc: 0.9889, Test Acc: 0.9893\n",
      "conv1.bias, gradient norm: 3.7828162824382616e-10\n",
      "conv1.lin.weight, gradient norm: 0.05682540684938431\n",
      "bn1.module.weight, gradient norm: 0.024495281279087067\n",
      "bn1.module.bias, gradient norm: 0.027455313131213188\n",
      "conv2.bias, gradient norm: 1.3933732390114528e-08\n",
      "conv2.lin.weight, gradient norm: 0.035792622715234756\n",
      "bn2.module.weight, gradient norm: 0.01732643134891987\n",
      "bn2.module.bias, gradient norm: 0.008546710945665836\n",
      "conv3.bias, gradient norm: 0.01527599897235632\n",
      "conv3.lin.weight, gradient norm: 0.04304948449134827\n",
      "Epoch: 110, Training Loss: 0.1838, Validation Loss: 0.3452, Train Acc: 0.9883, Val Acc: 0.9883, Test Acc: 0.9886\n",
      "conv1.bias, gradient norm: 3.3110642005951263e-10\n",
      "conv1.lin.weight, gradient norm: 0.035639453679323196\n",
      "bn1.module.weight, gradient norm: 0.026132039725780487\n",
      "bn1.module.bias, gradient norm: 0.02211005985736847\n",
      "conv2.bias, gradient norm: 6.7501684242188276e-09\n",
      "conv2.lin.weight, gradient norm: 0.03556470945477486\n",
      "bn2.module.weight, gradient norm: 0.016376229003071785\n",
      "bn2.module.bias, gradient norm: 0.016155818477272987\n",
      "conv3.bias, gradient norm: 0.008389662951231003\n",
      "conv3.lin.weight, gradient norm: 0.03172473981976509\n",
      "Epoch: 111, Training Loss: 0.1851, Validation Loss: 0.3476, Train Acc: 0.9883, Val Acc: 0.9883, Test Acc: 0.9886\n",
      "conv1.bias, gradient norm: 3.194574327292088e-10\n",
      "conv1.lin.weight, gradient norm: 0.04033873602747917\n",
      "bn1.module.weight, gradient norm: 0.020933156833052635\n",
      "bn1.module.bias, gradient norm: 0.01865014061331749\n",
      "conv2.bias, gradient norm: 6.745632941118629e-09\n",
      "conv2.lin.weight, gradient norm: 0.03433705121278763\n",
      "bn2.module.weight, gradient norm: 0.01612531952559948\n",
      "bn2.module.bias, gradient norm: 0.016322124749422073\n",
      "conv3.bias, gradient norm: 0.008346221409738064\n",
      "conv3.lin.weight, gradient norm: 0.03542128950357437\n",
      "Epoch: 112, Training Loss: 0.1836, Validation Loss: 0.3512, Train Acc: 0.9888, Val Acc: 0.9889, Test Acc: 0.9892\n",
      "conv1.bias, gradient norm: 4.162697131437909e-10\n",
      "conv1.lin.weight, gradient norm: 0.04846079647541046\n",
      "bn1.module.weight, gradient norm: 0.02205607108771801\n",
      "bn1.module.bias, gradient norm: 0.02406642958521843\n",
      "conv2.bias, gradient norm: 1.0135910777364643e-08\n",
      "conv2.lin.weight, gradient norm: 0.03216240927577019\n",
      "bn2.module.weight, gradient norm: 0.01718510314822197\n",
      "bn2.module.bias, gradient norm: 0.005989634897559881\n",
      "conv3.bias, gradient norm: 0.013196847401559353\n",
      "conv3.lin.weight, gradient norm: 0.02242744155228138\n",
      "Epoch: 113, Training Loss: 0.1848, Validation Loss: 0.3523, Train Acc: 0.9889, Val Acc: 0.9888, Test Acc: 0.9892\n",
      "conv1.bias, gradient norm: 4.635984929279857e-10\n",
      "conv1.lin.weight, gradient norm: 0.06878560036420822\n",
      "bn1.module.weight, gradient norm: 0.025297226384282112\n",
      "bn1.module.bias, gradient norm: 0.0325511209666729\n",
      "conv2.bias, gradient norm: 1.3030267531632944e-08\n",
      "conv2.lin.weight, gradient norm: 0.04172661155462265\n",
      "bn2.module.weight, gradient norm: 0.018114762380719185\n",
      "bn2.module.bias, gradient norm: 0.008222277276217937\n",
      "conv3.bias, gradient norm: 0.014828430488705635\n",
      "conv3.lin.weight, gradient norm: 0.02860173210501671\n",
      "Epoch: 114, Training Loss: 0.1831, Validation Loss: 0.3508, Train Acc: 0.9883, Val Acc: 0.9882, Test Acc: 0.9887\n",
      "conv1.bias, gradient norm: 3.330753450825341e-10\n",
      "conv1.lin.weight, gradient norm: 0.04382092133164406\n",
      "bn1.module.weight, gradient norm: 0.021323909983038902\n",
      "bn1.module.bias, gradient norm: 0.019389694556593895\n",
      "conv2.bias, gradient norm: 9.459681038492818e-09\n",
      "conv2.lin.weight, gradient norm: 0.03209047019481659\n",
      "bn2.module.weight, gradient norm: 0.015637297183275223\n",
      "bn2.module.bias, gradient norm: 0.015990158542990685\n",
      "conv3.bias, gradient norm: 0.007862585596740246\n",
      "conv3.lin.weight, gradient norm: 0.03888648748397827\n",
      "Epoch: 115, Training Loss: 0.1849, Validation Loss: 0.3481, Train Acc: 0.9884, Val Acc: 0.9882, Test Acc: 0.9888\n",
      "conv1.bias, gradient norm: 2.7284993708853733e-10\n",
      "conv1.lin.weight, gradient norm: 0.039351388812065125\n",
      "bn1.module.weight, gradient norm: 0.01955382525920868\n",
      "bn1.module.bias, gradient norm: 0.017998510971665382\n",
      "conv2.bias, gradient norm: 7.130348755879368e-09\n",
      "conv2.lin.weight, gradient norm: 0.031127003952860832\n",
      "bn2.module.weight, gradient norm: 0.015734650194644928\n",
      "bn2.module.bias, gradient norm: 0.015241974964737892\n",
      "conv3.bias, gradient norm: 0.008132148534059525\n",
      "conv3.lin.weight, gradient norm: 0.0382528118789196\n",
      "Epoch: 116, Training Loss: 0.1837, Validation Loss: 0.3481, Train Acc: 0.9889, Val Acc: 0.9889, Test Acc: 0.9894\n",
      "conv1.bias, gradient norm: 4.6547762866389064e-10\n",
      "conv1.lin.weight, gradient norm: 0.07449740171432495\n",
      "bn1.module.weight, gradient norm: 0.03014933317899704\n",
      "bn1.module.bias, gradient norm: 0.034843847155570984\n",
      "conv2.bias, gradient norm: 1.351904277413496e-08\n",
      "conv2.lin.weight, gradient norm: 0.0556938610970974\n",
      "bn2.module.weight, gradient norm: 0.017981288954615593\n",
      "bn2.module.bias, gradient norm: 0.010507260449230671\n",
      "conv3.bias, gradient norm: 0.014914139173924923\n",
      "conv3.lin.weight, gradient norm: 0.035547975450754166\n",
      "Epoch: 117, Training Loss: 0.1836, Validation Loss: 0.3507, Train Acc: 0.9884, Val Acc: 0.9884, Test Acc: 0.9890\n",
      "conv1.bias, gradient norm: 2.485305572452745e-10\n",
      "conv1.lin.weight, gradient norm: 0.02423124760389328\n",
      "bn1.module.weight, gradient norm: 0.01667400635778904\n",
      "bn1.module.bias, gradient norm: 0.00968543067574501\n",
      "conv2.bias, gradient norm: 7.698212733942e-09\n",
      "conv2.lin.weight, gradient norm: 0.01892886683344841\n",
      "bn2.module.weight, gradient norm: 0.015517191030085087\n",
      "bn2.module.bias, gradient norm: 0.007941313087940216\n",
      "conv3.bias, gradient norm: 0.010228138417005539\n",
      "conv3.lin.weight, gradient norm: 0.017903625965118408\n",
      "Epoch: 118, Training Loss: 0.1858, Validation Loss: 0.3548, Train Acc: 0.9880, Val Acc: 0.9879, Test Acc: 0.9884\n",
      "conv1.bias, gradient norm: 3.9960909581360227e-10\n",
      "conv1.lin.weight, gradient norm: 0.04922379553318024\n",
      "bn1.module.weight, gradient norm: 0.021510999649763107\n",
      "bn1.module.bias, gradient norm: 0.02091471664607525\n",
      "conv2.bias, gradient norm: 6.763494653228008e-09\n",
      "conv2.lin.weight, gradient norm: 0.0351334847509861\n",
      "bn2.module.weight, gradient norm: 0.016031168401241302\n",
      "bn2.module.bias, gradient norm: 0.01500727515667677\n",
      "conv3.bias, gradient norm: 0.00813058391213417\n",
      "conv3.lin.weight, gradient norm: 0.03700993210077286\n",
      "Epoch: 119, Training Loss: 0.1836, Validation Loss: 0.3582, Train Acc: 0.9884, Val Acc: 0.9884, Test Acc: 0.9888\n",
      "conv1.bias, gradient norm: 3.2476729638908353e-10\n",
      "conv1.lin.weight, gradient norm: 0.031206151470541954\n",
      "bn1.module.weight, gradient norm: 0.01652585156261921\n",
      "bn1.module.bias, gradient norm: 0.013493471778929234\n",
      "conv2.bias, gradient norm: 1.0338412792521012e-08\n",
      "conv2.lin.weight, gradient norm: 0.020051002502441406\n",
      "bn2.module.weight, gradient norm: 0.016036180779337883\n",
      "bn2.module.bias, gradient norm: 0.006532103288918734\n",
      "conv3.bias, gradient norm: 0.011843973770737648\n",
      "conv3.lin.weight, gradient norm: 0.020558761432766914\n",
      "Epoch: 120, Training Loss: 0.1819, Validation Loss: 0.3619, Train Acc: 0.9885, Val Acc: 0.9885, Test Acc: 0.9889\n",
      "conv1.bias, gradient norm: 5.14748466073911e-10\n",
      "conv1.lin.weight, gradient norm: 0.04083387181162834\n",
      "bn1.module.weight, gradient norm: 0.022974295541644096\n",
      "bn1.module.bias, gradient norm: 0.018872909247875214\n",
      "conv2.bias, gradient norm: 1.4805179304744343e-08\n",
      "conv2.lin.weight, gradient norm: 0.02908027544617653\n",
      "bn2.module.weight, gradient norm: 0.016963975504040718\n",
      "bn2.module.bias, gradient norm: 0.005916771478950977\n",
      "conv3.bias, gradient norm: 0.012982421554625034\n",
      "conv3.lin.weight, gradient norm: 0.02398914098739624\n",
      "Epoch: 121, Training Loss: 0.1814, Validation Loss: 0.3640, Train Acc: 0.9881, Val Acc: 0.9881, Test Acc: 0.9885\n",
      "conv1.bias, gradient norm: 3.8141409475223043e-10\n",
      "conv1.lin.weight, gradient norm: 0.049226514995098114\n",
      "bn1.module.weight, gradient norm: 0.017769860103726387\n",
      "bn1.module.bias, gradient norm: 0.016544677317142487\n",
      "conv2.bias, gradient norm: 8.916017257831754e-09\n",
      "conv2.lin.weight, gradient norm: 0.03297346085309982\n",
      "bn2.module.weight, gradient norm: 0.01553978119045496\n",
      "bn2.module.bias, gradient norm: 0.012774727307260036\n",
      "conv3.bias, gradient norm: 0.010097079910337925\n",
      "conv3.lin.weight, gradient norm: 0.02112145535647869\n",
      "Epoch: 122, Training Loss: 0.1808, Validation Loss: 0.3657, Train Acc: 0.9883, Val Acc: 0.9884, Test Acc: 0.9888\n",
      "conv1.bias, gradient norm: 4.1603195888306743e-10\n",
      "conv1.lin.weight, gradient norm: 0.05013910308480263\n",
      "bn1.module.weight, gradient norm: 0.022056296467781067\n",
      "bn1.module.bias, gradient norm: 0.023840300738811493\n",
      "conv2.bias, gradient norm: 1.3967685674742825e-08\n",
      "conv2.lin.weight, gradient norm: 0.030834896489977837\n",
      "bn2.module.weight, gradient norm: 0.015873173251748085\n",
      "bn2.module.bias, gradient norm: 0.006696884520351887\n",
      "conv3.bias, gradient norm: 0.014053172431886196\n",
      "conv3.lin.weight, gradient norm: 0.03114827163517475\n",
      "Epoch: 123, Training Loss: 0.1807, Validation Loss: 0.3669, Train Acc: 0.9880, Val Acc: 0.9882, Test Acc: 0.9883\n",
      "conv1.bias, gradient norm: 4.916323459447369e-10\n",
      "conv1.lin.weight, gradient norm: 0.038719672709703445\n",
      "bn1.module.weight, gradient norm: 0.023315677419304848\n",
      "bn1.module.bias, gradient norm: 0.015490137040615082\n",
      "conv2.bias, gradient norm: 9.728529093422367e-09\n",
      "conv2.lin.weight, gradient norm: 0.03004135936498642\n",
      "bn2.module.weight, gradient norm: 0.0174097940325737\n",
      "bn2.module.bias, gradient norm: 0.011694641783833504\n",
      "conv3.bias, gradient norm: 0.009846838191151619\n",
      "conv3.lin.weight, gradient norm: 0.021943051367998123\n",
      "Epoch: 124, Training Loss: 0.1774, Validation Loss: 0.3667, Train Acc: 0.9884, Val Acc: 0.9886, Test Acc: 0.9889\n",
      "conv1.bias, gradient norm: 7.323324724417546e-10\n",
      "conv1.lin.weight, gradient norm: 0.08243293315172195\n",
      "bn1.module.weight, gradient norm: 0.028430970385670662\n",
      "bn1.module.bias, gradient norm: 0.03415298834443092\n",
      "conv2.bias, gradient norm: 1.5539253439555978e-08\n",
      "conv2.lin.weight, gradient norm: 0.043850842863321304\n",
      "bn2.module.weight, gradient norm: 0.01708773709833622\n",
      "bn2.module.bias, gradient norm: 0.007319695316255093\n",
      "conv3.bias, gradient norm: 0.014919977635145187\n",
      "conv3.lin.weight, gradient norm: 0.0321727991104126\n",
      "Epoch: 125, Training Loss: 0.1801, Validation Loss: 0.3674, Train Acc: 0.9876, Val Acc: 0.9876, Test Acc: 0.9881\n",
      "conv1.bias, gradient norm: 4.6170542389312175e-10\n",
      "conv1.lin.weight, gradient norm: 0.08448087424039841\n",
      "bn1.module.weight, gradient norm: 0.037183139473199844\n",
      "bn1.module.bias, gradient norm: 0.03650486096739769\n",
      "conv2.bias, gradient norm: 1.0404774819505747e-08\n",
      "conv2.lin.weight, gradient norm: 0.06693356484174728\n",
      "bn2.module.weight, gradient norm: 0.015397613868117332\n",
      "bn2.module.bias, gradient norm: 0.020192226395010948\n",
      "conv3.bias, gradient norm: 0.008060302585363388\n",
      "conv3.lin.weight, gradient norm: 0.04287436977028847\n",
      "Epoch: 126, Training Loss: 0.1805, Validation Loss: 0.3710, Train Acc: 0.9882, Val Acc: 0.9884, Test Acc: 0.9888\n",
      "conv1.bias, gradient norm: 4.4935233312060063e-10\n",
      "conv1.lin.weight, gradient norm: 0.07885702699422836\n",
      "bn1.module.weight, gradient norm: 0.03800414502620697\n",
      "bn1.module.bias, gradient norm: 0.03342314437031746\n",
      "conv2.bias, gradient norm: 1.836818341871549e-08\n",
      "conv2.lin.weight, gradient norm: 0.05210040137171745\n",
      "bn2.module.weight, gradient norm: 0.016268907114863396\n",
      "bn2.module.bias, gradient norm: 0.007285330910235643\n",
      "conv3.bias, gradient norm: 0.015314274467527866\n",
      "conv3.lin.weight, gradient norm: 0.03751927614212036\n",
      "Epoch: 127, Training Loss: 0.1811, Validation Loss: 0.3648, Train Acc: 0.9874, Val Acc: 0.9874, Test Acc: 0.9880\n",
      "conv1.bias, gradient norm: 6.353463311015162e-10\n",
      "conv1.lin.weight, gradient norm: 0.06608060747385025\n",
      "bn1.module.weight, gradient norm: 0.02994500659406185\n",
      "bn1.module.bias, gradient norm: 0.025060584768652916\n",
      "conv2.bias, gradient norm: 1.1930951338001705e-08\n",
      "conv2.lin.weight, gradient norm: 0.04324185103178024\n",
      "bn2.module.weight, gradient norm: 0.015268059447407722\n",
      "bn2.module.bias, gradient norm: 0.01568702422082424\n",
      "conv3.bias, gradient norm: 0.008856159634888172\n",
      "conv3.lin.weight, gradient norm: 0.025792649015784264\n",
      "Epoch: 128, Training Loss: 0.1788, Validation Loss: 0.3630, Train Acc: 0.9883, Val Acc: 0.9883, Test Acc: 0.9887\n",
      "conv1.bias, gradient norm: 1.9905737058678596e-09\n",
      "conv1.lin.weight, gradient norm: 0.2029723823070526\n",
      "bn1.module.weight, gradient norm: 0.06750813126564026\n",
      "bn1.module.bias, gradient norm: 0.07730135321617126\n",
      "conv2.bias, gradient norm: 3.0206244616692857e-08\n",
      "conv2.lin.weight, gradient norm: 0.1104128509759903\n",
      "bn2.module.weight, gradient norm: 0.01881997287273407\n",
      "bn2.module.bias, gradient norm: 0.01574019528925419\n",
      "conv3.bias, gradient norm: 0.017765842378139496\n",
      "conv3.lin.weight, gradient norm: 0.0614558607339859\n",
      "Epoch: 129, Training Loss: 0.1804, Validation Loss: 0.3642, Train Acc: 0.9866, Val Acc: 0.9862, Test Acc: 0.9871\n",
      "conv1.bias, gradient norm: 1.0046998788482142e-09\n",
      "conv1.lin.weight, gradient norm: 0.14960205554962158\n",
      "bn1.module.weight, gradient norm: 0.0854891687631607\n",
      "bn1.module.bias, gradient norm: 0.06961380690336227\n",
      "conv2.bias, gradient norm: 1.9693402464326937e-08\n",
      "conv2.lin.weight, gradient norm: 0.13278910517692566\n",
      "bn2.module.weight, gradient norm: 0.015854978933930397\n",
      "bn2.module.bias, gradient norm: 0.02970789559185505\n",
      "conv3.bias, gradient norm: 0.01245805062353611\n",
      "conv3.lin.weight, gradient norm: 0.10709403455257416\n",
      "Epoch: 130, Training Loss: 0.1918, Validation Loss: 0.3678, Train Acc: 0.9892, Val Acc: 0.9893, Test Acc: 0.9900\n",
      "conv1.bias, gradient norm: 7.630768794619769e-10\n",
      "conv1.lin.weight, gradient norm: 0.17196860909461975\n",
      "bn1.module.weight, gradient norm: 0.06894221901893616\n",
      "bn1.module.bias, gradient norm: 0.053319986909627914\n",
      "conv2.bias, gradient norm: 2.9055128081267867e-08\n",
      "conv2.lin.weight, gradient norm: 0.07359667122364044\n",
      "bn2.module.weight, gradient norm: 0.017172040417790413\n",
      "bn2.module.bias, gradient norm: 0.008583379909396172\n",
      "conv3.bias, gradient norm: 0.015952330082654953\n",
      "conv3.lin.weight, gradient norm: 0.03441205620765686\n",
      "Epoch: 131, Training Loss: 0.1869, Validation Loss: 0.3624, Train Acc: 0.9881, Val Acc: 0.9883, Test Acc: 0.9887\n",
      "conv1.bias, gradient norm: 6.345014513797764e-10\n",
      "conv1.lin.weight, gradient norm: 0.08908383548259735\n",
      "bn1.module.weight, gradient norm: 0.04534917697310448\n",
      "bn1.module.bias, gradient norm: 0.03839743137359619\n",
      "conv2.bias, gradient norm: 1.3752770477992726e-08\n",
      "conv2.lin.weight, gradient norm: 0.0737169161438942\n",
      "bn2.module.weight, gradient norm: 0.014187974855303764\n",
      "bn2.module.bias, gradient norm: 0.017140811309218407\n",
      "conv3.bias, gradient norm: 0.00799105130136013\n",
      "conv3.lin.weight, gradient norm: 0.032295748591423035\n",
      "Epoch: 132, Training Loss: 0.1822, Validation Loss: 0.3619, Train Acc: 0.9890, Val Acc: 0.9891, Test Acc: 0.9897\n",
      "conv1.bias, gradient norm: 8.462622269611586e-10\n",
      "conv1.lin.weight, gradient norm: 0.23973040282726288\n",
      "bn1.module.weight, gradient norm: 0.13098053634166718\n",
      "bn1.module.bias, gradient norm: 0.11741562932729721\n",
      "conv2.bias, gradient norm: 4.0670897760719527e-08\n",
      "conv2.lin.weight, gradient norm: 0.22288082540035248\n",
      "bn2.module.weight, gradient norm: 0.0215046014636755\n",
      "bn2.module.bias, gradient norm: 0.032414283603429794\n",
      "conv3.bias, gradient norm: 0.02433369867503643\n",
      "conv3.lin.weight, gradient norm: 0.10154223442077637\n",
      "Epoch: 133, Training Loss: 0.1883, Validation Loss: 0.3463, Train Acc: 0.9882, Val Acc: 0.9883, Test Acc: 0.9886\n",
      "conv1.bias, gradient norm: 7.245105626552117e-10\n",
      "conv1.lin.weight, gradient norm: 0.1304849535226822\n",
      "bn1.module.weight, gradient norm: 0.09473070502281189\n",
      "bn1.module.bias, gradient norm: 0.07218436896800995\n",
      "conv2.bias, gradient norm: 1.53409214220801e-08\n",
      "conv2.lin.weight, gradient norm: 0.13170094788074493\n",
      "bn2.module.weight, gradient norm: 0.016157642006874084\n",
      "bn2.module.bias, gradient norm: 0.030561605468392372\n",
      "conv3.bias, gradient norm: 0.012142190709710121\n",
      "conv3.lin.weight, gradient norm: 0.10951574891805649\n",
      "Epoch: 134, Training Loss: 0.1921, Validation Loss: 0.3540, Train Acc: 0.9908, Val Acc: 0.9908, Test Acc: 0.9915\n",
      "conv1.bias, gradient norm: 9.634280040415888e-10\n",
      "conv1.lin.weight, gradient norm: 0.31067919731140137\n",
      "bn1.module.weight, gradient norm: 0.12499880790710449\n",
      "bn1.module.bias, gradient norm: 0.12217043340206146\n",
      "conv2.bias, gradient norm: 3.977845963731852e-08\n",
      "conv2.lin.weight, gradient norm: 0.12953530251979828\n",
      "bn2.module.weight, gradient norm: 0.018649302423000336\n",
      "bn2.module.bias, gradient norm: 0.015490041114389896\n",
      "conv3.bias, gradient norm: 0.019666720181703568\n",
      "conv3.lin.weight, gradient norm: 0.03800872340798378\n",
      "Epoch: 135, Training Loss: 0.1950, Validation Loss: 0.3494, Train Acc: 0.9889, Val Acc: 0.9891, Test Acc: 0.9895\n",
      "conv1.bias, gradient norm: 1.0735290434382705e-09\n",
      "conv1.lin.weight, gradient norm: 0.13182353973388672\n",
      "bn1.module.weight, gradient norm: 0.08755441009998322\n",
      "bn1.module.bias, gradient norm: 0.08433351665735245\n",
      "conv2.bias, gradient norm: 1.591625853336609e-08\n",
      "conv2.lin.weight, gradient norm: 0.12726899981498718\n",
      "bn2.module.weight, gradient norm: 0.014280121773481369\n",
      "bn2.module.bias, gradient norm: 0.027031391859054565\n",
      "conv3.bias, gradient norm: 0.009501039050519466\n",
      "conv3.lin.weight, gradient norm: 0.08498097956180573\n",
      "Epoch: 136, Training Loss: 0.1905, Validation Loss: 0.3580, Train Acc: 0.9904, Val Acc: 0.9903, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 3.193595943251637e-10\n",
      "conv1.lin.weight, gradient norm: 0.048229627311229706\n",
      "bn1.module.weight, gradient norm: 0.036374665796756744\n",
      "bn1.module.bias, gradient norm: 0.027166327461600304\n",
      "conv2.bias, gradient norm: 2.0366067943200505e-08\n",
      "conv2.lin.weight, gradient norm: 0.059737637639045715\n",
      "bn2.module.weight, gradient norm: 0.013997954316437244\n",
      "bn2.module.bias, gradient norm: 0.01044588815420866\n",
      "conv3.bias, gradient norm: 0.013953822664916515\n",
      "conv3.lin.weight, gradient norm: 0.03221447765827179\n",
      "Epoch: 137, Training Loss: 0.1864, Validation Loss: 0.3737, Train Acc: 0.9905, Val Acc: 0.9901, Test Acc: 0.9907\n",
      "conv1.bias, gradient norm: 1.7992674017364152e-09\n",
      "conv1.lin.weight, gradient norm: 0.36392807960510254\n",
      "bn1.module.weight, gradient norm: 0.2456086277961731\n",
      "bn1.module.bias, gradient norm: 0.2285320907831192\n",
      "conv2.bias, gradient norm: 4.84993911697984e-08\n",
      "conv2.lin.weight, gradient norm: 0.2937060594558716\n",
      "bn2.module.weight, gradient norm: 0.026775570586323738\n",
      "bn2.module.bias, gradient norm: 0.05203462019562721\n",
      "conv3.bias, gradient norm: 0.03765187785029411\n",
      "conv3.lin.weight, gradient norm: 0.16758333146572113\n",
      "Epoch: 138, Training Loss: 0.2009, Validation Loss: 0.3423, Train Acc: 0.9870, Val Acc: 0.9872, Test Acc: 0.9875\n",
      "conv1.bias, gradient norm: 7.379293287534949e-10\n",
      "conv1.lin.weight, gradient norm: 0.0888577476143837\n",
      "bn1.module.weight, gradient norm: 0.12453436106443405\n",
      "bn1.module.bias, gradient norm: 0.08267118036746979\n",
      "conv2.bias, gradient norm: 1.5399590935771812e-08\n",
      "conv2.lin.weight, gradient norm: 0.15605586767196655\n",
      "bn2.module.weight, gradient norm: 0.02052069641649723\n",
      "bn2.module.bias, gradient norm: 0.03494882583618164\n",
      "conv3.bias, gradient norm: 0.01805342175066471\n",
      "conv3.lin.weight, gradient norm: 0.15764369070529938\n",
      "Epoch: 139, Training Loss: 0.2043, Validation Loss: 0.3358, Train Acc: 0.9867, Val Acc: 0.9869, Test Acc: 0.9871\n",
      "conv1.bias, gradient norm: 5.54186085910402e-10\n",
      "conv1.lin.weight, gradient norm: 0.07276713848114014\n",
      "bn1.module.weight, gradient norm: 0.10181546956300735\n",
      "bn1.module.bias, gradient norm: 0.06798723340034485\n",
      "conv2.bias, gradient norm: 2.764668138866e-08\n",
      "conv2.lin.weight, gradient norm: 0.15262062847614288\n",
      "bn2.module.weight, gradient norm: 0.02059127576649189\n",
      "bn2.module.bias, gradient norm: 0.03476494178175926\n",
      "conv3.bias, gradient norm: 0.018639706075191498\n",
      "conv3.lin.weight, gradient norm: 0.16899681091308594\n",
      "Epoch: 140, Training Loss: 0.2075, Validation Loss: 0.3418, Train Acc: 0.9887, Val Acc: 0.9887, Test Acc: 0.9890\n",
      "conv1.bias, gradient norm: 3.6043684703557233e-10\n",
      "conv1.lin.weight, gradient norm: 0.06316638737916946\n",
      "bn1.module.weight, gradient norm: 0.04450727626681328\n",
      "bn1.module.bias, gradient norm: 0.029482506215572357\n",
      "conv2.bias, gradient norm: 1.1670215016579277e-08\n",
      "conv2.lin.weight, gradient norm: 0.09790732711553574\n",
      "bn2.module.weight, gradient norm: 0.013283072970807552\n",
      "bn2.module.bias, gradient norm: 0.022997861728072166\n",
      "conv3.bias, gradient norm: 0.009666642174124718\n",
      "conv3.lin.weight, gradient norm: 0.040101490914821625\n",
      "Epoch: 141, Training Loss: 0.1993, Validation Loss: 0.3321, Train Acc: 0.9906, Val Acc: 0.9904, Test Acc: 0.9909\n",
      "conv1.bias, gradient norm: 8.750766222753725e-10\n",
      "conv1.lin.weight, gradient norm: 0.09526855498552322\n",
      "bn1.module.weight, gradient norm: 0.09633399546146393\n",
      "bn1.module.bias, gradient norm: 0.06826204806566238\n",
      "conv2.bias, gradient norm: 1.9122923688996707e-08\n",
      "conv2.lin.weight, gradient norm: 0.11052090674638748\n",
      "bn2.module.weight, gradient norm: 0.014111325144767761\n",
      "bn2.module.bias, gradient norm: 0.010542665608227253\n",
      "conv3.bias, gradient norm: 0.028117477893829346\n",
      "conv3.lin.weight, gradient norm: 0.10279285907745361\n",
      "Epoch: 142, Training Loss: 0.1969, Validation Loss: 0.3031, Train Acc: 0.9910, Val Acc: 0.9910, Test Acc: 0.9915\n",
      "conv1.bias, gradient norm: 9.563417835423138e-10\n",
      "conv1.lin.weight, gradient norm: 0.1466091126203537\n",
      "bn1.module.weight, gradient norm: 0.11179465800523758\n",
      "bn1.module.bias, gradient norm: 0.10628144443035126\n",
      "conv2.bias, gradient norm: 1.8778015586917718e-08\n",
      "conv2.lin.weight, gradient norm: 0.16615815460681915\n",
      "bn2.module.weight, gradient norm: 0.0182558111846447\n",
      "bn2.module.bias, gradient norm: 0.03288458287715912\n",
      "conv3.bias, gradient norm: 0.03443238139152527\n",
      "conv3.lin.weight, gradient norm: 0.17097650468349457\n",
      "Epoch: 143, Training Loss: 0.1961, Validation Loss: 0.2884, Train Acc: 0.9892, Val Acc: 0.9891, Test Acc: 0.9899\n",
      "conv1.bias, gradient norm: 3.368283152394014e-10\n",
      "conv1.lin.weight, gradient norm: 0.043584275990724564\n",
      "bn1.module.weight, gradient norm: 0.046501580625772476\n",
      "bn1.module.bias, gradient norm: 0.02783324383199215\n",
      "conv2.bias, gradient norm: 1.55188715211807e-08\n",
      "conv2.lin.weight, gradient norm: 0.0700453445315361\n",
      "bn2.module.weight, gradient norm: 0.013949740678071976\n",
      "bn2.module.bias, gradient norm: 0.018630722537636757\n",
      "conv3.bias, gradient norm: 0.012308076024055481\n",
      "conv3.lin.weight, gradient norm: 0.030960002914071083\n",
      "Epoch: 144, Training Loss: 0.1938, Validation Loss: 0.2900, Train Acc: 0.9885, Val Acc: 0.9883, Test Acc: 0.9890\n",
      "conv1.bias, gradient norm: 5.044560880129723e-10\n",
      "conv1.lin.weight, gradient norm: 0.05013883858919144\n",
      "bn1.module.weight, gradient norm: 0.05944158509373665\n",
      "bn1.module.bias, gradient norm: 0.03836865723133087\n",
      "conv2.bias, gradient norm: 1.455236287029038e-08\n",
      "conv2.lin.weight, gradient norm: 0.08941613882780075\n",
      "bn2.module.weight, gradient norm: 0.01776787079870701\n",
      "bn2.module.bias, gradient norm: 0.027393942698836327\n",
      "conv3.bias, gradient norm: 0.00805866438895464\n",
      "conv3.lin.weight, gradient norm: 0.04801085591316223\n",
      "Epoch: 145, Training Loss: 0.1945, Validation Loss: 0.2903, Train Acc: 0.9888, Val Acc: 0.9887, Test Acc: 0.9894\n",
      "conv1.bias, gradient norm: 3.0347188650914347e-10\n",
      "conv1.lin.weight, gradient norm: 0.041840266436338425\n",
      "bn1.module.weight, gradient norm: 0.04166554659605026\n",
      "bn1.module.bias, gradient norm: 0.029078783467411995\n",
      "conv2.bias, gradient norm: 1.2385219072541531e-08\n",
      "conv2.lin.weight, gradient norm: 0.07575950026512146\n",
      "bn2.module.weight, gradient norm: 0.01735897734761238\n",
      "bn2.module.bias, gradient norm: 0.02314692549407482\n",
      "conv3.bias, gradient norm: 0.009387452155351639\n",
      "conv3.lin.weight, gradient norm: 0.035159047693014145\n",
      "Epoch: 146, Training Loss: 0.1922, Validation Loss: 0.2892, Train Acc: 0.9896, Val Acc: 0.9894, Test Acc: 0.9900\n",
      "conv1.bias, gradient norm: 2.999058779096231e-10\n",
      "conv1.lin.weight, gradient norm: 0.046316880732774734\n",
      "bn1.module.weight, gradient norm: 0.05354032292962074\n",
      "bn1.module.bias, gradient norm: 0.03764451667666435\n",
      "conv2.bias, gradient norm: 1.542571581580887e-08\n",
      "conv2.lin.weight, gradient norm: 0.06265296787023544\n",
      "bn2.module.weight, gradient norm: 0.019578957930207253\n",
      "bn2.module.bias, gradient norm: 0.011239542625844479\n",
      "conv3.bias, gradient norm: 0.018300872296094894\n",
      "conv3.lin.weight, gradient norm: 0.042788758873939514\n",
      "Epoch: 147, Training Loss: 0.1924, Validation Loss: 0.2924, Train Acc: 0.9898, Val Acc: 0.9897, Test Acc: 0.9901\n",
      "conv1.bias, gradient norm: 5.953377790746117e-10\n",
      "conv1.lin.weight, gradient norm: 0.09426868706941605\n",
      "bn1.module.weight, gradient norm: 0.08560700714588165\n",
      "bn1.module.bias, gradient norm: 0.07346704602241516\n",
      "conv2.bias, gradient norm: 1.7195610269027384e-08\n",
      "conv2.lin.weight, gradient norm: 0.10367108881473541\n",
      "bn2.module.weight, gradient norm: 0.02137129381299019\n",
      "bn2.module.bias, gradient norm: 0.02454182505607605\n",
      "conv3.bias, gradient norm: 0.02355874329805374\n",
      "conv3.lin.weight, gradient norm: 0.08324341475963593\n",
      "Epoch: 148, Training Loss: 0.1926, Validation Loss: 0.2965, Train Acc: 0.9883, Val Acc: 0.9885, Test Acc: 0.9889\n",
      "conv1.bias, gradient norm: 3.6898825661602075e-10\n",
      "conv1.lin.weight, gradient norm: 0.03785788267850876\n",
      "bn1.module.weight, gradient norm: 0.03954300284385681\n",
      "bn1.module.bias, gradient norm: 0.03206489235162735\n",
      "conv2.bias, gradient norm: 8.920228999897972e-09\n",
      "conv2.lin.weight, gradient norm: 0.04638294875621796\n",
      "bn2.module.weight, gradient norm: 0.01480980683118105\n",
      "bn2.module.bias, gradient norm: 0.015373683534562588\n",
      "conv3.bias, gradient norm: 0.009937169030308723\n",
      "conv3.lin.weight, gradient norm: 0.019193440675735474\n",
      "Epoch: 149, Training Loss: 0.1928, Validation Loss: 0.2894, Train Acc: 0.9879, Val Acc: 0.9878, Test Acc: 0.9884\n",
      "conv1.bias, gradient norm: 3.8857250750368166e-10\n",
      "conv1.lin.weight, gradient norm: 0.04265367239713669\n",
      "bn1.module.weight, gradient norm: 0.049208663403987885\n",
      "bn1.module.bias, gradient norm: 0.03397693112492561\n",
      "conv2.bias, gradient norm: 9.604808504093398e-09\n",
      "conv2.lin.weight, gradient norm: 0.0596911683678627\n",
      "bn2.module.weight, gradient norm: 0.016052598133683205\n",
      "bn2.module.bias, gradient norm: 0.01866258680820465\n",
      "conv3.bias, gradient norm: 0.007853742688894272\n",
      "conv3.lin.weight, gradient norm: 0.05100812390446663\n",
      "Epoch: 150, Training Loss: 0.1914, Validation Loss: 0.2803, Train Acc: 0.9884, Val Acc: 0.9883, Test Acc: 0.9886\n",
      "conv1.bias, gradient norm: 3.09861525327193e-10\n",
      "conv1.lin.weight, gradient norm: 0.03358416259288788\n",
      "bn1.module.weight, gradient norm: 0.03009546361863613\n",
      "bn1.module.bias, gradient norm: 0.019392438232898712\n",
      "conv2.bias, gradient norm: 1.0064129973841318e-08\n",
      "conv2.lin.weight, gradient norm: 0.04092548042535782\n",
      "bn2.module.weight, gradient norm: 0.01649453118443489\n",
      "bn2.module.bias, gradient norm: 0.01367561612278223\n",
      "conv3.bias, gradient norm: 0.008100561797618866\n",
      "conv3.lin.weight, gradient norm: 0.03816550225019455\n",
      "Epoch: 151, Training Loss: 0.1906, Validation Loss: 0.2759, Train Acc: 0.9891, Val Acc: 0.9888, Test Acc: 0.9893\n",
      "conv1.bias, gradient norm: 3.278319282706832e-10\n",
      "conv1.lin.weight, gradient norm: 0.04499027505517006\n",
      "bn1.module.weight, gradient norm: 0.02847655676305294\n",
      "bn1.module.bias, gradient norm: 0.023336751386523247\n",
      "conv2.bias, gradient norm: 1.4475725507168136e-08\n",
      "conv2.lin.weight, gradient norm: 0.033495284616947174\n",
      "bn2.module.weight, gradient norm: 0.019644519314169884\n",
      "bn2.module.bias, gradient norm: 0.008143967017531395\n",
      "conv3.bias, gradient norm: 0.011043597012758255\n",
      "conv3.lin.weight, gradient norm: 0.01952536217868328\n",
      "Epoch: 152, Training Loss: 0.1860, Validation Loss: 0.2732, Train Acc: 0.9896, Val Acc: 0.9893, Test Acc: 0.9899\n",
      "conv1.bias, gradient norm: 3.3627237105982033e-10\n",
      "conv1.lin.weight, gradient norm: 0.05088399723172188\n",
      "bn1.module.weight, gradient norm: 0.040567297488451004\n",
      "bn1.module.bias, gradient norm: 0.03662492334842682\n",
      "conv2.bias, gradient norm: 1.8669865653464512e-08\n",
      "conv2.lin.weight, gradient norm: 0.05415401980280876\n",
      "bn2.module.weight, gradient norm: 0.019735880196094513\n",
      "bn2.module.bias, gradient norm: 0.012315019965171814\n",
      "conv3.bias, gradient norm: 0.014105167239904404\n",
      "conv3.lin.weight, gradient norm: 0.023462967947125435\n",
      "Epoch: 153, Training Loss: 0.1922, Validation Loss: 0.2711, Train Acc: 0.9897, Val Acc: 0.9894, Test Acc: 0.9900\n",
      "conv1.bias, gradient norm: 2.2348044248499122e-10\n",
      "conv1.lin.weight, gradient norm: 0.03169798105955124\n",
      "bn1.module.weight, gradient norm: 0.024707138538360596\n",
      "bn1.module.bias, gradient norm: 0.022001570090651512\n",
      "conv2.bias, gradient norm: 1.3975537171972974e-08\n",
      "conv2.lin.weight, gradient norm: 0.03786736726760864\n",
      "bn2.module.weight, gradient norm: 0.019554926082491875\n",
      "bn2.module.bias, gradient norm: 0.007786309812217951\n",
      "conv3.bias, gradient norm: 0.011715355329215527\n",
      "conv3.lin.weight, gradient norm: 0.018981270492076874\n",
      "Epoch: 154, Training Loss: 0.1888, Validation Loss: 0.2709, Train Acc: 0.9894, Val Acc: 0.9893, Test Acc: 0.9897\n",
      "conv1.bias, gradient norm: 1.9506925241774553e-10\n",
      "conv1.lin.weight, gradient norm: 0.02630631811916828\n",
      "bn1.module.weight, gradient norm: 0.017740445211529732\n",
      "bn1.module.bias, gradient norm: 0.012000729329884052\n",
      "conv2.bias, gradient norm: 6.840886079828579e-09\n",
      "conv2.lin.weight, gradient norm: 0.02423727698624134\n",
      "bn2.module.weight, gradient norm: 0.017157243564724922\n",
      "bn2.module.bias, gradient norm: 0.007935278117656708\n",
      "conv3.bias, gradient norm: 0.009822373278439045\n",
      "conv3.lin.weight, gradient norm: 0.029025480151176453\n",
      "Epoch: 155, Training Loss: 0.1869, Validation Loss: 0.2706, Train Acc: 0.9894, Val Acc: 0.9893, Test Acc: 0.9898\n",
      "conv1.bias, gradient norm: 2.5495716649004407e-10\n",
      "conv1.lin.weight, gradient norm: 0.03070881962776184\n",
      "bn1.module.weight, gradient norm: 0.020264873281121254\n",
      "bn1.module.bias, gradient norm: 0.015715360641479492\n",
      "conv2.bias, gradient norm: 1.06922701803569e-08\n",
      "conv2.lin.weight, gradient norm: 0.029288724064826965\n",
      "bn2.module.weight, gradient norm: 0.016335440799593925\n",
      "bn2.module.bias, gradient norm: 0.010759186930954456\n",
      "conv3.bias, gradient norm: 0.009353846311569214\n",
      "conv3.lin.weight, gradient norm: 0.032983556389808655\n",
      "Epoch: 156, Training Loss: 0.1871, Validation Loss: 0.2694, Train Acc: 0.9895, Val Acc: 0.9895, Test Acc: 0.9898\n",
      "conv1.bias, gradient norm: 3.9528505468844344e-10\n",
      "conv1.lin.weight, gradient norm: 0.02444700337946415\n",
      "bn1.module.weight, gradient norm: 0.016849922016263008\n",
      "bn1.module.bias, gradient norm: 0.0117571372538805\n",
      "conv2.bias, gradient norm: 1.0457940291530576e-08\n",
      "conv2.lin.weight, gradient norm: 0.027544327080249786\n",
      "bn2.module.weight, gradient norm: 0.016477975994348526\n",
      "bn2.module.bias, gradient norm: 0.010519260540604591\n",
      "conv3.bias, gradient norm: 0.009495764039456844\n",
      "conv3.lin.weight, gradient norm: 0.026100562885403633\n",
      "Epoch: 157, Training Loss: 0.1868, Validation Loss: 0.2681, Train Acc: 0.9898, Val Acc: 0.9898, Test Acc: 0.9902\n",
      "conv1.bias, gradient norm: 3.060581510450078e-10\n",
      "conv1.lin.weight, gradient norm: 0.03213873505592346\n",
      "bn1.module.weight, gradient norm: 0.01417513843625784\n",
      "bn1.module.bias, gradient norm: 0.015959562733769417\n",
      "conv2.bias, gradient norm: 1.0249035398146589e-08\n",
      "conv2.lin.weight, gradient norm: 0.022762691602110863\n",
      "bn2.module.weight, gradient norm: 0.016631068661808968\n",
      "bn2.module.bias, gradient norm: 0.006557214539498091\n",
      "conv3.bias, gradient norm: 0.011776896193623543\n",
      "conv3.lin.weight, gradient norm: 0.01876542530953884\n",
      "Epoch: 158, Training Loss: 0.1852, Validation Loss: 0.2678, Train Acc: 0.9899, Val Acc: 0.9899, Test Acc: 0.9904\n",
      "conv1.bias, gradient norm: 3.195058939642337e-10\n",
      "conv1.lin.weight, gradient norm: 0.042502835392951965\n",
      "bn1.module.weight, gradient norm: 0.018331080675125122\n",
      "bn1.module.bias, gradient norm: 0.021367661654949188\n",
      "conv2.bias, gradient norm: 1.5039306688890974e-08\n",
      "conv2.lin.weight, gradient norm: 0.03245376795530319\n",
      "bn2.module.weight, gradient norm: 0.017501868307590485\n",
      "bn2.module.bias, gradient norm: 0.006705241743475199\n",
      "conv3.bias, gradient norm: 0.013287143781781197\n",
      "conv3.lin.weight, gradient norm: 0.02731689065694809\n",
      "Epoch: 159, Training Loss: 0.1812, Validation Loss: 0.2681, Train Acc: 0.9898, Val Acc: 0.9896, Test Acc: 0.9903\n",
      "conv1.bias, gradient norm: 4.65836202945269e-10\n",
      "conv1.lin.weight, gradient norm: 0.025544138625264168\n",
      "bn1.module.weight, gradient norm: 0.012989833019673824\n",
      "bn1.module.bias, gradient norm: 0.01342727616429329\n",
      "conv2.bias, gradient norm: 1.2152567840928441e-08\n",
      "conv2.lin.weight, gradient norm: 0.02147609181702137\n",
      "bn2.module.weight, gradient norm: 0.015661630779504776\n",
      "bn2.module.bias, gradient norm: 0.007722053211182356\n",
      "conv3.bias, gradient norm: 0.011345848441123962\n",
      "conv3.lin.weight, gradient norm: 0.018383296206593513\n",
      "Epoch: 160, Training Loss: 0.1864, Validation Loss: 0.2690, Train Acc: 0.9896, Val Acc: 0.9897, Test Acc: 0.9901\n",
      "conv1.bias, gradient norm: 3.113555246958555e-10\n",
      "conv1.lin.weight, gradient norm: 0.022583654150366783\n",
      "bn1.module.weight, gradient norm: 0.014164615422487259\n",
      "bn1.module.bias, gradient norm: 0.012792913243174553\n",
      "conv2.bias, gradient norm: 1.187276676972715e-08\n",
      "conv2.lin.weight, gradient norm: 0.024970650672912598\n",
      "bn2.module.weight, gradient norm: 0.01527855172753334\n",
      "bn2.module.bias, gradient norm: 0.009816684760153294\n",
      "conv3.bias, gradient norm: 0.01123149786144495\n",
      "conv3.lin.weight, gradient norm: 0.01870676502585411\n",
      "Epoch: 161, Training Loss: 0.1823, Validation Loss: 0.2702, Train Acc: 0.9896, Val Acc: 0.9896, Test Acc: 0.9901\n",
      "conv1.bias, gradient norm: 3.0574751064271766e-10\n",
      "conv1.lin.weight, gradient norm: 0.022340482100844383\n",
      "bn1.module.weight, gradient norm: 0.01571524143218994\n",
      "bn1.module.bias, gradient norm: 0.01612013764679432\n",
      "conv2.bias, gradient norm: 7.111482069888098e-09\n",
      "conv2.lin.weight, gradient norm: 0.03197421878576279\n",
      "bn2.module.weight, gradient norm: 0.014932368882000446\n",
      "bn2.module.bias, gradient norm: 0.011623301543295383\n",
      "conv3.bias, gradient norm: 0.010259507223963737\n",
      "conv3.lin.weight, gradient norm: 0.019717399030923843\n",
      "Epoch: 162, Training Loss: 0.1851, Validation Loss: 0.2716, Train Acc: 0.9898, Val Acc: 0.9897, Test Acc: 0.9902\n",
      "conv1.bias, gradient norm: 3.187710928553855e-10\n",
      "conv1.lin.weight, gradient norm: 0.020269818603992462\n",
      "bn1.module.weight, gradient norm: 0.018412865698337555\n",
      "bn1.module.bias, gradient norm: 0.014272917993366718\n",
      "conv2.bias, gradient norm: 1.0372801284574962e-08\n",
      "conv2.lin.weight, gradient norm: 0.0197286494076252\n",
      "bn2.module.weight, gradient norm: 0.01569174975156784\n",
      "bn2.module.bias, gradient norm: 0.009167433716356754\n",
      "conv3.bias, gradient norm: 0.011793483048677444\n",
      "conv3.lin.weight, gradient norm: 0.01907491125166416\n",
      "Epoch: 163, Training Loss: 0.1845, Validation Loss: 0.2740, Train Acc: 0.9900, Val Acc: 0.9898, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 3.8172456862106685e-10\n",
      "conv1.lin.weight, gradient norm: 0.04325515404343605\n",
      "bn1.module.weight, gradient norm: 0.032269835472106934\n",
      "bn1.module.bias, gradient norm: 0.02475464902818203\n",
      "conv2.bias, gradient norm: 1.156840667704273e-08\n",
      "conv2.lin.weight, gradient norm: 0.02595876343548298\n",
      "bn2.module.weight, gradient norm: 0.01648193784058094\n",
      "bn2.module.bias, gradient norm: 0.006157191935926676\n",
      "conv3.bias, gradient norm: 0.013415022753179073\n",
      "conv3.lin.weight, gradient norm: 0.024300221353769302\n",
      "Epoch: 164, Training Loss: 0.1846, Validation Loss: 0.2764, Train Acc: 0.9900, Val Acc: 0.9897, Test Acc: 0.9904\n",
      "conv1.bias, gradient norm: 2.330383941373526e-10\n",
      "conv1.lin.weight, gradient norm: 0.02329993061721325\n",
      "bn1.module.weight, gradient norm: 0.021702799946069717\n",
      "bn1.module.bias, gradient norm: 0.014740793034434319\n",
      "conv2.bias, gradient norm: 1.4500485256974116e-08\n",
      "conv2.lin.weight, gradient norm: 0.021502692252397537\n",
      "bn2.module.weight, gradient norm: 0.017296887934207916\n",
      "bn2.module.bias, gradient norm: 0.006987316068261862\n",
      "conv3.bias, gradient norm: 0.012398074381053448\n",
      "conv3.lin.weight, gradient norm: 0.021615400910377502\n",
      "Epoch: 165, Training Loss: 0.1817, Validation Loss: 0.2786, Train Acc: 0.9898, Val Acc: 0.9897, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 3.5336911174965735e-10\n",
      "conv1.lin.weight, gradient norm: 0.023272279649972916\n",
      "bn1.module.weight, gradient norm: 0.01325059775263071\n",
      "bn1.module.bias, gradient norm: 0.011956855654716492\n",
      "conv2.bias, gradient norm: 1.1450094206111316e-08\n",
      "conv2.lin.weight, gradient norm: 0.02500207908451557\n",
      "bn2.module.weight, gradient norm: 0.016491791233420372\n",
      "bn2.module.bias, gradient norm: 0.008925278671085835\n",
      "conv3.bias, gradient norm: 0.01136522926390171\n",
      "conv3.lin.weight, gradient norm: 0.018758496269583702\n",
      "Epoch: 166, Training Loss: 0.1827, Validation Loss: 0.2792, Train Acc: 0.9898, Val Acc: 0.9897, Test Acc: 0.9904\n",
      "conv1.bias, gradient norm: 2.4948021426496325e-10\n",
      "conv1.lin.weight, gradient norm: 0.02275521121919155\n",
      "bn1.module.weight, gradient norm: 0.010875257663428783\n",
      "bn1.module.bias, gradient norm: 0.011193881742656231\n",
      "conv2.bias, gradient norm: 1.0808833827979925e-08\n",
      "conv2.lin.weight, gradient norm: 0.02095641754567623\n",
      "bn2.module.weight, gradient norm: 0.016114387661218643\n",
      "bn2.module.bias, gradient norm: 0.009130461141467094\n",
      "conv3.bias, gradient norm: 0.010818960145115852\n",
      "conv3.lin.weight, gradient norm: 0.019014520570635796\n",
      "Epoch: 167, Training Loss: 0.1836, Validation Loss: 0.2790, Train Acc: 0.9896, Val Acc: 0.9895, Test Acc: 0.9900\n",
      "conv1.bias, gradient norm: 3.1060143346195446e-10\n",
      "conv1.lin.weight, gradient norm: 0.022406959906220436\n",
      "bn1.module.weight, gradient norm: 0.015585027635097504\n",
      "bn1.module.bias, gradient norm: 0.009187904186546803\n",
      "conv2.bias, gradient norm: 1.1126992660592805e-08\n",
      "conv2.lin.weight, gradient norm: 0.02122822403907776\n",
      "bn2.module.weight, gradient norm: 0.01639597676694393\n",
      "bn2.module.bias, gradient norm: 0.009969912469387054\n",
      "conv3.bias, gradient norm: 0.010891531594097614\n",
      "conv3.lin.weight, gradient norm: 0.019419288262724876\n",
      "Epoch: 168, Training Loss: 0.1787, Validation Loss: 0.2783, Train Acc: 0.9895, Val Acc: 0.9894, Test Acc: 0.9900\n",
      "conv1.bias, gradient norm: 3.0073621370974024e-10\n",
      "conv1.lin.weight, gradient norm: 0.024021880701184273\n",
      "bn1.module.weight, gradient norm: 0.018261289224028587\n",
      "bn1.module.bias, gradient norm: 0.013925697654485703\n",
      "conv2.bias, gradient norm: 1.0839331210377168e-08\n",
      "conv2.lin.weight, gradient norm: 0.019452717155218124\n",
      "bn2.module.weight, gradient norm: 0.01480141095817089\n",
      "bn2.module.bias, gradient norm: 0.008586474694311619\n",
      "conv3.bias, gradient norm: 0.011741869151592255\n",
      "conv3.lin.weight, gradient norm: 0.02085670456290245\n",
      "Epoch: 169, Training Loss: 0.1815, Validation Loss: 0.2787, Train Acc: 0.9895, Val Acc: 0.9895, Test Acc: 0.9900\n",
      "conv1.bias, gradient norm: 3.5067007631006675e-10\n",
      "conv1.lin.weight, gradient norm: 0.025197597220540047\n",
      "bn1.module.weight, gradient norm: 0.017087038606405258\n",
      "bn1.module.bias, gradient norm: 0.015142393298447132\n",
      "conv2.bias, gradient norm: 1.0147060081067139e-08\n",
      "conv2.lin.weight, gradient norm: 0.024320140480995178\n",
      "bn2.module.weight, gradient norm: 0.01596199907362461\n",
      "bn2.module.bias, gradient norm: 0.007444368209689856\n",
      "conv3.bias, gradient norm: 0.012618070468306541\n",
      "conv3.lin.weight, gradient norm: 0.02357504330575466\n",
      "Epoch: 170, Training Loss: 0.1796, Validation Loss: 0.2798, Train Acc: 0.9896, Val Acc: 0.9897, Test Acc: 0.9901\n",
      "conv1.bias, gradient norm: 3.3022173884234007e-10\n",
      "conv1.lin.weight, gradient norm: 0.021660948172211647\n",
      "bn1.module.weight, gradient norm: 0.013414480723440647\n",
      "bn1.module.bias, gradient norm: 0.009604271501302719\n",
      "conv2.bias, gradient norm: 7.2749690716023e-09\n",
      "conv2.lin.weight, gradient norm: 0.020237475633621216\n",
      "bn2.module.weight, gradient norm: 0.01661199890077114\n",
      "bn2.module.bias, gradient norm: 0.009173048660159111\n",
      "conv3.bias, gradient norm: 0.011510273441672325\n",
      "conv3.lin.weight, gradient norm: 0.01978800818324089\n",
      "Epoch: 171, Training Loss: 0.1774, Validation Loss: 0.2812, Train Acc: 0.9897, Val Acc: 0.9897, Test Acc: 0.9904\n",
      "conv1.bias, gradient norm: 3.075867616164629e-10\n",
      "conv1.lin.weight, gradient norm: 0.027542877942323685\n",
      "bn1.module.weight, gradient norm: 0.014329016208648682\n",
      "bn1.module.bias, gradient norm: 0.00740613741800189\n",
      "conv2.bias, gradient norm: 1.1940270105981199e-08\n",
      "conv2.lin.weight, gradient norm: 0.021949777379631996\n",
      "bn2.module.weight, gradient norm: 0.015450493432581425\n",
      "bn2.module.bias, gradient norm: 0.008376667276024818\n",
      "conv3.bias, gradient norm: 0.011314048431813717\n",
      "conv3.lin.weight, gradient norm: 0.018970347940921783\n",
      "Epoch: 172, Training Loss: 0.1817, Validation Loss: 0.2817, Train Acc: 0.9897, Val Acc: 0.9897, Test Acc: 0.9903\n",
      "conv1.bias, gradient norm: 3.731411846175092e-10\n",
      "conv1.lin.weight, gradient norm: 0.024889325723052025\n",
      "bn1.module.weight, gradient norm: 0.010052284225821495\n",
      "bn1.module.bias, gradient norm: 0.007855242118239403\n",
      "conv2.bias, gradient norm: 1.1668123356400883e-08\n",
      "conv2.lin.weight, gradient norm: 0.02297554910182953\n",
      "bn2.module.weight, gradient norm: 0.014774145558476448\n",
      "bn2.module.bias, gradient norm: 0.008477295748889446\n",
      "conv3.bias, gradient norm: 0.010808687657117844\n",
      "conv3.lin.weight, gradient norm: 0.017300205305218697\n",
      "Epoch: 173, Training Loss: 0.1796, Validation Loss: 0.2806, Train Acc: 0.9897, Val Acc: 0.9898, Test Acc: 0.9904\n",
      "conv1.bias, gradient norm: 3.52900347833085e-10\n",
      "conv1.lin.weight, gradient norm: 0.025770826265215874\n",
      "bn1.module.weight, gradient norm: 0.013672311790287495\n",
      "bn1.module.bias, gradient norm: 0.00993479322642088\n",
      "conv2.bias, gradient norm: 1.1357888851648568e-08\n",
      "conv2.lin.weight, gradient norm: 0.02004900947213173\n",
      "bn2.module.weight, gradient norm: 0.016189932823181152\n",
      "bn2.module.bias, gradient norm: 0.009314394555985928\n",
      "conv3.bias, gradient norm: 0.010724463500082493\n",
      "conv3.lin.weight, gradient norm: 0.019154483452439308\n",
      "Epoch: 174, Training Loss: 0.1768, Validation Loss: 0.2805, Train Acc: 0.9898, Val Acc: 0.9899, Test Acc: 0.9904\n",
      "conv1.bias, gradient norm: 4.90473217595877e-10\n",
      "conv1.lin.weight, gradient norm: 0.02239270880818367\n",
      "bn1.module.weight, gradient norm: 0.015045935288071632\n",
      "bn1.module.bias, gradient norm: 0.012170559726655483\n",
      "conv2.bias, gradient norm: 1.2360736434402497e-08\n",
      "conv2.lin.weight, gradient norm: 0.023018494248390198\n",
      "bn2.module.weight, gradient norm: 0.016334576532244682\n",
      "bn2.module.bias, gradient norm: 0.007617419585585594\n",
      "conv3.bias, gradient norm: 0.010800033807754517\n",
      "conv3.lin.weight, gradient norm: 0.01833205111324787\n",
      "Epoch: 175, Training Loss: 0.1794, Validation Loss: 0.2816, Train Acc: 0.9897, Val Acc: 0.9898, Test Acc: 0.9902\n",
      "conv1.bias, gradient norm: 3.127022252247258e-10\n",
      "conv1.lin.weight, gradient norm: 0.03008359670639038\n",
      "bn1.module.weight, gradient norm: 0.014824866317212582\n",
      "bn1.module.bias, gradient norm: 0.014202558435499668\n",
      "conv2.bias, gradient norm: 1.487262846211479e-08\n",
      "conv2.lin.weight, gradient norm: 0.025285812094807625\n",
      "bn2.module.weight, gradient norm: 0.016989655792713165\n",
      "bn2.module.bias, gradient norm: 0.008158594369888306\n",
      "conv3.bias, gradient norm: 0.011139266192913055\n",
      "conv3.lin.weight, gradient norm: 0.019724702462553978\n",
      "Epoch: 176, Training Loss: 0.1758, Validation Loss: 0.2853, Train Acc: 0.9894, Val Acc: 0.9895, Test Acc: 0.9901\n",
      "conv1.bias, gradient norm: 3.770730394592192e-10\n",
      "conv1.lin.weight, gradient norm: 0.02115497551858425\n",
      "bn1.module.weight, gradient norm: 0.017812522128224373\n",
      "bn1.module.bias, gradient norm: 0.009167692624032497\n",
      "conv2.bias, gradient norm: 1.692902529271123e-08\n",
      "conv2.lin.weight, gradient norm: 0.023725971579551697\n",
      "bn2.module.weight, gradient norm: 0.016341883689165115\n",
      "bn2.module.bias, gradient norm: 0.00993356853723526\n",
      "conv3.bias, gradient norm: 0.00952379871159792\n",
      "conv3.lin.weight, gradient norm: 0.023016713559627533\n",
      "Epoch: 177, Training Loss: 0.1796, Validation Loss: 0.2893, Train Acc: 0.9894, Val Acc: 0.9895, Test Acc: 0.9900\n",
      "conv1.bias, gradient norm: 4.555796850880256e-10\n",
      "conv1.lin.weight, gradient norm: 0.027511512860655785\n",
      "bn1.module.weight, gradient norm: 0.012016404420137405\n",
      "bn1.module.bias, gradient norm: 0.0092471307143569\n",
      "conv2.bias, gradient norm: 1.700191809561602e-08\n",
      "conv2.lin.weight, gradient norm: 0.02065538801252842\n",
      "bn2.module.weight, gradient norm: 0.016421029344201088\n",
      "bn2.module.bias, gradient norm: 0.009076179005205631\n",
      "conv3.bias, gradient norm: 0.010070734657347202\n",
      "conv3.lin.weight, gradient norm: 0.019873928278684616\n",
      "Epoch: 178, Training Loss: 0.1774, Validation Loss: 0.2928, Train Acc: 0.9895, Val Acc: 0.9895, Test Acc: 0.9901\n",
      "conv1.bias, gradient norm: 5.241404532618787e-10\n",
      "conv1.lin.weight, gradient norm: 0.03349866345524788\n",
      "bn1.module.weight, gradient norm: 0.019642623141407967\n",
      "bn1.module.bias, gradient norm: 0.016499560326337814\n",
      "conv2.bias, gradient norm: 1.849992514735277e-08\n",
      "conv2.lin.weight, gradient norm: 0.024778183549642563\n",
      "bn2.module.weight, gradient norm: 0.017086094245314598\n",
      "bn2.module.bias, gradient norm: 0.006837331689894199\n",
      "conv3.bias, gradient norm: 0.011344325728714466\n",
      "conv3.lin.weight, gradient norm: 0.019095037132501602\n",
      "Epoch: 179, Training Loss: 0.1770, Validation Loss: 0.2950, Train Acc: 0.9894, Val Acc: 0.9895, Test Acc: 0.9900\n",
      "conv1.bias, gradient norm: 2.783681618545586e-10\n",
      "conv1.lin.weight, gradient norm: 0.03286216780543327\n",
      "bn1.module.weight, gradient norm: 0.023239608854055405\n",
      "bn1.module.bias, gradient norm: 0.012899208813905716\n",
      "conv2.bias, gradient norm: 1.544832350930392e-08\n",
      "conv2.lin.weight, gradient norm: 0.03252803906798363\n",
      "bn2.module.weight, gradient norm: 0.016424821689724922\n",
      "bn2.module.bias, gradient norm: 0.009830759838223457\n",
      "conv3.bias, gradient norm: 0.010497755371034145\n",
      "conv3.lin.weight, gradient norm: 0.019757958129048347\n",
      "Epoch: 180, Training Loss: 0.1768, Validation Loss: 0.2944, Train Acc: 0.9894, Val Acc: 0.9895, Test Acc: 0.9900\n",
      "conv1.bias, gradient norm: 5.331531882646345e-10\n",
      "conv1.lin.weight, gradient norm: 0.03135709464550018\n",
      "bn1.module.weight, gradient norm: 0.018164103850722313\n",
      "bn1.module.bias, gradient norm: 0.01275300607085228\n",
      "conv2.bias, gradient norm: 1.867277887868113e-08\n",
      "conv2.lin.weight, gradient norm: 0.02468169666826725\n",
      "bn2.module.weight, gradient norm: 0.01671864278614521\n",
      "bn2.module.bias, gradient norm: 0.008035956881940365\n",
      "conv3.bias, gradient norm: 0.010762975551187992\n",
      "conv3.lin.weight, gradient norm: 0.01867469772696495\n",
      "Epoch: 181, Training Loss: 0.1788, Validation Loss: 0.2938, Train Acc: 0.9892, Val Acc: 0.9892, Test Acc: 0.9898\n",
      "conv1.bias, gradient norm: 5.543770997817887e-10\n",
      "conv1.lin.weight, gradient norm: 0.03858610987663269\n",
      "bn1.module.weight, gradient norm: 0.022362738847732544\n",
      "bn1.module.bias, gradient norm: 0.01094526145607233\n",
      "conv2.bias, gradient norm: 1.856762743557283e-08\n",
      "conv2.lin.weight, gradient norm: 0.029187198728322983\n",
      "bn2.module.weight, gradient norm: 0.01672380603849888\n",
      "bn2.module.bias, gradient norm: 0.00949408020824194\n",
      "conv3.bias, gradient norm: 0.009093180298805237\n",
      "conv3.lin.weight, gradient norm: 0.023788271471858025\n",
      "Epoch: 182, Training Loss: 0.1796, Validation Loss: 0.2931, Train Acc: 0.9892, Val Acc: 0.9894, Test Acc: 0.9898\n",
      "conv1.bias, gradient norm: 3.945572202290748e-10\n",
      "conv1.lin.weight, gradient norm: 0.040054745972156525\n",
      "bn1.module.weight, gradient norm: 0.017398934811353683\n",
      "bn1.module.bias, gradient norm: 0.014957479201257229\n",
      "conv2.bias, gradient norm: 1.8845089044816632e-08\n",
      "conv2.lin.weight, gradient norm: 0.03026619926095009\n",
      "bn2.module.weight, gradient norm: 0.017043132334947586\n",
      "bn2.module.bias, gradient norm: 0.007260023150593042\n",
      "conv3.bias, gradient norm: 0.010493163019418716\n",
      "conv3.lin.weight, gradient norm: 0.019977254793047905\n",
      "Epoch: 183, Training Loss: 0.1768, Validation Loss: 0.2934, Train Acc: 0.9891, Val Acc: 0.9892, Test Acc: 0.9897\n",
      "conv1.bias, gradient norm: 5.136833736152369e-10\n",
      "conv1.lin.weight, gradient norm: 0.03906292840838432\n",
      "bn1.module.weight, gradient norm: 0.020298250019550323\n",
      "bn1.module.bias, gradient norm: 0.014511442743241787\n",
      "conv2.bias, gradient norm: 1.8505648569089317e-08\n",
      "conv2.lin.weight, gradient norm: 0.027196040377020836\n",
      "bn2.module.weight, gradient norm: 0.016533132642507553\n",
      "bn2.module.bias, gradient norm: 0.00778094632551074\n",
      "conv3.bias, gradient norm: 0.010710046626627445\n",
      "conv3.lin.weight, gradient norm: 0.01916181854903698\n",
      "Epoch: 184, Training Loss: 0.1780, Validation Loss: 0.2969, Train Acc: 0.9888, Val Acc: 0.9890, Test Acc: 0.9893\n",
      "conv1.bias, gradient norm: 4.949932685960334e-10\n",
      "conv1.lin.weight, gradient norm: 0.045214638113975525\n",
      "bn1.module.weight, gradient norm: 0.019658006727695465\n",
      "bn1.module.bias, gradient norm: 0.01706276834011078\n",
      "conv2.bias, gradient norm: 1.6286213266880623e-08\n",
      "conv2.lin.weight, gradient norm: 0.03353581950068474\n",
      "bn2.module.weight, gradient norm: 0.015937460586428642\n",
      "bn2.module.bias, gradient norm: 0.010742302052676678\n",
      "conv3.bias, gradient norm: 0.009855206124484539\n",
      "conv3.lin.weight, gradient norm: 0.01999341882765293\n",
      "Epoch: 185, Training Loss: 0.1768, Validation Loss: 0.2994, Train Acc: 0.9888, Val Acc: 0.9890, Test Acc: 0.9892\n",
      "conv1.bias, gradient norm: 3.8871936225426396e-10\n",
      "conv1.lin.weight, gradient norm: 0.044082265347242355\n",
      "bn1.module.weight, gradient norm: 0.01177484542131424\n",
      "bn1.module.bias, gradient norm: 0.010067878291010857\n",
      "conv2.bias, gradient norm: 2.1769086089307166e-08\n",
      "conv2.lin.weight, gradient norm: 0.03219929710030556\n",
      "bn2.module.weight, gradient norm: 0.01712409406900406\n",
      "bn2.module.bias, gradient norm: 0.006927384063601494\n",
      "conv3.bias, gradient norm: 0.01085144467651844\n",
      "conv3.lin.weight, gradient norm: 0.01915579102933407\n",
      "Epoch: 186, Training Loss: 0.1755, Validation Loss: 0.3010, Train Acc: 0.9886, Val Acc: 0.9888, Test Acc: 0.9890\n",
      "conv1.bias, gradient norm: 7.373641142116583e-10\n",
      "conv1.lin.weight, gradient norm: 0.04797070100903511\n",
      "bn1.module.weight, gradient norm: 0.02466694265604019\n",
      "bn1.module.bias, gradient norm: 0.02161906287074089\n",
      "conv2.bias, gradient norm: 2.4447789570558598e-08\n",
      "conv2.lin.weight, gradient norm: 0.031855788081884384\n",
      "bn2.module.weight, gradient norm: 0.016836175695061684\n",
      "bn2.module.bias, gradient norm: 0.006961014121770859\n",
      "conv3.bias, gradient norm: 0.011153172701597214\n",
      "conv3.lin.weight, gradient norm: 0.018659990280866623\n",
      "Epoch: 187, Training Loss: 0.1777, Validation Loss: 0.3034, Train Acc: 0.9883, Val Acc: 0.9884, Test Acc: 0.9888\n",
      "conv1.bias, gradient norm: 5.352722154405853e-10\n",
      "conv1.lin.weight, gradient norm: 0.06419888138771057\n",
      "bn1.module.weight, gradient norm: 0.02824264205992222\n",
      "bn1.module.bias, gradient norm: 0.02143305353820324\n",
      "conv2.bias, gradient norm: 1.2809043603567716e-08\n",
      "conv2.lin.weight, gradient norm: 0.04715630039572716\n",
      "bn2.module.weight, gradient norm: 0.016409220173954964\n",
      "bn2.module.bias, gradient norm: 0.01117126364260912\n",
      "conv3.bias, gradient norm: 0.009954421781003475\n",
      "conv3.lin.weight, gradient norm: 0.022583501413464546\n",
      "Epoch: 188, Training Loss: 0.1760, Validation Loss: 0.3038, Train Acc: 0.9886, Val Acc: 0.9888, Test Acc: 0.9890\n",
      "conv1.bias, gradient norm: 1.3102406937193223e-09\n",
      "conv1.lin.weight, gradient norm: 0.12805113196372986\n",
      "bn1.module.weight, gradient norm: 0.043053869158029556\n",
      "bn1.module.bias, gradient norm: 0.04935326799750328\n",
      "conv2.bias, gradient norm: 3.283284755184468e-08\n",
      "conv2.lin.weight, gradient norm: 0.06549621373414993\n",
      "bn2.module.weight, gradient norm: 0.019031286239624023\n",
      "bn2.module.bias, gradient norm: 0.006860668305307627\n",
      "conv3.bias, gradient norm: 0.014507153071463108\n",
      "conv3.lin.weight, gradient norm: 0.027124600484967232\n",
      "Epoch: 189, Training Loss: 0.1724, Validation Loss: 0.3065, Train Acc: 0.9877, Val Acc: 0.9879, Test Acc: 0.9883\n",
      "conv1.bias, gradient norm: 9.72967262313773e-10\n",
      "conv1.lin.weight, gradient norm: 0.14617830514907837\n",
      "bn1.module.weight, gradient norm: 0.07910498231649399\n",
      "bn1.module.bias, gradient norm: 0.05663134902715683\n",
      "conv2.bias, gradient norm: 2.67074273807566e-08\n",
      "conv2.lin.weight, gradient norm: 0.11522819846868515\n",
      "bn2.module.weight, gradient norm: 0.01627669297158718\n",
      "bn2.module.bias, gradient norm: 0.01796634867787361\n",
      "conv3.bias, gradient norm: 0.007888885214924812\n",
      "conv3.lin.weight, gradient norm: 0.040879081934690475\n",
      "Epoch: 190, Training Loss: 0.1763, Validation Loss: 0.3025, Train Acc: 0.9887, Val Acc: 0.9890, Test Acc: 0.9894\n",
      "conv1.bias, gradient norm: 1.2815254413212074e-09\n",
      "conv1.lin.weight, gradient norm: 0.1523723155260086\n",
      "bn1.module.weight, gradient norm: 0.08371590077877045\n",
      "bn1.module.bias, gradient norm: 0.09129302948713303\n",
      "conv2.bias, gradient norm: 3.913256563237155e-08\n",
      "conv2.lin.weight, gradient norm: 0.15576745569705963\n",
      "bn2.module.weight, gradient norm: 0.01854558289051056\n",
      "bn2.module.bias, gradient norm: 0.015393584035336971\n",
      "conv3.bias, gradient norm: 0.018066531047225\n",
      "conv3.lin.weight, gradient norm: 0.06006690114736557\n",
      "Epoch: 191, Training Loss: 0.1731, Validation Loss: 0.3030, Train Acc: 0.9875, Val Acc: 0.9875, Test Acc: 0.9880\n",
      "conv1.bias, gradient norm: 1.2225557233236373e-09\n",
      "conv1.lin.weight, gradient norm: 0.13724550604820251\n",
      "bn1.module.weight, gradient norm: 0.08358164876699448\n",
      "bn1.module.bias, gradient norm: 0.06566398590803146\n",
      "conv2.bias, gradient norm: 2.618039651736126e-08\n",
      "conv2.lin.weight, gradient norm: 0.11829978227615356\n",
      "bn2.module.weight, gradient norm: 0.01623166725039482\n",
      "bn2.module.bias, gradient norm: 0.020807616412639618\n",
      "conv3.bias, gradient norm: 0.008239149115979671\n",
      "conv3.lin.weight, gradient norm: 0.04594314843416214\n",
      "Epoch: 192, Training Loss: 0.1784, Validation Loss: 0.3058, Train Acc: 0.9884, Val Acc: 0.9886, Test Acc: 0.9888\n",
      "conv1.bias, gradient norm: 1.0328176092144759e-09\n",
      "conv1.lin.weight, gradient norm: 0.4459364116191864\n",
      "bn1.module.weight, gradient norm: 0.21857129037380219\n",
      "bn1.module.bias, gradient norm: 0.1524735391139984\n",
      "conv2.bias, gradient norm: 3.6081718945979446e-08\n",
      "conv2.lin.weight, gradient norm: 0.1463812291622162\n",
      "bn2.module.weight, gradient norm: 0.016886919736862183\n",
      "bn2.module.bias, gradient norm: 0.013201840221881866\n",
      "conv3.bias, gradient norm: 0.017987024039030075\n",
      "conv3.lin.weight, gradient norm: 0.05336957424879074\n",
      "Epoch: 193, Training Loss: 0.1810, Validation Loss: 0.3099, Train Acc: 0.9874, Val Acc: 0.9875, Test Acc: 0.9880\n",
      "conv1.bias, gradient norm: 9.971065084712905e-10\n",
      "conv1.lin.weight, gradient norm: 0.1552927941083908\n",
      "bn1.module.weight, gradient norm: 0.09808992594480515\n",
      "bn1.module.bias, gradient norm: 0.07264149934053421\n",
      "conv2.bias, gradient norm: 1.4487306465582606e-08\n",
      "conv2.lin.weight, gradient norm: 0.1695091426372528\n",
      "bn2.module.weight, gradient norm: 0.01592128351330757\n",
      "bn2.module.bias, gradient norm: 0.022835610434412956\n",
      "conv3.bias, gradient norm: 0.008436686359345913\n",
      "conv3.lin.weight, gradient norm: 0.07110761106014252\n",
      "Epoch: 194, Training Loss: 0.1834, Validation Loss: 0.3082, Train Acc: 0.9885, Val Acc: 0.9886, Test Acc: 0.9891\n",
      "conv1.bias, gradient norm: 5.78244119253668e-10\n",
      "conv1.lin.weight, gradient norm: 0.09308521449565887\n",
      "bn1.module.weight, gradient norm: 0.04150274768471718\n",
      "bn1.module.bias, gradient norm: 0.0395960733294487\n",
      "conv2.bias, gradient norm: 2.8235470850290767e-08\n",
      "conv2.lin.weight, gradient norm: 0.09977708011865616\n",
      "bn2.module.weight, gradient norm: 0.013864640146493912\n",
      "bn2.module.bias, gradient norm: 0.015653161332011223\n",
      "conv3.bias, gradient norm: 0.009927675127983093\n",
      "conv3.lin.weight, gradient norm: 0.024669833481311798\n",
      "Epoch: 195, Training Loss: 0.1816, Validation Loss: 0.3045, Train Acc: 0.9900, Val Acc: 0.9902, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.7096875026823e-09\n",
      "conv1.lin.weight, gradient norm: 0.2816932797431946\n",
      "bn1.module.weight, gradient norm: 0.37216150760650635\n",
      "bn1.module.bias, gradient norm: 0.22161443531513214\n",
      "conv2.bias, gradient norm: 7.148005209955954e-08\n",
      "conv2.lin.weight, gradient norm: 0.3405160903930664\n",
      "bn2.module.weight, gradient norm: 0.02313634753227234\n",
      "bn2.module.bias, gradient norm: 0.0386468730866909\n",
      "conv3.bias, gradient norm: 0.028447721153497696\n",
      "conv3.lin.weight, gradient norm: 0.10475575178861618\n",
      "Epoch: 196, Training Loss: 0.1852, Validation Loss: 0.2984, Train Acc: 0.9875, Val Acc: 0.9875, Test Acc: 0.9880\n",
      "conv1.bias, gradient norm: 9.892375807396547e-10\n",
      "conv1.lin.weight, gradient norm: 0.13026496767997742\n",
      "bn1.module.weight, gradient norm: 0.13077159225940704\n",
      "bn1.module.bias, gradient norm: 0.07156169414520264\n",
      "conv2.bias, gradient norm: 2.601766802001748e-08\n",
      "conv2.lin.weight, gradient norm: 0.17538763582706451\n",
      "bn2.module.weight, gradient norm: 0.018373915925621986\n",
      "bn2.module.bias, gradient norm: 0.02470356412231922\n",
      "conv3.bias, gradient norm: 0.008989518508315086\n",
      "conv3.lin.weight, gradient norm: 0.0788300633430481\n",
      "Epoch: 197, Training Loss: 0.1839, Validation Loss: 0.2948, Train Acc: 0.9869, Val Acc: 0.9870, Test Acc: 0.9876\n",
      "conv1.bias, gradient norm: 7.556122394447584e-10\n",
      "conv1.lin.weight, gradient norm: 0.12010331451892853\n",
      "bn1.module.weight, gradient norm: 0.14146190881729126\n",
      "bn1.module.bias, gradient norm: 0.07889987528324127\n",
      "conv2.bias, gradient norm: 2.184762060153389e-08\n",
      "conv2.lin.weight, gradient norm: 0.17257800698280334\n",
      "bn2.module.weight, gradient norm: 0.018689582124352455\n",
      "bn2.module.bias, gradient norm: 0.025367774069309235\n",
      "conv3.bias, gradient norm: 0.0089222127571702\n",
      "conv3.lin.weight, gradient norm: 0.07736501842737198\n",
      "Epoch: 198, Training Loss: 0.1820, Validation Loss: 0.2925, Train Acc: 0.9892, Val Acc: 0.9889, Test Acc: 0.9896\n",
      "conv1.bias, gradient norm: 5.163313332445796e-09\n",
      "conv1.lin.weight, gradient norm: 0.977339506149292\n",
      "bn1.module.weight, gradient norm: 0.33297109603881836\n",
      "bn1.module.bias, gradient norm: 0.3500434458255768\n",
      "conv2.bias, gradient norm: 1.091917667395137e-07\n",
      "conv2.lin.weight, gradient norm: 0.5752683281898499\n",
      "bn2.module.weight, gradient norm: 0.02703944221138954\n",
      "bn2.module.bias, gradient norm: 0.06583504378795624\n",
      "conv3.bias, gradient norm: 0.03979882597923279\n",
      "conv3.lin.weight, gradient norm: 0.16019845008850098\n",
      "Epoch: 199, Training Loss: 0.1964, Validation Loss: 0.2995, Train Acc: 0.9865, Val Acc: 0.9865, Test Acc: 0.9871\n",
      "conv1.bias, gradient norm: 7.654747391505623e-10\n",
      "conv1.lin.weight, gradient norm: 0.12304948270320892\n",
      "bn1.module.weight, gradient norm: 0.12129680812358856\n",
      "bn1.module.bias, gradient norm: 0.07138630002737045\n",
      "conv2.bias, gradient norm: 2.4229354522731228e-08\n",
      "conv2.lin.weight, gradient norm: 0.1650676429271698\n",
      "bn2.module.weight, gradient norm: 0.015309536829590797\n",
      "bn2.module.bias, gradient norm: 0.027098162099719048\n",
      "conv3.bias, gradient norm: 0.009656180627644062\n",
      "conv3.lin.weight, gradient norm: 0.08812613040208817\n",
      "Epoch: 200, Training Loss: 0.1887, Validation Loss: 0.3081, Train Acc: 0.9867, Val Acc: 0.9865, Test Acc: 0.9873\n",
      "conv1.bias, gradient norm: 8.249758653988692e-10\n",
      "conv1.lin.weight, gradient norm: 0.1231987401843071\n",
      "bn1.module.weight, gradient norm: 0.11113548278808594\n",
      "bn1.module.bias, gradient norm: 0.06972581893205643\n",
      "conv2.bias, gradient norm: 2.9960212089008564e-08\n",
      "conv2.lin.weight, gradient norm: 0.14486734569072723\n",
      "bn2.module.weight, gradient norm: 0.014379042200744152\n",
      "bn2.module.bias, gradient norm: 0.02593856304883957\n",
      "conv3.bias, gradient norm: 0.010268284007906914\n",
      "conv3.lin.weight, gradient norm: 0.0866205096244812\n",
      "Epoch: 201, Training Loss: 0.1966, Validation Loss: 0.3068, Train Acc: 0.9875, Val Acc: 0.9875, Test Acc: 0.9878\n",
      "conv1.bias, gradient norm: 8.295474307473683e-10\n",
      "conv1.lin.weight, gradient norm: 0.13995209336280823\n",
      "bn1.module.weight, gradient norm: 0.10573112964630127\n",
      "bn1.module.bias, gradient norm: 0.07697346061468124\n",
      "conv2.bias, gradient norm: 1.578747976793693e-08\n",
      "conv2.lin.weight, gradient norm: 0.1222967654466629\n",
      "bn2.module.weight, gradient norm: 0.01524455938488245\n",
      "bn2.module.bias, gradient norm: 0.024506714195013046\n",
      "conv3.bias, gradient norm: 0.009145122952759266\n",
      "conv3.lin.weight, gradient norm: 0.06731773167848587\n",
      "Epoch: 202, Training Loss: 0.1915, Validation Loss: 0.3062, Train Acc: 0.9889, Val Acc: 0.9888, Test Acc: 0.9891\n",
      "conv1.bias, gradient norm: 1.0507352765642963e-09\n",
      "conv1.lin.weight, gradient norm: 0.0950307846069336\n",
      "bn1.module.weight, gradient norm: 0.06345878541469574\n",
      "bn1.module.bias, gradient norm: 0.04596634581685066\n",
      "conv2.bias, gradient norm: 1.7260171958355386e-08\n",
      "conv2.lin.weight, gradient norm: 0.08496885001659393\n",
      "bn2.module.weight, gradient norm: 0.014883195050060749\n",
      "bn2.module.bias, gradient norm: 0.016077730804681778\n",
      "conv3.bias, gradient norm: 0.011601042933762074\n",
      "conv3.lin.weight, gradient norm: 0.023240961134433746\n",
      "Epoch: 203, Training Loss: 0.1892, Validation Loss: 0.3077, Train Acc: 0.9903, Val Acc: 0.9902, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 3.461204656218797e-09\n",
      "conv1.lin.weight, gradient norm: 0.36113840341567993\n",
      "bn1.module.weight, gradient norm: 0.22629228234291077\n",
      "bn1.module.bias, gradient norm: 0.18845638632774353\n",
      "conv2.bias, gradient norm: 6.235203642290799e-08\n",
      "conv2.lin.weight, gradient norm: 0.17736995220184326\n",
      "bn2.module.weight, gradient norm: 0.019300788640975952\n",
      "bn2.module.bias, gradient norm: 0.023960983380675316\n",
      "conv3.bias, gradient norm: 0.026839451864361763\n",
      "conv3.lin.weight, gradient norm: 0.08518528938293457\n",
      "Epoch: 204, Training Loss: 0.1930, Validation Loss: 0.3017, Train Acc: 0.9897, Val Acc: 0.9897, Test Acc: 0.9899\n",
      "conv1.bias, gradient norm: 8.595186784532416e-10\n",
      "conv1.lin.weight, gradient norm: 0.12891241908073425\n",
      "bn1.module.weight, gradient norm: 0.0792623832821846\n",
      "bn1.module.bias, gradient norm: 0.07219221442937851\n",
      "conv2.bias, gradient norm: 2.475735350060404e-08\n",
      "conv2.lin.weight, gradient norm: 0.0728689506649971\n",
      "bn2.module.weight, gradient norm: 0.015104188583791256\n",
      "bn2.module.bias, gradient norm: 0.008637907914817333\n",
      "conv3.bias, gradient norm: 0.015728527680039406\n",
      "conv3.lin.weight, gradient norm: 0.03701915591955185\n",
      "Epoch: 205, Training Loss: 0.1874, Validation Loss: 0.2979, Train Acc: 0.9886, Val Acc: 0.9887, Test Acc: 0.9888\n",
      "conv1.bias, gradient norm: 9.839602466144015e-10\n",
      "conv1.lin.weight, gradient norm: 0.10692460089921951\n",
      "bn1.module.weight, gradient norm: 0.07930678874254227\n",
      "bn1.module.bias, gradient norm: 0.054192401468753815\n",
      "conv2.bias, gradient norm: 2.645410113188973e-08\n",
      "conv2.lin.weight, gradient norm: 0.08825240284204483\n",
      "bn2.module.weight, gradient norm: 0.01585119590163231\n",
      "bn2.module.bias, gradient norm: 0.02032143622636795\n",
      "conv3.bias, gradient norm: 0.009134267456829548\n",
      "conv3.lin.weight, gradient norm: 0.030999034643173218\n",
      "Epoch: 206, Training Loss: 0.1841, Validation Loss: 0.2959, Train Acc: 0.9882, Val Acc: 0.9882, Test Acc: 0.9884\n",
      "conv1.bias, gradient norm: 6.980804823086828e-10\n",
      "conv1.lin.weight, gradient norm: 0.10545796155929565\n",
      "bn1.module.weight, gradient norm: 0.08163166791200638\n",
      "bn1.module.bias, gradient norm: 0.056995004415512085\n",
      "conv2.bias, gradient norm: 2.2123199272527927e-08\n",
      "conv2.lin.weight, gradient norm: 0.09553113579750061\n",
      "bn2.module.weight, gradient norm: 0.015113024041056633\n",
      "bn2.module.bias, gradient norm: 0.023365363478660583\n",
      "conv3.bias, gradient norm: 0.008353076875209808\n",
      "conv3.lin.weight, gradient norm: 0.04223320260643959\n",
      "Epoch: 207, Training Loss: 0.1853, Validation Loss: 0.2979, Train Acc: 0.9882, Val Acc: 0.9882, Test Acc: 0.9887\n",
      "conv1.bias, gradient norm: 5.757512244741747e-10\n",
      "conv1.lin.weight, gradient norm: 0.08435816317796707\n",
      "bn1.module.weight, gradient norm: 0.06643164157867432\n",
      "bn1.module.bias, gradient norm: 0.045798126608133316\n",
      "conv2.bias, gradient norm: 2.011411659452733e-08\n",
      "conv2.lin.weight, gradient norm: 0.0826978012919426\n",
      "bn2.module.weight, gradient norm: 0.01572249084711075\n",
      "bn2.module.bias, gradient norm: 0.02194700576364994\n",
      "conv3.bias, gradient norm: 0.009147677570581436\n",
      "conv3.lin.weight, gradient norm: 0.03208375722169876\n",
      "Epoch: 208, Training Loss: 0.1814, Validation Loss: 0.3031, Train Acc: 0.9888, Val Acc: 0.9884, Test Acc: 0.9891\n",
      "conv1.bias, gradient norm: 4.521643892640981e-10\n",
      "conv1.lin.weight, gradient norm: 0.042624205350875854\n",
      "bn1.module.weight, gradient norm: 0.04316055774688721\n",
      "bn1.module.bias, gradient norm: 0.02840282768011093\n",
      "conv2.bias, gradient norm: 2.01065137872547e-08\n",
      "conv2.lin.weight, gradient norm: 0.05613821744918823\n",
      "bn2.module.weight, gradient norm: 0.014588800258934498\n",
      "bn2.module.bias, gradient norm: 0.010239830240607262\n",
      "conv3.bias, gradient norm: 0.013926449231803417\n",
      "conv3.lin.weight, gradient norm: 0.02483232319355011\n",
      "Epoch: 209, Training Loss: 0.1865, Validation Loss: 0.3080, Train Acc: 0.9891, Val Acc: 0.9891, Test Acc: 0.9896\n",
      "conv1.bias, gradient norm: 1.0334085809304838e-09\n",
      "conv1.lin.weight, gradient norm: 0.18099334836006165\n",
      "bn1.module.weight, gradient norm: 0.13876140117645264\n",
      "bn1.module.bias, gradient norm: 0.1055966317653656\n",
      "conv2.bias, gradient norm: 3.196496223267786e-08\n",
      "conv2.lin.weight, gradient norm: 0.10610561072826385\n",
      "bn2.module.weight, gradient norm: 0.01918393187224865\n",
      "bn2.module.bias, gradient norm: 0.01735222153365612\n",
      "conv3.bias, gradient norm: 0.02212676592171192\n",
      "conv3.lin.weight, gradient norm: 0.06908559799194336\n",
      "Epoch: 210, Training Loss: 0.1841, Validation Loss: 0.3069, Train Acc: 0.9891, Val Acc: 0.9890, Test Acc: 0.9895\n",
      "conv1.bias, gradient norm: 7.363114007397087e-10\n",
      "conv1.lin.weight, gradient norm: 0.09712826460599899\n",
      "bn1.module.weight, gradient norm: 0.10124965012073517\n",
      "bn1.module.bias, gradient norm: 0.0700070783495903\n",
      "conv2.bias, gradient norm: 2.9153612857157896e-08\n",
      "conv2.lin.weight, gradient norm: 0.06845677644014359\n",
      "bn2.module.weight, gradient norm: 0.018249772489070892\n",
      "bn2.module.bias, gradient norm: 0.010425091721117496\n",
      "conv3.bias, gradient norm: 0.0183008573949337\n",
      "conv3.lin.weight, gradient norm: 0.0512625053524971\n",
      "Epoch: 211, Training Loss: 0.1819, Validation Loss: 0.3023, Train Acc: 0.9888, Val Acc: 0.9886, Test Acc: 0.9891\n",
      "conv1.bias, gradient norm: 5.943922576356897e-10\n",
      "conv1.lin.weight, gradient norm: 0.040816232562065125\n",
      "bn1.module.weight, gradient norm: 0.03985253721475601\n",
      "bn1.module.bias, gradient norm: 0.027286620810627937\n",
      "conv2.bias, gradient norm: 2.1862247123749512e-08\n",
      "conv2.lin.weight, gradient norm: 0.03545239195227623\n",
      "bn2.module.weight, gradient norm: 0.01793065294623375\n",
      "bn2.module.bias, gradient norm: 0.010064617730677128\n",
      "conv3.bias, gradient norm: 0.012447593733668327\n",
      "conv3.lin.weight, gradient norm: 0.02394074760377407\n",
      "Epoch: 212, Training Loss: 0.1806, Validation Loss: 0.3011, Train Acc: 0.9886, Val Acc: 0.9885, Test Acc: 0.9889\n",
      "conv1.bias, gradient norm: 6.115836725939516e-10\n",
      "conv1.lin.weight, gradient norm: 0.05671456828713417\n",
      "bn1.module.weight, gradient norm: 0.04753303527832031\n",
      "bn1.module.bias, gradient norm: 0.0318424217402935\n",
      "conv2.bias, gradient norm: 2.1473262279414485e-08\n",
      "conv2.lin.weight, gradient norm: 0.05006762221455574\n",
      "bn2.module.weight, gradient norm: 0.017019836232066154\n",
      "bn2.module.bias, gradient norm: 0.01579907350242138\n",
      "conv3.bias, gradient norm: 0.009600958786904812\n",
      "conv3.lin.weight, gradient norm: 0.02275042049586773\n",
      "Epoch: 213, Training Loss: 0.1817, Validation Loss: 0.3009, Train Acc: 0.9888, Val Acc: 0.9887, Test Acc: 0.9889\n",
      "conv1.bias, gradient norm: 7.276253488619489e-10\n",
      "conv1.lin.weight, gradient norm: 0.06170343607664108\n",
      "bn1.module.weight, gradient norm: 0.04926910623908043\n",
      "bn1.module.bias, gradient norm: 0.031004492193460464\n",
      "conv2.bias, gradient norm: 2.3935736948033082e-08\n",
      "conv2.lin.weight, gradient norm: 0.053612120449543\n",
      "bn2.module.weight, gradient norm: 0.01659044809639454\n",
      "bn2.module.bias, gradient norm: 0.015186004340648651\n",
      "conv3.bias, gradient norm: 0.01000879891216755\n",
      "conv3.lin.weight, gradient norm: 0.0225853119045496\n",
      "Epoch: 214, Training Loss: 0.1833, Validation Loss: 0.2975, Train Acc: 0.9890, Val Acc: 0.9889, Test Acc: 0.9892\n",
      "conv1.bias, gradient norm: 4.562509814398652e-10\n",
      "conv1.lin.weight, gradient norm: 0.051514092832803726\n",
      "bn1.module.weight, gradient norm: 0.04131358489394188\n",
      "bn1.module.bias, gradient norm: 0.02168404497206211\n",
      "conv2.bias, gradient norm: 1.873011790110013e-08\n",
      "conv2.lin.weight, gradient norm: 0.04184463620185852\n",
      "bn2.module.weight, gradient norm: 0.017997929826378822\n",
      "bn2.module.bias, gradient norm: 0.01169689279049635\n",
      "conv3.bias, gradient norm: 0.01114115584641695\n",
      "conv3.lin.weight, gradient norm: 0.020341508090496063\n",
      "Epoch: 215, Training Loss: 0.1810, Validation Loss: 0.2890, Train Acc: 0.9893, Val Acc: 0.9891, Test Acc: 0.9894\n",
      "conv1.bias, gradient norm: 6.2318394888905e-10\n",
      "conv1.lin.weight, gradient norm: 0.05080776661634445\n",
      "bn1.module.weight, gradient norm: 0.027865983545780182\n",
      "bn1.module.bias, gradient norm: 0.036524996161460876\n",
      "conv2.bias, gradient norm: 2.924574360463339e-08\n",
      "conv2.lin.weight, gradient norm: 0.043479256331920624\n",
      "bn2.module.weight, gradient norm: 0.01911856234073639\n",
      "bn2.module.bias, gradient norm: 0.008747677318751812\n",
      "conv3.bias, gradient norm: 0.01335866004228592\n",
      "conv3.lin.weight, gradient norm: 0.025310024619102478\n",
      "Epoch: 216, Training Loss: 0.1813, Validation Loss: 0.2820, Train Acc: 0.9896, Val Acc: 0.9894, Test Acc: 0.9899\n",
      "conv1.bias, gradient norm: 4.6343587301045375e-10\n",
      "conv1.lin.weight, gradient norm: 0.07851624488830566\n",
      "bn1.module.weight, gradient norm: 0.04648676514625549\n",
      "bn1.module.bias, gradient norm: 0.05236780270934105\n",
      "conv2.bias, gradient norm: 2.1301856278910236e-08\n",
      "conv2.lin.weight, gradient norm: 0.06329412013292313\n",
      "bn2.module.weight, gradient norm: 0.018909215927124023\n",
      "bn2.module.bias, gradient norm: 0.010884381830692291\n",
      "conv3.bias, gradient norm: 0.015408053062856197\n",
      "conv3.lin.weight, gradient norm: 0.03556394204497337\n",
      "Epoch: 217, Training Loss: 0.1809, Validation Loss: 0.2802, Train Acc: 0.9897, Val Acc: 0.9894, Test Acc: 0.9899\n",
      "conv1.bias, gradient norm: 4.458454161415659e-10\n",
      "conv1.lin.weight, gradient norm: 0.04649489372968674\n",
      "bn1.module.weight, gradient norm: 0.033617421984672546\n",
      "bn1.module.bias, gradient norm: 0.03504689782857895\n",
      "conv2.bias, gradient norm: 2.2527267162786302e-08\n",
      "conv2.lin.weight, gradient norm: 0.035255685448646545\n",
      "bn2.module.weight, gradient norm: 0.017649084329605103\n",
      "bn2.module.bias, gradient norm: 0.007369025610387325\n",
      "conv3.bias, gradient norm: 0.013151518069207668\n",
      "conv3.lin.weight, gradient norm: 0.02381879650056362\n",
      "Epoch: 218, Training Loss: 0.1828, Validation Loss: 0.2803, Train Acc: 0.9895, Val Acc: 0.9893, Test Acc: 0.9898\n",
      "conv1.bias, gradient norm: 4.2830156088413673e-10\n",
      "conv1.lin.weight, gradient norm: 0.03450275957584381\n",
      "bn1.module.weight, gradient norm: 0.019245406612753868\n",
      "bn1.module.bias, gradient norm: 0.017900481820106506\n",
      "conv2.bias, gradient norm: 2.2033809443655628e-08\n",
      "conv2.lin.weight, gradient norm: 0.03386591002345085\n",
      "bn2.module.weight, gradient norm: 0.016683747991919518\n",
      "bn2.module.bias, gradient norm: 0.011489951983094215\n",
      "conv3.bias, gradient norm: 0.011422956362366676\n",
      "conv3.lin.weight, gradient norm: 0.01911144144833088\n",
      "Epoch: 219, Training Loss: 0.1780, Validation Loss: 0.2814, Train Acc: 0.9895, Val Acc: 0.9893, Test Acc: 0.9897\n",
      "conv1.bias, gradient norm: 6.394491602890184e-10\n",
      "conv1.lin.weight, gradient norm: 0.05043357238173485\n",
      "bn1.module.weight, gradient norm: 0.024002552032470703\n",
      "bn1.module.bias, gradient norm: 0.027113214135169983\n",
      "conv2.bias, gradient norm: 1.7272700603143676e-08\n",
      "conv2.lin.weight, gradient norm: 0.04529660940170288\n",
      "bn2.module.weight, gradient norm: 0.015028728172183037\n",
      "bn2.module.bias, gradient norm: 0.012327676638960838\n",
      "conv3.bias, gradient norm: 0.010977325960993767\n",
      "conv3.lin.weight, gradient norm: 0.01827928051352501\n",
      "Epoch: 220, Training Loss: 0.1816, Validation Loss: 0.2792, Train Acc: 0.9895, Val Acc: 0.9893, Test Acc: 0.9898\n",
      "conv1.bias, gradient norm: 3.1172200931628424e-10\n",
      "conv1.lin.weight, gradient norm: 0.038159869611263275\n",
      "bn1.module.weight, gradient norm: 0.026149023324251175\n",
      "bn1.module.bias, gradient norm: 0.02502046711742878\n",
      "conv2.bias, gradient norm: 1.6742628616839283e-08\n",
      "conv2.lin.weight, gradient norm: 0.03426092863082886\n",
      "bn2.module.weight, gradient norm: 0.016249975189566612\n",
      "bn2.module.bias, gradient norm: 0.011840017512440681\n",
      "conv3.bias, gradient norm: 0.010571686550974846\n",
      "conv3.lin.weight, gradient norm: 0.019697781652212143\n",
      "Epoch: 221, Training Loss: 0.1798, Validation Loss: 0.2766, Train Acc: 0.9898, Val Acc: 0.9896, Test Acc: 0.9899\n",
      "conv1.bias, gradient norm: 3.644339829911303e-10\n",
      "conv1.lin.weight, gradient norm: 0.041767898947000504\n",
      "bn1.module.weight, gradient norm: 0.029657477512955666\n",
      "bn1.module.bias, gradient norm: 0.023483609780669212\n",
      "conv2.bias, gradient norm: 2.6647985151839748e-08\n",
      "conv2.lin.weight, gradient norm: 0.03652511537075043\n",
      "bn2.module.weight, gradient norm: 0.018237292766571045\n",
      "bn2.module.bias, gradient norm: 0.00710549158975482\n",
      "conv3.bias, gradient norm: 0.012415653094649315\n",
      "conv3.lin.weight, gradient norm: 0.02140096202492714\n",
      "Epoch: 222, Training Loss: 0.1757, Validation Loss: 0.2750, Train Acc: 0.9899, Val Acc: 0.9897, Test Acc: 0.9901\n",
      "conv1.bias, gradient norm: 1.4584166097009188e-09\n",
      "conv1.lin.weight, gradient norm: 0.07707523554563522\n",
      "bn1.module.weight, gradient norm: 0.05386311560869217\n",
      "bn1.module.bias, gradient norm: 0.04373190924525261\n",
      "conv2.bias, gradient norm: 2.206584781561105e-08\n",
      "conv2.lin.weight, gradient norm: 0.05440431460738182\n",
      "bn2.module.weight, gradient norm: 0.01828453689813614\n",
      "bn2.module.bias, gradient norm: 0.006443483289331198\n",
      "conv3.bias, gradient norm: 0.014187206514179707\n",
      "conv3.lin.weight, gradient norm: 0.03416212275624275\n",
      "Epoch: 223, Training Loss: 0.1764, Validation Loss: 0.2729, Train Acc: 0.9898, Val Acc: 0.9894, Test Acc: 0.9898\n",
      "conv1.bias, gradient norm: 5.248533274659906e-10\n",
      "conv1.lin.weight, gradient norm: 0.0327727273106575\n",
      "bn1.module.weight, gradient norm: 0.023304710164666176\n",
      "bn1.module.bias, gradient norm: 0.016346512362360954\n",
      "conv2.bias, gradient norm: 1.8240092103383176e-08\n",
      "conv2.lin.weight, gradient norm: 0.02804812416434288\n",
      "bn2.module.weight, gradient norm: 0.01743404008448124\n",
      "bn2.module.bias, gradient norm: 0.008723106235265732\n",
      "conv3.bias, gradient norm: 0.010928106494247913\n",
      "conv3.lin.weight, gradient norm: 0.01950790360569954\n",
      "Epoch: 224, Training Loss: 0.1767, Validation Loss: 0.2716, Train Acc: 0.9898, Val Acc: 0.9893, Test Acc: 0.9897\n",
      "conv1.bias, gradient norm: 5.195468499863409e-10\n",
      "conv1.lin.weight, gradient norm: 0.03211779519915581\n",
      "bn1.module.weight, gradient norm: 0.035490404814481735\n",
      "bn1.module.bias, gradient norm: 0.018694130703806877\n",
      "conv2.bias, gradient norm: 1.9320953725809886e-08\n",
      "conv2.lin.weight, gradient norm: 0.03095480240881443\n",
      "bn2.module.weight, gradient norm: 0.017596416175365448\n",
      "bn2.module.bias, gradient norm: 0.011079562827944756\n",
      "conv3.bias, gradient norm: 0.010050562210381031\n",
      "conv3.lin.weight, gradient norm: 0.02323540672659874\n",
      "Epoch: 225, Training Loss: 0.1789, Validation Loss: 0.2701, Train Acc: 0.9898, Val Acc: 0.9894, Test Acc: 0.9898\n",
      "conv1.bias, gradient norm: 4.479445148142247e-10\n",
      "conv1.lin.weight, gradient norm: 0.0446062833070755\n",
      "bn1.module.weight, gradient norm: 0.027224531397223473\n",
      "bn1.module.bias, gradient norm: 0.02336733415722847\n",
      "conv2.bias, gradient norm: 2.0247567178444115e-08\n",
      "conv2.lin.weight, gradient norm: 0.04069889336824417\n",
      "bn2.module.weight, gradient norm: 0.01639096438884735\n",
      "bn2.module.bias, gradient norm: 0.012618067674338818\n",
      "conv3.bias, gradient norm: 0.009925651364028454\n",
      "conv3.lin.weight, gradient norm: 0.022267278283834457\n",
      "Epoch: 226, Training Loss: 0.1763, Validation Loss: 0.2691, Train Acc: 0.9899, Val Acc: 0.9896, Test Acc: 0.9901\n",
      "conv1.bias, gradient norm: 5.043949702354666e-10\n",
      "conv1.lin.weight, gradient norm: 0.03205651044845581\n",
      "bn1.module.weight, gradient norm: 0.02668466977775097\n",
      "bn1.module.bias, gradient norm: 0.016164716333150864\n",
      "conv2.bias, gradient norm: 2.0996109739712665e-08\n",
      "conv2.lin.weight, gradient norm: 0.031047113239765167\n",
      "bn2.module.weight, gradient norm: 0.016341278329491615\n",
      "bn2.module.bias, gradient norm: 0.007989662699401379\n",
      "conv3.bias, gradient norm: 0.012081561610102654\n",
      "conv3.lin.weight, gradient norm: 0.018158141523599625\n",
      "Epoch: 227, Training Loss: 0.1790, Validation Loss: 0.2673, Train Acc: 0.9900, Val Acc: 0.9896, Test Acc: 0.9903\n",
      "conv1.bias, gradient norm: 5.723692075854103e-10\n",
      "conv1.lin.weight, gradient norm: 0.05072461813688278\n",
      "bn1.module.weight, gradient norm: 0.03448258712887764\n",
      "bn1.module.bias, gradient norm: 0.03145634010434151\n",
      "conv2.bias, gradient norm: 2.5745755749539967e-08\n",
      "conv2.lin.weight, gradient norm: 0.03762355074286461\n",
      "bn2.module.weight, gradient norm: 0.01740434020757675\n",
      "bn2.module.bias, gradient norm: 0.007234085816890001\n",
      "conv3.bias, gradient norm: 0.012962691485881805\n",
      "conv3.lin.weight, gradient norm: 0.02250967174768448\n",
      "Epoch: 228, Training Loss: 0.1775, Validation Loss: 0.2654, Train Acc: 0.9900, Val Acc: 0.9895, Test Acc: 0.9902\n",
      "conv1.bias, gradient norm: 4.573447454081503e-10\n",
      "conv1.lin.weight, gradient norm: 0.032641563564538956\n",
      "bn1.module.weight, gradient norm: 0.0222221277654171\n",
      "bn1.module.bias, gradient norm: 0.01663493923842907\n",
      "conv2.bias, gradient norm: 2.5541160297848364e-08\n",
      "conv2.lin.weight, gradient norm: 0.027009986340999603\n",
      "bn2.module.weight, gradient norm: 0.01771649159491062\n",
      "bn2.module.bias, gradient norm: 0.008126997388899326\n",
      "conv3.bias, gradient norm: 0.01123165525496006\n",
      "conv3.lin.weight, gradient norm: 0.019478749483823776\n",
      "Epoch: 229, Training Loss: 0.1750, Validation Loss: 0.2637, Train Acc: 0.9899, Val Acc: 0.9896, Test Acc: 0.9902\n",
      "conv1.bias, gradient norm: 3.293894046407786e-10\n",
      "conv1.lin.weight, gradient norm: 0.03003140538930893\n",
      "bn1.module.weight, gradient norm: 0.020052138715982437\n",
      "bn1.module.bias, gradient norm: 0.012249529361724854\n",
      "conv2.bias, gradient norm: 2.166333956665767e-08\n",
      "conv2.lin.weight, gradient norm: 0.02658507227897644\n",
      "bn2.module.weight, gradient norm: 0.018450751900672913\n",
      "bn2.module.bias, gradient norm: 0.010326295159757137\n",
      "conv3.bias, gradient norm: 0.009744737297296524\n",
      "conv3.lin.weight, gradient norm: 0.022827886044979095\n",
      "Epoch: 230, Training Loss: 0.1755, Validation Loss: 0.2647, Train Acc: 0.9899, Val Acc: 0.9896, Test Acc: 0.9902\n",
      "conv1.bias, gradient norm: 4.0854439276039045e-10\n",
      "conv1.lin.weight, gradient norm: 0.032923560589551926\n",
      "bn1.module.weight, gradient norm: 0.026397118344902992\n",
      "bn1.module.bias, gradient norm: 0.01493944600224495\n",
      "conv2.bias, gradient norm: 2.6387530382976365e-08\n",
      "conv2.lin.weight, gradient norm: 0.02844098210334778\n",
      "bn2.module.weight, gradient norm: 0.017456023022532463\n",
      "bn2.module.bias, gradient norm: 0.010101433843374252\n",
      "conv3.bias, gradient norm: 0.009515050798654556\n",
      "conv3.lin.weight, gradient norm: 0.02215750329196453\n",
      "Epoch: 231, Training Loss: 0.1766, Validation Loss: 0.2685, Train Acc: 0.9900, Val Acc: 0.9897, Test Acc: 0.9903\n",
      "conv1.bias, gradient norm: 6.131828933497729e-10\n",
      "conv1.lin.weight, gradient norm: 0.0328565277159214\n",
      "bn1.module.weight, gradient norm: 0.027629463002085686\n",
      "bn1.module.bias, gradient norm: 0.018625617027282715\n",
      "conv2.bias, gradient norm: 2.1308457220925447e-08\n",
      "conv2.lin.weight, gradient norm: 0.031385958194732666\n",
      "bn2.module.weight, gradient norm: 0.016540370881557465\n",
      "bn2.module.bias, gradient norm: 0.007714481558650732\n",
      "conv3.bias, gradient norm: 0.010885961353778839\n",
      "conv3.lin.weight, gradient norm: 0.018142659217119217\n",
      "Epoch: 232, Training Loss: 0.1779, Validation Loss: 0.2719, Train Acc: 0.9900, Val Acc: 0.9897, Test Acc: 0.9903\n",
      "conv1.bias, gradient norm: 4.3340930844237846e-10\n",
      "conv1.lin.weight, gradient norm: 0.032091159373521805\n",
      "bn1.module.weight, gradient norm: 0.019465001299977303\n",
      "bn1.module.bias, gradient norm: 0.016215531155467033\n",
      "conv2.bias, gradient norm: 1.5528250685292733e-08\n",
      "conv2.lin.weight, gradient norm: 0.028769096359610558\n",
      "bn2.module.weight, gradient norm: 0.016422970220446587\n",
      "bn2.module.bias, gradient norm: 0.007053987588733435\n",
      "conv3.bias, gradient norm: 0.011125301942229271\n",
      "conv3.lin.weight, gradient norm: 0.018754243850708008\n",
      "Epoch: 233, Training Loss: 0.1773, Validation Loss: 0.2730, Train Acc: 0.9900, Val Acc: 0.9897, Test Acc: 0.9902\n",
      "conv1.bias, gradient norm: 5.815932180297523e-10\n",
      "conv1.lin.weight, gradient norm: 0.03271696716547012\n",
      "bn1.module.weight, gradient norm: 0.017492277547717094\n",
      "bn1.module.bias, gradient norm: 0.014094182290136814\n",
      "conv2.bias, gradient norm: 2.6465032831879398e-08\n",
      "conv2.lin.weight, gradient norm: 0.02422657050192356\n",
      "bn2.module.weight, gradient norm: 0.01661420427262783\n",
      "bn2.module.bias, gradient norm: 0.007826365530490875\n",
      "conv3.bias, gradient norm: 0.011462478898465633\n",
      "conv3.lin.weight, gradient norm: 0.020472174510359764\n",
      "Epoch: 234, Training Loss: 0.1740, Validation Loss: 0.2735, Train Acc: 0.9899, Val Acc: 0.9896, Test Acc: 0.9902\n",
      "conv1.bias, gradient norm: 5.54251866624611e-10\n",
      "conv1.lin.weight, gradient norm: 0.03588622435927391\n",
      "bn1.module.weight, gradient norm: 0.020491432398557663\n",
      "bn1.module.bias, gradient norm: 0.0151567617431283\n",
      "conv2.bias, gradient norm: 2.3900607715177102e-08\n",
      "conv2.lin.weight, gradient norm: 0.027043955400586128\n",
      "bn2.module.weight, gradient norm: 0.016512710601091385\n",
      "bn2.module.bias, gradient norm: 0.008290414698421955\n",
      "conv3.bias, gradient norm: 0.010745053179562092\n",
      "conv3.lin.weight, gradient norm: 0.01894085481762886\n",
      "Epoch: 235, Training Loss: 0.1756, Validation Loss: 0.2747, Train Acc: 0.9897, Val Acc: 0.9895, Test Acc: 0.9901\n",
      "conv1.bias, gradient norm: 4.5510414881100303e-10\n",
      "conv1.lin.weight, gradient norm: 0.031290456652641296\n",
      "bn1.module.weight, gradient norm: 0.017088163644075394\n",
      "bn1.module.bias, gradient norm: 0.012302789837121964\n",
      "conv2.bias, gradient norm: 2.519573172321543e-08\n",
      "conv2.lin.weight, gradient norm: 0.028650252148509026\n",
      "bn2.module.weight, gradient norm: 0.016750438138842583\n",
      "bn2.module.bias, gradient norm: 0.00982834305614233\n",
      "conv3.bias, gradient norm: 0.010547596029937267\n",
      "conv3.lin.weight, gradient norm: 0.019064808264374733\n",
      "Epoch: 236, Training Loss: 0.1751, Validation Loss: 0.2766, Train Acc: 0.9896, Val Acc: 0.9894, Test Acc: 0.9900\n",
      "conv1.bias, gradient norm: 4.750871918091093e-10\n",
      "conv1.lin.weight, gradient norm: 0.04195239767432213\n",
      "bn1.module.weight, gradient norm: 0.02523023821413517\n",
      "bn1.module.bias, gradient norm: 0.015360428020358086\n",
      "conv2.bias, gradient norm: 2.898303108622713e-08\n",
      "conv2.lin.weight, gradient norm: 0.03577565774321556\n",
      "bn2.module.weight, gradient norm: 0.017379015684127808\n",
      "bn2.module.bias, gradient norm: 0.011994942091405392\n",
      "conv3.bias, gradient norm: 0.009493217803537846\n",
      "conv3.lin.weight, gradient norm: 0.021954530850052834\n",
      "Epoch: 237, Training Loss: 0.1739, Validation Loss: 0.2805, Train Acc: 0.9897, Val Acc: 0.9895, Test Acc: 0.9901\n",
      "conv1.bias, gradient norm: 3.1537841782558473e-10\n",
      "conv1.lin.weight, gradient norm: 0.035260267555713654\n",
      "bn1.module.weight, gradient norm: 0.024067742750048637\n",
      "bn1.module.bias, gradient norm: 0.015190299600362778\n",
      "conv2.bias, gradient norm: 1.4128651137923498e-08\n",
      "conv2.lin.weight, gradient norm: 0.024999303743243217\n",
      "bn2.module.weight, gradient norm: 0.017255384474992752\n",
      "bn2.module.bias, gradient norm: 0.009229758754372597\n",
      "conv3.bias, gradient norm: 0.01039938349276781\n",
      "conv3.lin.weight, gradient norm: 0.01934645138680935\n",
      "Epoch: 238, Training Loss: 0.1754, Validation Loss: 0.2830, Train Acc: 0.9899, Val Acc: 0.9897, Test Acc: 0.9903\n",
      "conv1.bias, gradient norm: 5.985816287079615e-10\n",
      "conv1.lin.weight, gradient norm: 0.050227679312229156\n",
      "bn1.module.weight, gradient norm: 0.03525440767407417\n",
      "bn1.module.bias, gradient norm: 0.022346559911966324\n",
      "conv2.bias, gradient norm: 2.3950454064447513e-08\n",
      "conv2.lin.weight, gradient norm: 0.04804784059524536\n",
      "bn2.module.weight, gradient norm: 0.017073817551136017\n",
      "bn2.module.bias, gradient norm: 0.006520556751638651\n",
      "conv3.bias, gradient norm: 0.012235659174621105\n",
      "conv3.lin.weight, gradient norm: 0.02322310581803322\n",
      "Epoch: 239, Training Loss: 0.1735, Validation Loss: 0.2815, Train Acc: 0.9898, Val Acc: 0.9896, Test Acc: 0.9902\n",
      "conv1.bias, gradient norm: 6.455772583180419e-10\n",
      "conv1.lin.weight, gradient norm: 0.03151816502213478\n",
      "bn1.module.weight, gradient norm: 0.03770717233419418\n",
      "bn1.module.bias, gradient norm: 0.02236652560532093\n",
      "conv2.bias, gradient norm: 3.451355823358426e-08\n",
      "conv2.lin.weight, gradient norm: 0.02983953058719635\n",
      "bn2.module.weight, gradient norm: 0.01615777239203453\n",
      "bn2.module.bias, gradient norm: 0.00826379843056202\n",
      "conv3.bias, gradient norm: 0.010919117368757725\n",
      "conv3.lin.weight, gradient norm: 0.0185741875320673\n",
      "Epoch: 240, Training Loss: 0.1756, Validation Loss: 0.2804, Train Acc: 0.9895, Val Acc: 0.9895, Test Acc: 0.9898\n",
      "conv1.bias, gradient norm: 4.3267497917831577e-10\n",
      "conv1.lin.weight, gradient norm: 0.03529337793588638\n",
      "bn1.module.weight, gradient norm: 0.04535042494535446\n",
      "bn1.module.bias, gradient norm: 0.025191189721226692\n",
      "conv2.bias, gradient norm: 2.8878055502445932e-08\n",
      "conv2.lin.weight, gradient norm: 0.032032281160354614\n",
      "bn2.module.weight, gradient norm: 0.01723484694957733\n",
      "bn2.module.bias, gradient norm: 0.010736706666648388\n",
      "conv3.bias, gradient norm: 0.009951673448085785\n",
      "conv3.lin.weight, gradient norm: 0.020722433924674988\n",
      "Epoch: 241, Training Loss: 0.1745, Validation Loss: 0.2813, Train Acc: 0.9894, Val Acc: 0.9893, Test Acc: 0.9897\n",
      "conv1.bias, gradient norm: 4.189724400749384e-10\n",
      "conv1.lin.weight, gradient norm: 0.03739786148071289\n",
      "bn1.module.weight, gradient norm: 0.03012794442474842\n",
      "bn1.module.bias, gradient norm: 0.017052510753273964\n",
      "conv2.bias, gradient norm: 2.7273566516328174e-08\n",
      "conv2.lin.weight, gradient norm: 0.03659999743103981\n",
      "bn2.module.weight, gradient norm: 0.018310189247131348\n",
      "bn2.module.bias, gradient norm: 0.011119907721877098\n",
      "conv3.bias, gradient norm: 0.009436100721359253\n",
      "conv3.lin.weight, gradient norm: 0.022411497309803963\n",
      "Epoch: 242, Training Loss: 0.1729, Validation Loss: 0.2826, Train Acc: 0.9895, Val Acc: 0.9893, Test Acc: 0.9897\n",
      "conv1.bias, gradient norm: 4.220375438013235e-10\n",
      "conv1.lin.weight, gradient norm: 0.034296371042728424\n",
      "bn1.module.weight, gradient norm: 0.03604453429579735\n",
      "bn1.module.bias, gradient norm: 0.016542548313736916\n",
      "conv2.bias, gradient norm: 3.373353507640786e-08\n",
      "conv2.lin.weight, gradient norm: 0.03286318480968475\n",
      "bn2.module.weight, gradient norm: 0.01788478158414364\n",
      "bn2.module.bias, gradient norm: 0.0076585435308516026\n",
      "conv3.bias, gradient norm: 0.011385353282094002\n",
      "conv3.lin.weight, gradient norm: 0.02024483121931553\n",
      "Epoch: 243, Training Loss: 0.1708, Validation Loss: 0.2826, Train Acc: 0.9896, Val Acc: 0.9894, Test Acc: 0.9900\n",
      "conv1.bias, gradient norm: 5.544342207564057e-10\n",
      "conv1.lin.weight, gradient norm: 0.042759932577610016\n",
      "bn1.module.weight, gradient norm: 0.024655021727085114\n",
      "bn1.module.bias, gradient norm: 0.017501698806881905\n",
      "conv2.bias, gradient norm: 3.747578958268605e-08\n",
      "conv2.lin.weight, gradient norm: 0.027959471568465233\n",
      "bn2.module.weight, gradient norm: 0.017842082306742668\n",
      "bn2.module.bias, gradient norm: 0.007428152486681938\n",
      "conv3.bias, gradient norm: 0.011483066715300083\n",
      "conv3.lin.weight, gradient norm: 0.020987382158637047\n",
      "Epoch: 244, Training Loss: 0.1733, Validation Loss: 0.2800, Train Acc: 0.9896, Val Acc: 0.9894, Test Acc: 0.9901\n",
      "conv1.bias, gradient norm: 5.4445847830209e-10\n",
      "conv1.lin.weight, gradient norm: 0.03892001882195473\n",
      "bn1.module.weight, gradient norm: 0.018798355013132095\n",
      "bn1.module.bias, gradient norm: 0.019804203882813454\n",
      "conv2.bias, gradient norm: 2.9312687388483027e-08\n",
      "conv2.lin.weight, gradient norm: 0.02854926697909832\n",
      "bn2.module.weight, gradient norm: 0.018033210188150406\n",
      "bn2.module.bias, gradient norm: 0.007289552129805088\n",
      "conv3.bias, gradient norm: 0.011546854861080647\n",
      "conv3.lin.weight, gradient norm: 0.020505812019109726\n",
      "Epoch: 245, Training Loss: 0.1712, Validation Loss: 0.2774, Train Acc: 0.9896, Val Acc: 0.9895, Test Acc: 0.9901\n",
      "conv1.bias, gradient norm: 4.5808348780873587e-10\n",
      "conv1.lin.weight, gradient norm: 0.04040568694472313\n",
      "bn1.module.weight, gradient norm: 0.026885680854320526\n",
      "bn1.module.bias, gradient norm: 0.013997931964695454\n",
      "conv2.bias, gradient norm: 4.145121224041759e-08\n",
      "conv2.lin.weight, gradient norm: 0.0282159261405468\n",
      "bn2.module.weight, gradient norm: 0.01809610426425934\n",
      "bn2.module.bias, gradient norm: 0.008971684612333775\n",
      "conv3.bias, gradient norm: 0.010644535534083843\n",
      "conv3.lin.weight, gradient norm: 0.020136015489697456\n",
      "Epoch: 246, Training Loss: 0.1710, Validation Loss: 0.2751, Train Acc: 0.9896, Val Acc: 0.9895, Test Acc: 0.9901\n",
      "conv1.bias, gradient norm: 6.711291522520924e-10\n",
      "conv1.lin.weight, gradient norm: 0.0454232431948185\n",
      "bn1.module.weight, gradient norm: 0.028184965252876282\n",
      "bn1.module.bias, gradient norm: 0.017374154180288315\n",
      "conv2.bias, gradient norm: 2.1300504471355453e-08\n",
      "conv2.lin.weight, gradient norm: 0.034425657242536545\n",
      "bn2.module.weight, gradient norm: 0.01660061627626419\n",
      "bn2.module.bias, gradient norm: 0.01035055611282587\n",
      "conv3.bias, gradient norm: 0.010367102921009064\n",
      "conv3.lin.weight, gradient norm: 0.019411545246839523\n",
      "Epoch: 247, Training Loss: 0.1705, Validation Loss: 0.2728, Train Acc: 0.9896, Val Acc: 0.9895, Test Acc: 0.9902\n",
      "conv1.bias, gradient norm: 7.061897178139986e-10\n",
      "conv1.lin.weight, gradient norm: 0.043774254620075226\n",
      "bn1.module.weight, gradient norm: 0.03625953570008278\n",
      "bn1.module.bias, gradient norm: 0.020616507157683372\n",
      "conv2.bias, gradient norm: 3.45187132211322e-08\n",
      "conv2.lin.weight, gradient norm: 0.03666502982378006\n",
      "bn2.module.weight, gradient norm: 0.017319031059741974\n",
      "bn2.module.bias, gradient norm: 0.010369621217250824\n",
      "conv3.bias, gradient norm: 0.009784567169845104\n",
      "conv3.lin.weight, gradient norm: 0.01995893195271492\n",
      "Epoch: 248, Training Loss: 0.1725, Validation Loss: 0.2747, Train Acc: 0.9898, Val Acc: 0.9897, Test Acc: 0.9903\n",
      "conv1.bias, gradient norm: 7.683890745902033e-10\n",
      "conv1.lin.weight, gradient norm: 0.07030577957630157\n",
      "bn1.module.weight, gradient norm: 0.03599895164370537\n",
      "bn1.module.bias, gradient norm: 0.038347382098436356\n",
      "conv2.bias, gradient norm: 4.019296895307889e-08\n",
      "conv2.lin.weight, gradient norm: 0.051747266203165054\n",
      "bn2.module.weight, gradient norm: 0.018055610358715057\n",
      "bn2.module.bias, gradient norm: 0.007355374749749899\n",
      "conv3.bias, gradient norm: 0.011754035018384457\n",
      "conv3.lin.weight, gradient norm: 0.024381259456276894\n",
      "Epoch: 249, Training Loss: 0.1727, Validation Loss: 0.2797, Train Acc: 0.9896, Val Acc: 0.9894, Test Acc: 0.9901\n",
      "conv1.bias, gradient norm: 7.210913532951224e-10\n",
      "conv1.lin.weight, gradient norm: 0.0328645296394825\n",
      "bn1.module.weight, gradient norm: 0.022247198969125748\n",
      "bn1.module.bias, gradient norm: 0.011751904152333736\n",
      "conv2.bias, gradient norm: 3.4408557780807314e-08\n",
      "conv2.lin.weight, gradient norm: 0.026218723505735397\n",
      "bn2.module.weight, gradient norm: 0.016762040555477142\n",
      "bn2.module.bias, gradient norm: 0.00919242762029171\n",
      "conv3.bias, gradient norm: 0.010455497540533543\n",
      "conv3.lin.weight, gradient norm: 0.019458677619695663\n",
      "Epoch: 250, Training Loss: 0.1724, Validation Loss: 0.2831, Train Acc: 0.9894, Val Acc: 0.9893, Test Acc: 0.9899\n",
      "conv1.bias, gradient norm: 1.0344172185483558e-09\n",
      "conv1.lin.weight, gradient norm: 0.0565301775932312\n",
      "bn1.module.weight, gradient norm: 0.048037949949502945\n",
      "bn1.module.bias, gradient norm: 0.035274017602205276\n",
      "conv2.bias, gradient norm: 2.3623133671435426e-08\n",
      "conv2.lin.weight, gradient norm: 0.04154912382364273\n",
      "bn2.module.weight, gradient norm: 0.016621742397546768\n",
      "bn2.module.bias, gradient norm: 0.011825782246887684\n",
      "conv3.bias, gradient norm: 0.009979136288166046\n",
      "conv3.lin.weight, gradient norm: 0.01953907310962677\n",
      "Epoch: 251, Training Loss: 0.1710, Validation Loss: 0.2826, Train Acc: 0.9896, Val Acc: 0.9893, Test Acc: 0.9900\n",
      "conv1.bias, gradient norm: 6.464688784291184e-10\n",
      "conv1.lin.weight, gradient norm: 0.039888862520456314\n",
      "bn1.module.weight, gradient norm: 0.027992770075798035\n",
      "bn1.module.bias, gradient norm: 0.017075153067708015\n",
      "conv2.bias, gradient norm: 3.12262251611628e-08\n",
      "conv2.lin.weight, gradient norm: 0.03555148467421532\n",
      "bn2.module.weight, gradient norm: 0.017015967518091202\n",
      "bn2.module.bias, gradient norm: 0.0102706179022789\n",
      "conv3.bias, gradient norm: 0.01060079038143158\n",
      "conv3.lin.weight, gradient norm: 0.019794810563325882\n",
      "Epoch: 252, Training Loss: 0.1716, Validation Loss: 0.2807, Train Acc: 0.9897, Val Acc: 0.9895, Test Acc: 0.9902\n",
      "conv1.bias, gradient norm: 5.737988972853714e-10\n",
      "conv1.lin.weight, gradient norm: 0.044872600585222244\n",
      "bn1.module.weight, gradient norm: 0.04777843505144119\n",
      "bn1.module.bias, gradient norm: 0.028512639924883842\n",
      "conv2.bias, gradient norm: 4.412079235294186e-08\n",
      "conv2.lin.weight, gradient norm: 0.03245409578084946\n",
      "bn2.module.weight, gradient norm: 0.01781027764081955\n",
      "bn2.module.bias, gradient norm: 0.007190377917140722\n",
      "conv3.bias, gradient norm: 0.011793537065386772\n",
      "conv3.lin.weight, gradient norm: 0.022825291380286217\n",
      "Epoch: 253, Training Loss: 0.1715, Validation Loss: 0.2774, Train Acc: 0.9898, Val Acc: 0.9896, Test Acc: 0.9903\n",
      "conv1.bias, gradient norm: 8.655408612057158e-10\n",
      "conv1.lin.weight, gradient norm: 0.07413004338741302\n",
      "bn1.module.weight, gradient norm: 0.05523459240794182\n",
      "bn1.module.bias, gradient norm: 0.031234659254550934\n",
      "conv2.bias, gradient norm: 2.903669837905909e-08\n",
      "conv2.lin.weight, gradient norm: 0.03470737859606743\n",
      "bn2.module.weight, gradient norm: 0.017116669565439224\n",
      "bn2.module.bias, gradient norm: 0.006553343962877989\n",
      "conv3.bias, gradient norm: 0.012153983116149902\n",
      "conv3.lin.weight, gradient norm: 0.02229994162917137\n",
      "Epoch: 254, Training Loss: 0.1721, Validation Loss: 0.2743, Train Acc: 0.9897, Val Acc: 0.9895, Test Acc: 0.9902\n",
      "conv1.bias, gradient norm: 6.973449595548686e-10\n",
      "conv1.lin.weight, gradient norm: 0.042952973395586014\n",
      "bn1.module.weight, gradient norm: 0.05979236215353012\n",
      "bn1.module.bias, gradient norm: 0.030413154512643814\n",
      "conv2.bias, gradient norm: 4.051554114425926e-08\n",
      "conv2.lin.weight, gradient norm: 0.030475959181785583\n",
      "bn2.module.weight, gradient norm: 0.017917968332767487\n",
      "bn2.module.bias, gradient norm: 0.008030909113585949\n",
      "conv3.bias, gradient norm: 0.01109377946704626\n",
      "conv3.lin.weight, gradient norm: 0.021287037059664726\n",
      "Epoch: 255, Training Loss: 0.1707, Validation Loss: 0.2724, Train Acc: 0.9895, Val Acc: 0.9894, Test Acc: 0.9900\n",
      "conv1.bias, gradient norm: 7.162137549698855e-10\n",
      "conv1.lin.weight, gradient norm: 0.05949905514717102\n",
      "bn1.module.weight, gradient norm: 0.03907337784767151\n",
      "bn1.module.bias, gradient norm: 0.017078783363103867\n",
      "conv2.bias, gradient norm: 3.497597589330326e-08\n",
      "conv2.lin.weight, gradient norm: 0.030693503096699715\n",
      "bn2.module.weight, gradient norm: 0.017505737021565437\n",
      "bn2.module.bias, gradient norm: 0.009397577494382858\n",
      "conv3.bias, gradient norm: 0.009537451900541782\n",
      "conv3.lin.weight, gradient norm: 0.01950192078948021\n",
      "Epoch: 256, Training Loss: 0.1719, Validation Loss: 0.2703, Train Acc: 0.9895, Val Acc: 0.9894, Test Acc: 0.9900\n",
      "conv1.bias, gradient norm: 7.281326097619001e-10\n",
      "conv1.lin.weight, gradient norm: 0.05663323402404785\n",
      "bn1.module.weight, gradient norm: 0.03950294479727745\n",
      "bn1.module.bias, gradient norm: 0.020493466407060623\n",
      "conv2.bias, gradient norm: 4.707009892968017e-08\n",
      "conv2.lin.weight, gradient norm: 0.047422003000974655\n",
      "bn2.module.weight, gradient norm: 0.018173832446336746\n",
      "bn2.module.bias, gradient norm: 0.01108978595584631\n",
      "conv3.bias, gradient norm: 0.008884228765964508\n",
      "conv3.lin.weight, gradient norm: 0.022932268679142\n",
      "Epoch: 257, Training Loss: 0.1726, Validation Loss: 0.2687, Train Acc: 0.9898, Val Acc: 0.9895, Test Acc: 0.9903\n",
      "conv1.bias, gradient norm: 6.575693878296818e-10\n",
      "conv1.lin.weight, gradient norm: 0.04238801449537277\n",
      "bn1.module.weight, gradient norm: 0.042891159653663635\n",
      "bn1.module.bias, gradient norm: 0.01794360764324665\n",
      "conv2.bias, gradient norm: 4.7964949345669083e-08\n",
      "conv2.lin.weight, gradient norm: 0.032955437898635864\n",
      "bn2.module.weight, gradient norm: 0.01817852258682251\n",
      "bn2.module.bias, gradient norm: 0.008709040470421314\n",
      "conv3.bias, gradient norm: 0.01051083579659462\n",
      "conv3.lin.weight, gradient norm: 0.020015910267829895\n",
      "Epoch: 258, Training Loss: 0.1695, Validation Loss: 0.2683, Train Acc: 0.9900, Val Acc: 0.9897, Test Acc: 0.9904\n",
      "conv1.bias, gradient norm: 1.012019135160358e-09\n",
      "conv1.lin.weight, gradient norm: 0.0668976679444313\n",
      "bn1.module.weight, gradient norm: 0.03489305078983307\n",
      "bn1.module.bias, gradient norm: 0.026993729174137115\n",
      "conv2.bias, gradient norm: 4.206513537496903e-08\n",
      "conv2.lin.weight, gradient norm: 0.04010089859366417\n",
      "bn2.module.weight, gradient norm: 0.017674803733825684\n",
      "bn2.module.bias, gradient norm: 0.0073378561064600945\n",
      "conv3.bias, gradient norm: 0.011774685233831406\n",
      "conv3.lin.weight, gradient norm: 0.022443154826760292\n",
      "Epoch: 259, Training Loss: 0.1706, Validation Loss: 0.2683, Train Acc: 0.9899, Val Acc: 0.9897, Test Acc: 0.9904\n",
      "conv1.bias, gradient norm: 8.662517925195345e-10\n",
      "conv1.lin.weight, gradient norm: 0.05419861897826195\n",
      "bn1.module.weight, gradient norm: 0.03168535977602005\n",
      "bn1.module.bias, gradient norm: 0.020291542634367943\n",
      "conv2.bias, gradient norm: 4.153417521024494e-08\n",
      "conv2.lin.weight, gradient norm: 0.033131759613752365\n",
      "bn2.module.weight, gradient norm: 0.017253387719392776\n",
      "bn2.module.bias, gradient norm: 0.007544043939560652\n",
      "conv3.bias, gradient norm: 0.011122227646410465\n",
      "conv3.lin.weight, gradient norm: 0.02033034712076187\n",
      "Epoch: 260, Training Loss: 0.1701, Validation Loss: 0.2679, Train Acc: 0.9897, Val Acc: 0.9895, Test Acc: 0.9901\n",
      "conv1.bias, gradient norm: 7.176189642521535e-10\n",
      "conv1.lin.weight, gradient norm: 0.05009745433926582\n",
      "bn1.module.weight, gradient norm: 0.05378090962767601\n",
      "bn1.module.bias, gradient norm: 0.025879785418510437\n",
      "conv2.bias, gradient norm: 5.281048487404405e-08\n",
      "conv2.lin.weight, gradient norm: 0.03907522186636925\n",
      "bn2.module.weight, gradient norm: 0.018571611493825912\n",
      "bn2.module.bias, gradient norm: 0.010702972300350666\n",
      "conv3.bias, gradient norm: 0.009257092140614986\n",
      "conv3.lin.weight, gradient norm: 0.024639898911118507\n",
      "Epoch: 261, Training Loss: 0.1699, Validation Loss: 0.2677, Train Acc: 0.9897, Val Acc: 0.9894, Test Acc: 0.9902\n",
      "conv1.bias, gradient norm: 5.829088323139331e-10\n",
      "conv1.lin.weight, gradient norm: 0.05457805469632149\n",
      "bn1.module.weight, gradient norm: 0.021567214280366898\n",
      "bn1.module.bias, gradient norm: 0.016917511820793152\n",
      "conv2.bias, gradient norm: 4.4458193571017546e-08\n",
      "conv2.lin.weight, gradient norm: 0.038942862302064896\n",
      "bn2.module.weight, gradient norm: 0.01738164946436882\n",
      "bn2.module.bias, gradient norm: 0.011108389124274254\n",
      "conv3.bias, gradient norm: 0.009687274694442749\n",
      "conv3.lin.weight, gradient norm: 0.021081818267703056\n",
      "Epoch: 262, Training Loss: 0.1690, Validation Loss: 0.2679, Train Acc: 0.9899, Val Acc: 0.9895, Test Acc: 0.9902\n",
      "conv1.bias, gradient norm: 1.0663812055611288e-09\n",
      "conv1.lin.weight, gradient norm: 0.05068761482834816\n",
      "bn1.module.weight, gradient norm: 0.048697032034397125\n",
      "bn1.module.bias, gradient norm: 0.026697013527154922\n",
      "conv2.bias, gradient norm: 4.600044434255324e-08\n",
      "conv2.lin.weight, gradient norm: 0.03302730619907379\n",
      "bn2.module.weight, gradient norm: 0.016330428421497345\n",
      "bn2.module.bias, gradient norm: 0.008582117967307568\n",
      "conv3.bias, gradient norm: 0.010005149990320206\n",
      "conv3.lin.weight, gradient norm: 0.018261786550283432\n",
      "Epoch: 263, Training Loss: 0.1708, Validation Loss: 0.2682, Train Acc: 0.9900, Val Acc: 0.9898, Test Acc: 0.9903\n",
      "conv1.bias, gradient norm: 7.910578858627559e-10\n",
      "conv1.lin.weight, gradient norm: 0.06192057579755783\n",
      "bn1.module.weight, gradient norm: 0.0760837122797966\n",
      "bn1.module.bias, gradient norm: 0.039424728602170944\n",
      "conv2.bias, gradient norm: 4.1531400540861796e-08\n",
      "conv2.lin.weight, gradient norm: 0.04443146660923958\n",
      "bn2.module.weight, gradient norm: 0.017326481640338898\n",
      "bn2.module.bias, gradient norm: 0.006491746753454208\n",
      "conv3.bias, gradient norm: 0.011965210549533367\n",
      "conv3.lin.weight, gradient norm: 0.020866891369223595\n",
      "Epoch: 264, Training Loss: 0.1692, Validation Loss: 0.2676, Train Acc: 0.9899, Val Acc: 0.9897, Test Acc: 0.9902\n",
      "conv1.bias, gradient norm: 7.415624225792783e-10\n",
      "conv1.lin.weight, gradient norm: 0.07159562408924103\n",
      "bn1.module.weight, gradient norm: 0.026102907955646515\n",
      "bn1.module.bias, gradient norm: 0.02455839514732361\n",
      "conv2.bias, gradient norm: 4.860880054025074e-08\n",
      "conv2.lin.weight, gradient norm: 0.03500499203801155\n",
      "bn2.module.weight, gradient norm: 0.017493480816483498\n",
      "bn2.module.bias, gradient norm: 0.007676427718251944\n",
      "conv3.bias, gradient norm: 0.010581026785075665\n",
      "conv3.lin.weight, gradient norm: 0.01951693184673786\n",
      "Epoch: 265, Training Loss: 0.1715, Validation Loss: 0.2677, Train Acc: 0.9896, Val Acc: 0.9894, Test Acc: 0.9899\n",
      "conv1.bias, gradient norm: 6.447850031676694e-10\n",
      "conv1.lin.weight, gradient norm: 0.07336457073688507\n",
      "bn1.module.weight, gradient norm: 0.05038946121931076\n",
      "bn1.module.bias, gradient norm: 0.028153369203209877\n",
      "conv2.bias, gradient norm: 5.381700418638502e-08\n",
      "conv2.lin.weight, gradient norm: 0.05221344158053398\n",
      "bn2.module.weight, gradient norm: 0.018259214237332344\n",
      "bn2.module.bias, gradient norm: 0.011362827382981777\n",
      "conv3.bias, gradient norm: 0.009044724516570568\n",
      "conv3.lin.weight, gradient norm: 0.025486597791314125\n",
      "Epoch: 266, Training Loss: 0.1703, Validation Loss: 0.2681, Train Acc: 0.9897, Val Acc: 0.9894, Test Acc: 0.9900\n",
      "conv1.bias, gradient norm: 7.791823297687017e-10\n",
      "conv1.lin.weight, gradient norm: 0.06136903166770935\n",
      "bn1.module.weight, gradient norm: 0.03408987075090408\n",
      "bn1.module.bias, gradient norm: 0.015421077609062195\n",
      "conv2.bias, gradient norm: 4.171780076944742e-08\n",
      "conv2.lin.weight, gradient norm: 0.04588660970330238\n",
      "bn2.module.weight, gradient norm: 0.01878637820482254\n",
      "bn2.module.bias, gradient norm: 0.009243077598512173\n",
      "conv3.bias, gradient norm: 0.00967920757830143\n",
      "conv3.lin.weight, gradient norm: 0.0223609060049057\n",
      "Epoch: 267, Training Loss: 0.1685, Validation Loss: 0.2700, Train Acc: 0.9900, Val Acc: 0.9897, Test Acc: 0.9903\n",
      "conv1.bias, gradient norm: 8.721594002558675e-10\n",
      "conv1.lin.weight, gradient norm: 0.051166240125894547\n",
      "bn1.module.weight, gradient norm: 0.025354163721203804\n",
      "bn1.module.bias, gradient norm: 0.01921163871884346\n",
      "conv2.bias, gradient norm: 4.2652040121993195e-08\n",
      "conv2.lin.weight, gradient norm: 0.03459572419524193\n",
      "bn2.module.weight, gradient norm: 0.01685786060988903\n",
      "bn2.module.bias, gradient norm: 0.007280326914042234\n",
      "conv3.bias, gradient norm: 0.011469549499452114\n",
      "conv3.lin.weight, gradient norm: 0.02317020110785961\n",
      "Epoch: 268, Training Loss: 0.1701, Validation Loss: 0.2709, Train Acc: 0.9901, Val Acc: 0.9897, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.176772346234145e-09\n",
      "conv1.lin.weight, gradient norm: 0.07232198864221573\n",
      "bn1.module.weight, gradient norm: 0.0512499175965786\n",
      "bn1.module.bias, gradient norm: 0.03477666899561882\n",
      "conv2.bias, gradient norm: 3.9831100195897307e-08\n",
      "conv2.lin.weight, gradient norm: 0.07802509516477585\n",
      "bn2.module.weight, gradient norm: 0.016013428568840027\n",
      "bn2.module.bias, gradient norm: 0.00648856908082962\n",
      "conv3.bias, gradient norm: 0.011713451705873013\n",
      "conv3.lin.weight, gradient norm: 0.023698540404438972\n",
      "Epoch: 269, Training Loss: 0.1721, Validation Loss: 0.2703, Train Acc: 0.9899, Val Acc: 0.9896, Test Acc: 0.9904\n",
      "conv1.bias, gradient norm: 9.453887672705719e-10\n",
      "conv1.lin.weight, gradient norm: 0.08025913685560226\n",
      "bn1.module.weight, gradient norm: 0.04284140467643738\n",
      "bn1.module.bias, gradient norm: 0.023167632520198822\n",
      "conv2.bias, gradient norm: 5.082978660198023e-08\n",
      "conv2.lin.weight, gradient norm: 0.03696908801794052\n",
      "bn2.module.weight, gradient norm: 0.017344430088996887\n",
      "bn2.module.bias, gradient norm: 0.008485062047839165\n",
      "conv3.bias, gradient norm: 0.009438912384212017\n",
      "conv3.lin.weight, gradient norm: 0.01926051452755928\n",
      "Epoch: 270, Training Loss: 0.1712, Validation Loss: 0.2676, Train Acc: 0.9898, Val Acc: 0.9897, Test Acc: 0.9902\n",
      "conv1.bias, gradient norm: 8.354063552040714e-10\n",
      "conv1.lin.weight, gradient norm: 0.051819562911987305\n",
      "bn1.module.weight, gradient norm: 0.030093442648649216\n",
      "bn1.module.bias, gradient norm: 0.010507960803806782\n",
      "conv2.bias, gradient norm: 3.8431164028907006e-08\n",
      "conv2.lin.weight, gradient norm: 0.033525627106428146\n",
      "bn2.module.weight, gradient norm: 0.017328603193163872\n",
      "bn2.module.bias, gradient norm: 0.010182764381170273\n",
      "conv3.bias, gradient norm: 0.009776011109352112\n",
      "conv3.lin.weight, gradient norm: 0.020715007558465004\n",
      "Epoch: 271, Training Loss: 0.1716, Validation Loss: 0.2656, Train Acc: 0.9899, Val Acc: 0.9897, Test Acc: 0.9904\n",
      "conv1.bias, gradient norm: 7.69611041562257e-10\n",
      "conv1.lin.weight, gradient norm: 0.06639230251312256\n",
      "bn1.module.weight, gradient norm: 0.0476982481777668\n",
      "bn1.module.bias, gradient norm: 0.024742620065808296\n",
      "conv2.bias, gradient norm: 3.386185198905878e-08\n",
      "conv2.lin.weight, gradient norm: 0.04222030192613602\n",
      "bn2.module.weight, gradient norm: 0.01796775497496128\n",
      "bn2.module.bias, gradient norm: 0.010841196402907372\n",
      "conv3.bias, gradient norm: 0.00958930142223835\n",
      "conv3.lin.weight, gradient norm: 0.022418973967432976\n",
      "Epoch: 272, Training Loss: 0.1689, Validation Loss: 0.2660, Train Acc: 0.9902, Val Acc: 0.9899, Test Acc: 0.9908\n",
      "conv1.bias, gradient norm: 9.376410758932252e-10\n",
      "conv1.lin.weight, gradient norm: 0.09500668942928314\n",
      "bn1.module.weight, gradient norm: 0.032087989151477814\n",
      "bn1.module.bias, gradient norm: 0.028616497293114662\n",
      "conv2.bias, gradient norm: 4.055131697100478e-08\n",
      "conv2.lin.weight, gradient norm: 0.042034175246953964\n",
      "bn2.module.weight, gradient norm: 0.017048068344593048\n",
      "bn2.module.bias, gradient norm: 0.0070441653952002525\n",
      "conv3.bias, gradient norm: 0.01120917871594429\n",
      "conv3.lin.weight, gradient norm: 0.021942755207419395\n",
      "Epoch: 273, Training Loss: 0.1709, Validation Loss: 0.2667, Train Acc: 0.9901, Val Acc: 0.9898, Test Acc: 0.9907\n",
      "conv1.bias, gradient norm: 8.198570156103813e-10\n",
      "conv1.lin.weight, gradient norm: 0.06200424209237099\n",
      "bn1.module.weight, gradient norm: 0.02554519847035408\n",
      "bn1.module.bias, gradient norm: 0.016429143026471138\n",
      "conv2.bias, gradient norm: 5.791495638618471e-08\n",
      "conv2.lin.weight, gradient norm: 0.034323710948228836\n",
      "bn2.module.weight, gradient norm: 0.017692536115646362\n",
      "bn2.module.bias, gradient norm: 0.008892718702554703\n",
      "conv3.bias, gradient norm: 0.010340902023017406\n",
      "conv3.lin.weight, gradient norm: 0.019764631986618042\n",
      "Epoch: 274, Training Loss: 0.1668, Validation Loss: 0.2673, Train Acc: 0.9899, Val Acc: 0.9896, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 9.839118408905279e-10\n",
      "conv1.lin.weight, gradient norm: 0.10526780784130096\n",
      "bn1.module.weight, gradient norm: 0.04985658451914787\n",
      "bn1.module.bias, gradient norm: 0.031188370659947395\n",
      "conv2.bias, gradient norm: 4.108405349256827e-08\n",
      "conv2.lin.weight, gradient norm: 0.049881141632795334\n",
      "bn2.module.weight, gradient norm: 0.01729767583310604\n",
      "bn2.module.bias, gradient norm: 0.011580604128539562\n",
      "conv3.bias, gradient norm: 0.009158779866993427\n",
      "conv3.lin.weight, gradient norm: 0.021004483103752136\n",
      "Epoch: 275, Training Loss: 0.1707, Validation Loss: 0.2670, Train Acc: 0.9901, Val Acc: 0.9897, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 7.136376489746965e-10\n",
      "conv1.lin.weight, gradient norm: 0.0898938998579979\n",
      "bn1.module.weight, gradient norm: 0.05112806335091591\n",
      "bn1.module.bias, gradient norm: 0.038634851574897766\n",
      "conv2.bias, gradient norm: 3.85382179501903e-08\n",
      "conv2.lin.weight, gradient norm: 0.03664581850171089\n",
      "bn2.module.weight, gradient norm: 0.017469873651862144\n",
      "bn2.module.bias, gradient norm: 0.00721702678129077\n",
      "conv3.bias, gradient norm: 0.011029185727238655\n",
      "conv3.lin.weight, gradient norm: 0.020713405683636665\n",
      "Epoch: 276, Training Loss: 0.1690, Validation Loss: 0.2662, Train Acc: 0.9903, Val Acc: 0.9900, Test Acc: 0.9907\n",
      "conv1.bias, gradient norm: 1.5123581276199616e-09\n",
      "conv1.lin.weight, gradient norm: 0.11580048501491547\n",
      "bn1.module.weight, gradient norm: 0.039717644453048706\n",
      "bn1.module.bias, gradient norm: 0.03784014657139778\n",
      "conv2.bias, gradient norm: 5.6359837685704406e-08\n",
      "conv2.lin.weight, gradient norm: 0.05595453828573227\n",
      "bn2.module.weight, gradient norm: 0.016989706084132195\n",
      "bn2.module.bias, gradient norm: 0.006266667507588863\n",
      "conv3.bias, gradient norm: 0.01258098240941763\n",
      "conv3.lin.weight, gradient norm: 0.02738947607576847\n",
      "Epoch: 277, Training Loss: 0.1667, Validation Loss: 0.2659, Train Acc: 0.9901, Val Acc: 0.9897, Test Acc: 0.9904\n",
      "conv1.bias, gradient norm: 8.463099110400663e-10\n",
      "conv1.lin.weight, gradient norm: 0.06906134635210037\n",
      "bn1.module.weight, gradient norm: 0.040863532572984695\n",
      "bn1.module.bias, gradient norm: 0.02230018749833107\n",
      "conv2.bias, gradient norm: 4.4643758911888654e-08\n",
      "conv2.lin.weight, gradient norm: 0.0405002124607563\n",
      "bn2.module.weight, gradient norm: 0.017642783001065254\n",
      "bn2.module.bias, gradient norm: 0.009730741381645203\n",
      "conv3.bias, gradient norm: 0.010301021859049797\n",
      "conv3.lin.weight, gradient norm: 0.019564764574170113\n",
      "Epoch: 278, Training Loss: 0.1671, Validation Loss: 0.2642, Train Acc: 0.9899, Val Acc: 0.9895, Test Acc: 0.9902\n",
      "conv1.bias, gradient norm: 1.3636795026528148e-09\n",
      "conv1.lin.weight, gradient norm: 0.10199934244155884\n",
      "bn1.module.weight, gradient norm: 0.04379578307271004\n",
      "bn1.module.bias, gradient norm: 0.03728725016117096\n",
      "conv2.bias, gradient norm: 4.7805890801555506e-08\n",
      "conv2.lin.weight, gradient norm: 0.059963591396808624\n",
      "bn2.module.weight, gradient norm: 0.0169249065220356\n",
      "bn2.module.bias, gradient norm: 0.012815541587769985\n",
      "conv3.bias, gradient norm: 0.009181192144751549\n",
      "conv3.lin.weight, gradient norm: 0.024835124611854553\n",
      "Epoch: 279, Training Loss: 0.1676, Validation Loss: 0.2624, Train Acc: 0.9901, Val Acc: 0.9896, Test Acc: 0.9904\n",
      "conv1.bias, gradient norm: 1.3327670078666642e-09\n",
      "conv1.lin.weight, gradient norm: 0.06313573569059372\n",
      "bn1.module.weight, gradient norm: 0.0320071280002594\n",
      "bn1.module.bias, gradient norm: 0.01783214509487152\n",
      "conv2.bias, gradient norm: 4.314738788480099e-08\n",
      "conv2.lin.weight, gradient norm: 0.032433852553367615\n",
      "bn2.module.weight, gradient norm: 0.01717415452003479\n",
      "bn2.module.bias, gradient norm: 0.010402487590909004\n",
      "conv3.bias, gradient norm: 0.009503350593149662\n",
      "conv3.lin.weight, gradient norm: 0.019851408898830414\n",
      "Epoch: 280, Training Loss: 0.1696, Validation Loss: 0.2629, Train Acc: 0.9902, Val Acc: 0.9898, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.4599282893712484e-09\n",
      "conv1.lin.weight, gradient norm: 0.14265474677085876\n",
      "bn1.module.weight, gradient norm: 0.06527485698461533\n",
      "bn1.module.bias, gradient norm: 0.05037456378340721\n",
      "conv2.bias, gradient norm: 6.81216931752715e-08\n",
      "conv2.lin.weight, gradient norm: 0.07266451418399811\n",
      "bn2.module.weight, gradient norm: 0.017539922147989273\n",
      "bn2.module.bias, gradient norm: 0.007620276417583227\n",
      "conv3.bias, gradient norm: 0.01212993822991848\n",
      "conv3.lin.weight, gradient norm: 0.027434904128313065\n",
      "Epoch: 281, Training Loss: 0.1666, Validation Loss: 0.2649, Train Acc: 0.9899, Val Acc: 0.9894, Test Acc: 0.9902\n",
      "conv1.bias, gradient norm: 8.945601481791243e-10\n",
      "conv1.lin.weight, gradient norm: 0.06717580556869507\n",
      "bn1.module.weight, gradient norm: 0.026954278349876404\n",
      "bn1.module.bias, gradient norm: 0.018786722794175148\n",
      "conv2.bias, gradient norm: 5.102883449126239e-08\n",
      "conv2.lin.weight, gradient norm: 0.03607632592320442\n",
      "bn2.module.weight, gradient norm: 0.017215771600604057\n",
      "bn2.module.bias, gradient norm: 0.009709281846880913\n",
      "conv3.bias, gradient norm: 0.009906303137540817\n",
      "conv3.lin.weight, gradient norm: 0.019690468907356262\n",
      "Epoch: 282, Training Loss: 0.1656, Validation Loss: 0.2676, Train Acc: 0.9898, Val Acc: 0.9893, Test Acc: 0.9901\n",
      "conv1.bias, gradient norm: 8.570517073813733e-10\n",
      "conv1.lin.weight, gradient norm: 0.07341450452804565\n",
      "bn1.module.weight, gradient norm: 0.04701748117804527\n",
      "bn1.module.bias, gradient norm: 0.026553837582468987\n",
      "conv2.bias, gradient norm: 4.8135905927892964e-08\n",
      "conv2.lin.weight, gradient norm: 0.04095066711306572\n",
      "bn2.module.weight, gradient norm: 0.016769569367170334\n",
      "bn2.module.bias, gradient norm: 0.010074644349515438\n",
      "conv3.bias, gradient norm: 0.009832964278757572\n",
      "conv3.lin.weight, gradient norm: 0.01961141638457775\n",
      "Epoch: 283, Training Loss: 0.1689, Validation Loss: 0.2684, Train Acc: 0.9900, Val Acc: 0.9897, Test Acc: 0.9904\n",
      "conv1.bias, gradient norm: 1.2442420427305478e-09\n",
      "conv1.lin.weight, gradient norm: 0.07529228925704956\n",
      "bn1.module.weight, gradient norm: 0.03343023359775543\n",
      "bn1.module.bias, gradient norm: 0.02882038615643978\n",
      "conv2.bias, gradient norm: 3.4606497223421684e-08\n",
      "conv2.lin.weight, gradient norm: 0.029196081683039665\n",
      "bn2.module.weight, gradient norm: 0.0172076728194952\n",
      "bn2.module.bias, gradient norm: 0.008294690400362015\n",
      "conv3.bias, gradient norm: 0.010530809871852398\n",
      "conv3.lin.weight, gradient norm: 0.01909753866493702\n",
      "Epoch: 284, Training Loss: 0.1671, Validation Loss: 0.2693, Train Acc: 0.9901, Val Acc: 0.9899, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.3434834356118586e-09\n",
      "conv1.lin.weight, gradient norm: 0.12444672733545303\n",
      "bn1.module.weight, gradient norm: 0.04790673777461052\n",
      "bn1.module.bias, gradient norm: 0.04282807186245918\n",
      "conv2.bias, gradient norm: 4.287704413741267e-08\n",
      "conv2.lin.weight, gradient norm: 0.05457140877842903\n",
      "bn2.module.weight, gradient norm: 0.016449639573693275\n",
      "bn2.module.bias, gradient norm: 0.007061989512294531\n",
      "conv3.bias, gradient norm: 0.011527220718562603\n",
      "conv3.lin.weight, gradient norm: 0.02246989496052265\n",
      "Epoch: 285, Training Loss: 0.1697, Validation Loss: 0.2698, Train Acc: 0.9899, Val Acc: 0.9896, Test Acc: 0.9903\n",
      "conv1.bias, gradient norm: 1.6170315086938558e-09\n",
      "conv1.lin.weight, gradient norm: 0.09326081722974777\n",
      "bn1.module.weight, gradient norm: 0.04320337250828743\n",
      "bn1.module.bias, gradient norm: 0.027307139709591866\n",
      "conv2.bias, gradient norm: 4.487232985184164e-08\n",
      "conv2.lin.weight, gradient norm: 0.04119376093149185\n",
      "bn2.module.weight, gradient norm: 0.017415398731827736\n",
      "bn2.module.bias, gradient norm: 0.010468725115060806\n",
      "conv3.bias, gradient norm: 0.009411634877324104\n",
      "conv3.lin.weight, gradient norm: 0.021703369915485382\n",
      "Epoch: 286, Training Loss: 0.1683, Validation Loss: 0.2682, Train Acc: 0.9900, Val Acc: 0.9897, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 7.562390158533105e-10\n",
      "conv1.lin.weight, gradient norm: 0.08634510636329651\n",
      "bn1.module.weight, gradient norm: 0.038540925830602646\n",
      "bn1.module.bias, gradient norm: 0.0251386109739542\n",
      "conv2.bias, gradient norm: 3.221571986955496e-08\n",
      "conv2.lin.weight, gradient norm: 0.0503353625535965\n",
      "bn2.module.weight, gradient norm: 0.01643543317914009\n",
      "bn2.module.bias, gradient norm: 0.008479525335133076\n",
      "conv3.bias, gradient norm: 0.010449470020830631\n",
      "conv3.lin.weight, gradient norm: 0.018647773191332817\n",
      "Epoch: 287, Training Loss: 0.1685, Validation Loss: 0.2636, Train Acc: 0.9901, Val Acc: 0.9898, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 1.1605794103530798e-09\n",
      "conv1.lin.weight, gradient norm: 0.08813866972923279\n",
      "bn1.module.weight, gradient norm: 0.03131029009819031\n",
      "bn1.module.bias, gradient norm: 0.022393319755792618\n",
      "conv2.bias, gradient norm: 5.7524431440469925e-08\n",
      "conv2.lin.weight, gradient norm: 0.0340963713824749\n",
      "bn2.module.weight, gradient norm: 0.016711045056581497\n",
      "bn2.module.bias, gradient norm: 0.00826341938227415\n",
      "conv3.bias, gradient norm: 0.010969606228172779\n",
      "conv3.lin.weight, gradient norm: 0.0191383957862854\n",
      "Epoch: 288, Training Loss: 0.1680, Validation Loss: 0.2588, Train Acc: 0.9901, Val Acc: 0.9897, Test Acc: 0.9903\n",
      "conv1.bias, gradient norm: 2.0556092383827718e-09\n",
      "conv1.lin.weight, gradient norm: 0.08904310315847397\n",
      "bn1.module.weight, gradient norm: 0.04961436614394188\n",
      "bn1.module.bias, gradient norm: 0.030868256464600563\n",
      "conv2.bias, gradient norm: 5.991036289287877e-08\n",
      "conv2.lin.weight, gradient norm: 0.036113377660512924\n",
      "bn2.module.weight, gradient norm: 0.018121270462870598\n",
      "bn2.module.bias, gradient norm: 0.00907690916210413\n",
      "conv3.bias, gradient norm: 0.009581553749740124\n",
      "conv3.lin.weight, gradient norm: 0.02005964145064354\n",
      "Epoch: 289, Training Loss: 0.1676, Validation Loss: 0.2564, Train Acc: 0.9901, Val Acc: 0.9897, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 1.7110106664830482e-09\n",
      "conv1.lin.weight, gradient norm: 0.06986325979232788\n",
      "bn1.module.weight, gradient norm: 0.029492931440472603\n",
      "bn1.module.bias, gradient norm: 0.018492629751563072\n",
      "conv2.bias, gradient norm: 4.3417141881718635e-08\n",
      "conv2.lin.weight, gradient norm: 0.045211005955934525\n",
      "bn2.module.weight, gradient norm: 0.018460841849446297\n",
      "bn2.module.bias, gradient norm: 0.009378463961184025\n",
      "conv3.bias, gradient norm: 0.009781304746866226\n",
      "conv3.lin.weight, gradient norm: 0.020739464089274406\n",
      "Epoch: 290, Training Loss: 0.1666, Validation Loss: 0.2559, Train Acc: 0.9902, Val Acc: 0.9897, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 8.486574776256361e-10\n",
      "conv1.lin.weight, gradient norm: 0.1049238070845604\n",
      "bn1.module.weight, gradient norm: 0.06440408527851105\n",
      "bn1.module.bias, gradient norm: 0.04969579353928566\n",
      "conv2.bias, gradient norm: 5.504450939497474e-08\n",
      "conv2.lin.weight, gradient norm: 0.04090132191777229\n",
      "bn2.module.weight, gradient norm: 0.017234113067388535\n",
      "bn2.module.bias, gradient norm: 0.007691249251365662\n",
      "conv3.bias, gradient norm: 0.01035334076732397\n",
      "conv3.lin.weight, gradient norm: 0.019471926614642143\n",
      "Epoch: 291, Training Loss: 0.1700, Validation Loss: 0.2564, Train Acc: 0.9901, Val Acc: 0.9897, Test Acc: 0.9904\n",
      "conv1.bias, gradient norm: 1.2200308541210347e-09\n",
      "conv1.lin.weight, gradient norm: 0.0887347087264061\n",
      "bn1.module.weight, gradient norm: 0.03937845304608345\n",
      "bn1.module.bias, gradient norm: 0.0241457037627697\n",
      "conv2.bias, gradient norm: 5.228188726391636e-08\n",
      "conv2.lin.weight, gradient norm: 0.0351090133190155\n",
      "bn2.module.weight, gradient norm: 0.01776670478284359\n",
      "bn2.module.bias, gradient norm: 0.007708226330578327\n",
      "conv3.bias, gradient norm: 0.010711060836911201\n",
      "conv3.lin.weight, gradient norm: 0.020603179931640625\n",
      "Epoch: 292, Training Loss: 0.1653, Validation Loss: 0.2570, Train Acc: 0.9900, Val Acc: 0.9896, Test Acc: 0.9904\n",
      "conv1.bias, gradient norm: 1.2371177415815282e-09\n",
      "conv1.lin.weight, gradient norm: 0.08512003719806671\n",
      "bn1.module.weight, gradient norm: 0.043033916503190994\n",
      "bn1.module.bias, gradient norm: 0.025809282436966896\n",
      "conv2.bias, gradient norm: 5.703269678747347e-08\n",
      "conv2.lin.weight, gradient norm: 0.05284145846962929\n",
      "bn2.module.weight, gradient norm: 0.016588618978857994\n",
      "bn2.module.bias, gradient norm: 0.010292543098330498\n",
      "conv3.bias, gradient norm: 0.009459050372242928\n",
      "conv3.lin.weight, gradient norm: 0.020497605204582214\n",
      "Epoch: 293, Training Loss: 0.1679, Validation Loss: 0.2571, Train Acc: 0.9901, Val Acc: 0.9898, Test Acc: 0.9907\n",
      "conv1.bias, gradient norm: 1.4161800621081966e-09\n",
      "conv1.lin.weight, gradient norm: 0.10214393585920334\n",
      "bn1.module.weight, gradient norm: 0.06810027360916138\n",
      "bn1.module.bias, gradient norm: 0.039296478033065796\n",
      "conv2.bias, gradient norm: 5.4647696146048474e-08\n",
      "conv2.lin.weight, gradient norm: 0.04459511488676071\n",
      "bn2.module.weight, gradient norm: 0.016122279688715935\n",
      "bn2.module.bias, gradient norm: 0.008051379583775997\n",
      "conv3.bias, gradient norm: 0.01068896148353815\n",
      "conv3.lin.weight, gradient norm: 0.01820196956396103\n",
      "Epoch: 294, Training Loss: 0.1710, Validation Loss: 0.2553, Train Acc: 0.9901, Val Acc: 0.9898, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 7.722998907055967e-10\n",
      "conv1.lin.weight, gradient norm: 0.09222781658172607\n",
      "bn1.module.weight, gradient norm: 0.06099070608615875\n",
      "bn1.module.bias, gradient norm: 0.028232118114829063\n",
      "conv2.bias, gradient norm: 5.160516280966476e-08\n",
      "conv2.lin.weight, gradient norm: 0.03574654087424278\n",
      "bn2.module.weight, gradient norm: 0.017566366121172905\n",
      "bn2.module.bias, gradient norm: 0.009097750298678875\n",
      "conv3.bias, gradient norm: 0.009820038452744484\n",
      "conv3.lin.weight, gradient norm: 0.0212845541536808\n",
      "Epoch: 295, Training Loss: 0.1698, Validation Loss: 0.2551, Train Acc: 0.9902, Val Acc: 0.9899, Test Acc: 0.9908\n",
      "conv1.bias, gradient norm: 2.249771480222762e-09\n",
      "conv1.lin.weight, gradient norm: 0.14448365569114685\n",
      "bn1.module.weight, gradient norm: 0.054699599742889404\n",
      "bn1.module.bias, gradient norm: 0.03972957283258438\n",
      "conv2.bias, gradient norm: 6.97849387165661e-08\n",
      "conv2.lin.weight, gradient norm: 0.05576515197753906\n",
      "bn2.module.weight, gradient norm: 0.01859923265874386\n",
      "bn2.module.bias, gradient norm: 0.0069496724754571915\n",
      "conv3.bias, gradient norm: 0.010562078095972538\n",
      "conv3.lin.weight, gradient norm: 0.019511187449097633\n",
      "Epoch: 296, Training Loss: 0.1655, Validation Loss: 0.2562, Train Acc: 0.9901, Val Acc: 0.9898, Test Acc: 0.9908\n",
      "conv1.bias, gradient norm: 1.6152770232480407e-09\n",
      "conv1.lin.weight, gradient norm: 0.07337868958711624\n",
      "bn1.module.weight, gradient norm: 0.029603149741888046\n",
      "bn1.module.bias, gradient norm: 0.015561277978122234\n",
      "conv2.bias, gradient norm: 3.7137766639716574e-08\n",
      "conv2.lin.weight, gradient norm: 0.03914792463183403\n",
      "bn2.module.weight, gradient norm: 0.016032151877880096\n",
      "bn2.module.bias, gradient norm: 0.009006490930914879\n",
      "conv3.bias, gradient norm: 0.009916141629219055\n",
      "conv3.lin.weight, gradient norm: 0.01805567927658558\n",
      "Epoch: 297, Training Loss: 0.1683, Validation Loss: 0.2571, Train Acc: 0.9901, Val Acc: 0.9898, Test Acc: 0.9908\n",
      "conv1.bias, gradient norm: 7.736932761126525e-10\n",
      "conv1.lin.weight, gradient norm: 0.10283251851797104\n",
      "bn1.module.weight, gradient norm: 0.034745313227176666\n",
      "bn1.module.bias, gradient norm: 0.02806352823972702\n",
      "conv2.bias, gradient norm: 3.558183436780382e-08\n",
      "conv2.lin.weight, gradient norm: 0.04635768383741379\n",
      "bn2.module.weight, gradient norm: 0.015728911384940147\n",
      "bn2.module.bias, gradient norm: 0.00758229149505496\n",
      "conv3.bias, gradient norm: 0.010689078830182552\n",
      "conv3.lin.weight, gradient norm: 0.017581209540367126\n",
      "Epoch: 298, Training Loss: 0.1695, Validation Loss: 0.2567, Train Acc: 0.9901, Val Acc: 0.9897, Test Acc: 0.9907\n",
      "conv1.bias, gradient norm: 1.407375993522919e-09\n",
      "conv1.lin.weight, gradient norm: 0.07873808592557907\n",
      "bn1.module.weight, gradient norm: 0.026555197313427925\n",
      "bn1.module.bias, gradient norm: 0.02191365696489811\n",
      "conv2.bias, gradient norm: 4.4309452107427205e-08\n",
      "conv2.lin.weight, gradient norm: 0.03846586495637894\n",
      "bn2.module.weight, gradient norm: 0.016602283343672752\n",
      "bn2.module.bias, gradient norm: 0.009853857569396496\n",
      "conv3.bias, gradient norm: 0.009890303947031498\n",
      "conv3.lin.weight, gradient norm: 0.01905878074467182\n",
      "Epoch: 299, Training Loss: 0.1661, Validation Loss: 0.2561, Train Acc: 0.9901, Val Acc: 0.9897, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 9.705787284985945e-10\n",
      "conv1.lin.weight, gradient norm: 0.13383829593658447\n",
      "bn1.module.weight, gradient norm: 0.051938630640506744\n",
      "bn1.module.bias, gradient norm: 0.02650953084230423\n",
      "conv2.bias, gradient norm: 5.538433711649304e-08\n",
      "conv2.lin.weight, gradient norm: 0.05616564676165581\n",
      "bn2.module.weight, gradient norm: 0.017384536564350128\n",
      "bn2.module.bias, gradient norm: 0.008724692277610302\n",
      "conv3.bias, gradient norm: 0.009978724643588066\n",
      "conv3.lin.weight, gradient norm: 0.019907819107174873\n",
      "Epoch: 300, Training Loss: 0.1696, Validation Loss: 0.2568, Train Acc: 0.9902, Val Acc: 0.9898, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 9.572171943972307e-10\n",
      "conv1.lin.weight, gradient norm: 0.09061708301305771\n",
      "bn1.module.weight, gradient norm: 0.03118446096777916\n",
      "bn1.module.bias, gradient norm: 0.019142353907227516\n",
      "conv2.bias, gradient norm: 6.886642722747638e-08\n",
      "conv2.lin.weight, gradient norm: 0.04826495051383972\n",
      "bn2.module.weight, gradient norm: 0.017150377854704857\n",
      "bn2.module.bias, gradient norm: 0.0073896110989153385\n",
      "conv3.bias, gradient norm: 0.010794014669954777\n",
      "conv3.lin.weight, gradient norm: 0.019803019240498543\n",
      "Epoch: 301, Training Loss: 0.1670, Validation Loss: 0.2574, Train Acc: 0.9901, Val Acc: 0.9897, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 9.873208917099419e-10\n",
      "conv1.lin.weight, gradient norm: 0.06734824925661087\n",
      "bn1.module.weight, gradient norm: 0.03052586317062378\n",
      "bn1.module.bias, gradient norm: 0.01820305362343788\n",
      "conv2.bias, gradient norm: 3.6388868807080144e-08\n",
      "conv2.lin.weight, gradient norm: 0.03615806996822357\n",
      "bn2.module.weight, gradient norm: 0.018381932750344276\n",
      "bn2.module.bias, gradient norm: 0.008616529405117035\n",
      "conv3.bias, gradient norm: 0.010028502903878689\n",
      "conv3.lin.weight, gradient norm: 0.020659469068050385\n",
      "Epoch: 302, Training Loss: 0.1639, Validation Loss: 0.2576, Train Acc: 0.9901, Val Acc: 0.9899, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.4746923682196211e-09\n",
      "conv1.lin.weight, gradient norm: 0.07819560170173645\n",
      "bn1.module.weight, gradient norm: 0.048490043729543686\n",
      "bn1.module.bias, gradient norm: 0.028530124574899673\n",
      "conv2.bias, gradient norm: 6.206847302792085e-08\n",
      "conv2.lin.weight, gradient norm: 0.034353677183389664\n",
      "bn2.module.weight, gradient norm: 0.017673566937446594\n",
      "bn2.module.bias, gradient norm: 0.008703671395778656\n",
      "conv3.bias, gradient norm: 0.010600171983242035\n",
      "conv3.lin.weight, gradient norm: 0.020012449473142624\n",
      "Epoch: 303, Training Loss: 0.1662, Validation Loss: 0.2584, Train Acc: 0.9901, Val Acc: 0.9898, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.5718143453469224e-09\n",
      "conv1.lin.weight, gradient norm: 0.10107595473527908\n",
      "bn1.module.weight, gradient norm: 0.06947781890630722\n",
      "bn1.module.bias, gradient norm: 0.037762660533189774\n",
      "conv2.bias, gradient norm: 4.0845854698545736e-08\n",
      "conv2.lin.weight, gradient norm: 0.04402367025613785\n",
      "bn2.module.weight, gradient norm: 0.015268835239112377\n",
      "bn2.module.bias, gradient norm: 0.008288719691336155\n",
      "conv3.bias, gradient norm: 0.010936927050352097\n",
      "conv3.lin.weight, gradient norm: 0.020334994420409203\n",
      "Epoch: 304, Training Loss: 0.1669, Validation Loss: 0.2583, Train Acc: 0.9901, Val Acc: 0.9898, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.072520738887306e-09\n",
      "conv1.lin.weight, gradient norm: 0.0876997858285904\n",
      "bn1.module.weight, gradient norm: 0.0246608667075634\n",
      "bn1.module.bias, gradient norm: 0.01757749542593956\n",
      "conv2.bias, gradient norm: 5.242672074246002e-08\n",
      "conv2.lin.weight, gradient norm: 0.03473547846078873\n",
      "bn2.module.weight, gradient norm: 0.01635320670902729\n",
      "bn2.module.bias, gradient norm: 0.008365231566131115\n",
      "conv3.bias, gradient norm: 0.010644827969372272\n",
      "conv3.lin.weight, gradient norm: 0.019299935549497604\n",
      "Epoch: 305, Training Loss: 0.1649, Validation Loss: 0.2575, Train Acc: 0.9901, Val Acc: 0.9899, Test Acc: 0.9908\n",
      "conv1.bias, gradient norm: 1.221553969088518e-09\n",
      "conv1.lin.weight, gradient norm: 0.07398590445518494\n",
      "bn1.module.weight, gradient norm: 0.04799678549170494\n",
      "bn1.module.bias, gradient norm: 0.020102668553590775\n",
      "conv2.bias, gradient norm: 4.598994607363238e-08\n",
      "conv2.lin.weight, gradient norm: 0.042806338518857956\n",
      "bn2.module.weight, gradient norm: 0.016644839197397232\n",
      "bn2.module.bias, gradient norm: 0.009970367886126041\n",
      "conv3.bias, gradient norm: 0.010307693853974342\n",
      "conv3.lin.weight, gradient norm: 0.019994063302874565\n",
      "Epoch: 306, Training Loss: 0.1642, Validation Loss: 0.2570, Train Acc: 0.9901, Val Acc: 0.9899, Test Acc: 0.9907\n",
      "conv1.bias, gradient norm: 1.5489248772482256e-09\n",
      "conv1.lin.weight, gradient norm: 0.07690620422363281\n",
      "bn1.module.weight, gradient norm: 0.059279315173625946\n",
      "bn1.module.bias, gradient norm: 0.027761563658714294\n",
      "conv2.bias, gradient norm: 5.585910045624587e-08\n",
      "conv2.lin.weight, gradient norm: 0.040619928389787674\n",
      "bn2.module.weight, gradient norm: 0.017789073288440704\n",
      "bn2.module.bias, gradient norm: 0.008491539396345615\n",
      "conv3.bias, gradient norm: 0.009970240294933319\n",
      "conv3.lin.weight, gradient norm: 0.019786102697253227\n",
      "Epoch: 307, Training Loss: 0.1660, Validation Loss: 0.2569, Train Acc: 0.9902, Val Acc: 0.9898, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.0745595524497276e-09\n",
      "conv1.lin.weight, gradient norm: 0.0979408472776413\n",
      "bn1.module.weight, gradient norm: 0.07277671247720718\n",
      "bn1.module.bias, gradient norm: 0.033541131764650345\n",
      "conv2.bias, gradient norm: 5.297255256664357e-08\n",
      "conv2.lin.weight, gradient norm: 0.05103590339422226\n",
      "bn2.module.weight, gradient norm: 0.017520207911729813\n",
      "bn2.module.bias, gradient norm: 0.007087701465934515\n",
      "conv3.bias, gradient norm: 0.010411420837044716\n",
      "conv3.lin.weight, gradient norm: 0.019278572872281075\n",
      "Epoch: 308, Training Loss: 0.1672, Validation Loss: 0.2564, Train Acc: 0.9902, Val Acc: 0.9897, Test Acc: 0.9907\n",
      "conv1.bias, gradient norm: 1.0634416680588288e-09\n",
      "conv1.lin.weight, gradient norm: 0.10988398641347885\n",
      "bn1.module.weight, gradient norm: 0.026650600135326385\n",
      "bn1.module.bias, gradient norm: 0.016613906249403954\n",
      "conv2.bias, gradient norm: 6.048848888440261e-08\n",
      "conv2.lin.weight, gradient norm: 0.03961654007434845\n",
      "bn2.module.weight, gradient norm: 0.017629647627472878\n",
      "bn2.module.bias, gradient norm: 0.008592766709625721\n",
      "conv3.bias, gradient norm: 0.010129660367965698\n",
      "conv3.lin.weight, gradient norm: 0.019241776317358017\n",
      "Epoch: 309, Training Loss: 0.1665, Validation Loss: 0.2565, Train Acc: 0.9901, Val Acc: 0.9897, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 9.11587805241254e-10\n",
      "conv1.lin.weight, gradient norm: 0.0745876133441925\n",
      "bn1.module.weight, gradient norm: 0.03162666782736778\n",
      "bn1.module.bias, gradient norm: 0.01863715425133705\n",
      "conv2.bias, gradient norm: 4.7729713514854666e-08\n",
      "conv2.lin.weight, gradient norm: 0.03011534921824932\n",
      "bn2.module.weight, gradient norm: 0.01751737855374813\n",
      "bn2.module.bias, gradient norm: 0.008060485124588013\n",
      "conv3.bias, gradient norm: 0.01056617684662342\n",
      "conv3.lin.weight, gradient norm: 0.018907057121396065\n",
      "Epoch: 310, Training Loss: 0.1650, Validation Loss: 0.2562, Train Acc: 0.9901, Val Acc: 0.9897, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.1281019451914176e-09\n",
      "conv1.lin.weight, gradient norm: 0.09712078422307968\n",
      "bn1.module.weight, gradient norm: 0.04316351190209389\n",
      "bn1.module.bias, gradient norm: 0.026899881660938263\n",
      "conv2.bias, gradient norm: 6.121243245615915e-08\n",
      "conv2.lin.weight, gradient norm: 0.033399686217308044\n",
      "bn2.module.weight, gradient norm: 0.01612800732254982\n",
      "bn2.module.bias, gradient norm: 0.00830573309212923\n",
      "conv3.bias, gradient norm: 0.010510409250855446\n",
      "conv3.lin.weight, gradient norm: 0.018816251307725906\n",
      "Epoch: 311, Training Loss: 0.1664, Validation Loss: 0.2558, Train Acc: 0.9901, Val Acc: 0.9896, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.6067187580404152e-09\n",
      "conv1.lin.weight, gradient norm: 0.12168534845113754\n",
      "bn1.module.weight, gradient norm: 0.0337052121758461\n",
      "bn1.module.bias, gradient norm: 0.022994602099061012\n",
      "conv2.bias, gradient norm: 3.650117008646703e-08\n",
      "conv2.lin.weight, gradient norm: 0.05343089625239372\n",
      "bn2.module.weight, gradient norm: 0.01656397618353367\n",
      "bn2.module.bias, gradient norm: 0.010655182413756847\n",
      "conv3.bias, gradient norm: 0.009816719219088554\n",
      "conv3.lin.weight, gradient norm: 0.019612381234765053\n",
      "Epoch: 312, Training Loss: 0.1657, Validation Loss: 0.2555, Train Acc: 0.9902, Val Acc: 0.9896, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.7702159738064438e-09\n",
      "conv1.lin.weight, gradient norm: 0.11913684755563736\n",
      "bn1.module.weight, gradient norm: 0.04900631681084633\n",
      "bn1.module.bias, gradient norm: 0.02909340336918831\n",
      "conv2.bias, gradient norm: 5.742710129652551e-08\n",
      "conv2.lin.weight, gradient norm: 0.04662323743104935\n",
      "bn2.module.weight, gradient norm: 0.01515455637127161\n",
      "bn2.module.bias, gradient norm: 0.008660468272864819\n",
      "conv3.bias, gradient norm: 0.01042949315160513\n",
      "conv3.lin.weight, gradient norm: 0.018135197460651398\n",
      "Epoch: 313, Training Loss: 0.1697, Validation Loss: 0.2557, Train Acc: 0.9902, Val Acc: 0.9897, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.458176135393785e-09\n",
      "conv1.lin.weight, gradient norm: 0.0924348309636116\n",
      "bn1.module.weight, gradient norm: 0.03643772006034851\n",
      "bn1.module.bias, gradient norm: 0.02113523706793785\n",
      "conv2.bias, gradient norm: 6.364075488818344e-08\n",
      "conv2.lin.weight, gradient norm: 0.039507023990154266\n",
      "bn2.module.weight, gradient norm: 0.017129622399806976\n",
      "bn2.module.bias, gradient norm: 0.009906669147312641\n",
      "conv3.bias, gradient norm: 0.009991911239922047\n",
      "conv3.lin.weight, gradient norm: 0.019563356414437294\n",
      "Epoch: 314, Training Loss: 0.1677, Validation Loss: 0.2568, Train Acc: 0.9903, Val Acc: 0.9898, Test Acc: 0.9907\n",
      "conv1.bias, gradient norm: 1.7687276088196313e-09\n",
      "conv1.lin.weight, gradient norm: 0.12653043866157532\n",
      "bn1.module.weight, gradient norm: 0.04716679826378822\n",
      "bn1.module.bias, gradient norm: 0.029727045446634293\n",
      "conv2.bias, gradient norm: 8.647035798503566e-08\n",
      "conv2.lin.weight, gradient norm: 0.06455526500940323\n",
      "bn2.module.weight, gradient norm: 0.017298933118581772\n",
      "bn2.module.bias, gradient norm: 0.006970852147787809\n",
      "conv3.bias, gradient norm: 0.010858339257538319\n",
      "conv3.lin.weight, gradient norm: 0.020995253697037697\n",
      "Epoch: 315, Training Loss: 0.1695, Validation Loss: 0.2591, Train Acc: 0.9903, Val Acc: 0.9897, Test Acc: 0.9908\n",
      "conv1.bias, gradient norm: 1.4198422437772251e-09\n",
      "conv1.lin.weight, gradient norm: 0.10846363008022308\n",
      "bn1.module.weight, gradient norm: 0.025076143443584442\n",
      "bn1.module.bias, gradient norm: 0.01781662553548813\n",
      "conv2.bias, gradient norm: 6.271707064797738e-08\n",
      "conv2.lin.weight, gradient norm: 0.03896181657910347\n",
      "bn2.module.weight, gradient norm: 0.01733454316854477\n",
      "bn2.module.bias, gradient norm: 0.007516204379498959\n",
      "conv3.bias, gradient norm: 0.010833317413926125\n",
      "conv3.lin.weight, gradient norm: 0.02035578154027462\n",
      "Epoch: 316, Training Loss: 0.1687, Validation Loss: 0.2619, Train Acc: 0.9903, Val Acc: 0.9899, Test Acc: 0.9908\n",
      "conv1.bias, gradient norm: 1.0400931227394494e-09\n",
      "conv1.lin.weight, gradient norm: 0.08211712539196014\n",
      "bn1.module.weight, gradient norm: 0.023093849420547485\n",
      "bn1.module.bias, gradient norm: 0.020438862964510918\n",
      "conv2.bias, gradient norm: 3.529669712065697e-08\n",
      "conv2.lin.weight, gradient norm: 0.03251393511891365\n",
      "bn2.module.weight, gradient norm: 0.016275035217404366\n",
      "bn2.module.bias, gradient norm: 0.008890879340469837\n",
      "conv3.bias, gradient norm: 0.01034429669380188\n",
      "conv3.lin.weight, gradient norm: 0.019275428727269173\n",
      "Epoch: 317, Training Loss: 0.1658, Validation Loss: 0.2634, Train Acc: 0.9904, Val Acc: 0.9900, Test Acc: 0.9908\n",
      "conv1.bias, gradient norm: 1.2905753132841369e-09\n",
      "conv1.lin.weight, gradient norm: 0.10299116373062134\n",
      "bn1.module.weight, gradient norm: 0.03469099476933479\n",
      "bn1.module.bias, gradient norm: 0.022256925702095032\n",
      "conv2.bias, gradient norm: 5.1830873815106315e-08\n",
      "conv2.lin.weight, gradient norm: 0.04531161114573479\n",
      "bn2.module.weight, gradient norm: 0.015639953315258026\n",
      "bn2.module.bias, gradient norm: 0.007436700165271759\n",
      "conv3.bias, gradient norm: 0.011260570026934147\n",
      "conv3.lin.weight, gradient norm: 0.02112739346921444\n",
      "Epoch: 318, Training Loss: 0.1659, Validation Loss: 0.2637, Train Acc: 0.9903, Val Acc: 0.9900, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.2748261335460143e-09\n",
      "conv1.lin.weight, gradient norm: 0.11218540370464325\n",
      "bn1.module.weight, gradient norm: 0.03644988313317299\n",
      "bn1.module.bias, gradient norm: 0.02262205444276333\n",
      "conv2.bias, gradient norm: 5.2829985719426986e-08\n",
      "conv2.lin.weight, gradient norm: 0.039637647569179535\n",
      "bn2.module.weight, gradient norm: 0.015668105334043503\n",
      "bn2.module.bias, gradient norm: 0.007819036953151226\n",
      "conv3.bias, gradient norm: 0.01064363494515419\n",
      "conv3.lin.weight, gradient norm: 0.01814960315823555\n",
      "Epoch: 319, Training Loss: 0.1671, Validation Loss: 0.2627, Train Acc: 0.9903, Val Acc: 0.9899, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 1.210906042103943e-09\n",
      "conv1.lin.weight, gradient norm: 0.09224841743707657\n",
      "bn1.module.weight, gradient norm: 0.03614813834428787\n",
      "bn1.module.bias, gradient norm: 0.01670822687447071\n",
      "conv2.bias, gradient norm: 4.031467781828724e-08\n",
      "conv2.lin.weight, gradient norm: 0.03479239344596863\n",
      "bn2.module.weight, gradient norm: 0.015390375629067421\n",
      "bn2.module.bias, gradient norm: 0.008564155548810959\n",
      "conv3.bias, gradient norm: 0.010517180897295475\n",
      "conv3.lin.weight, gradient norm: 0.01797223649919033\n",
      "Epoch: 320, Training Loss: 0.1698, Validation Loss: 0.2616, Train Acc: 0.9903, Val Acc: 0.9898, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 7.771919219301537e-10\n",
      "conv1.lin.weight, gradient norm: 0.11967763304710388\n",
      "bn1.module.weight, gradient norm: 0.11087334156036377\n",
      "bn1.module.bias, gradient norm: 0.049570176750421524\n",
      "conv2.bias, gradient norm: 5.931475044462786e-08\n",
      "conv2.lin.weight, gradient norm: 0.038860172033309937\n",
      "bn2.module.weight, gradient norm: 0.01744804158806801\n",
      "bn2.module.bias, gradient norm: 0.009837518446147442\n",
      "conv3.bias, gradient norm: 0.009903240948915482\n",
      "conv3.lin.weight, gradient norm: 0.020013585686683655\n",
      "Epoch: 321, Training Loss: 0.1658, Validation Loss: 0.2616, Train Acc: 0.9903, Val Acc: 0.9899, Test Acc: 0.9907\n",
      "conv1.bias, gradient norm: 1.9559889263831565e-09\n",
      "conv1.lin.weight, gradient norm: 0.16207657754421234\n",
      "bn1.module.weight, gradient norm: 0.03491886332631111\n",
      "bn1.module.bias, gradient norm: 0.0216066874563694\n",
      "conv2.bias, gradient norm: 5.790056079035821e-08\n",
      "conv2.lin.weight, gradient norm: 0.04484838247299194\n",
      "bn2.module.weight, gradient norm: 0.01788470335304737\n",
      "bn2.module.bias, gradient norm: 0.00914015807211399\n",
      "conv3.bias, gradient norm: 0.009615889750421047\n",
      "conv3.lin.weight, gradient norm: 0.02031489461660385\n",
      "Epoch: 322, Training Loss: 0.1682, Validation Loss: 0.2625, Train Acc: 0.9903, Val Acc: 0.9899, Test Acc: 0.9907\n",
      "conv1.bias, gradient norm: 1.3546624932914142e-09\n",
      "conv1.lin.weight, gradient norm: 0.08665253967046738\n",
      "bn1.module.weight, gradient norm: 0.02743339352309704\n",
      "bn1.module.bias, gradient norm: 0.018301211297512054\n",
      "conv2.bias, gradient norm: 6.028234622590389e-08\n",
      "conv2.lin.weight, gradient norm: 0.03327848017215729\n",
      "bn2.module.weight, gradient norm: 0.01603335700929165\n",
      "bn2.module.bias, gradient norm: 0.008709828369319439\n",
      "conv3.bias, gradient norm: 0.010568033903837204\n",
      "conv3.lin.weight, gradient norm: 0.019435541704297066\n",
      "Epoch: 323, Training Loss: 0.1671, Validation Loss: 0.2638, Train Acc: 0.9903, Val Acc: 0.9899, Test Acc: 0.9907\n",
      "conv1.bias, gradient norm: 1.8509161980873046e-09\n",
      "conv1.lin.weight, gradient norm: 0.08965808898210526\n",
      "bn1.module.weight, gradient norm: 0.046643130481243134\n",
      "bn1.module.bias, gradient norm: 0.02653433382511139\n",
      "conv2.bias, gradient norm: 5.923149259956517e-08\n",
      "conv2.lin.weight, gradient norm: 0.03507108986377716\n",
      "bn2.module.weight, gradient norm: 0.016584720462560654\n",
      "bn2.module.bias, gradient norm: 0.008934387005865574\n",
      "conv3.bias, gradient norm: 0.010138320736587048\n",
      "conv3.lin.weight, gradient norm: 0.01861482858657837\n",
      "Epoch: 324, Training Loss: 0.1658, Validation Loss: 0.2647, Train Acc: 0.9903, Val Acc: 0.9898, Test Acc: 0.9907\n",
      "conv1.bias, gradient norm: 1.1296026336538034e-09\n",
      "conv1.lin.weight, gradient norm: 0.14558161795139313\n",
      "bn1.module.weight, gradient norm: 0.05721719563007355\n",
      "bn1.module.bias, gradient norm: 0.03410407155752182\n",
      "conv2.bias, gradient norm: 3.128021575093953e-08\n",
      "conv2.lin.weight, gradient norm: 0.03910896182060242\n",
      "bn2.module.weight, gradient norm: 0.0158408023416996\n",
      "bn2.module.bias, gradient norm: 0.009167429059743881\n",
      "conv3.bias, gradient norm: 0.009820536710321903\n",
      "conv3.lin.weight, gradient norm: 0.018085144460201263\n",
      "Epoch: 325, Training Loss: 0.1679, Validation Loss: 0.2637, Train Acc: 0.9903, Val Acc: 0.9898, Test Acc: 0.9907\n",
      "conv1.bias, gradient norm: 1.6773143984849526e-09\n",
      "conv1.lin.weight, gradient norm: 0.1489446759223938\n",
      "bn1.module.weight, gradient norm: 0.03841917961835861\n",
      "bn1.module.bias, gradient norm: 0.030700834468007088\n",
      "conv2.bias, gradient norm: 4.631223404771845e-08\n",
      "conv2.lin.weight, gradient norm: 0.03134411945939064\n",
      "bn2.module.weight, gradient norm: 0.015602033585309982\n",
      "bn2.module.bias, gradient norm: 0.007961791940033436\n",
      "conv3.bias, gradient norm: 0.010636469349265099\n",
      "conv3.lin.weight, gradient norm: 0.01872418448328972\n",
      "Epoch: 326, Training Loss: 0.1676, Validation Loss: 0.2615, Train Acc: 0.9902, Val Acc: 0.9898, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.7444479194494988e-09\n",
      "conv1.lin.weight, gradient norm: 0.10477156937122345\n",
      "bn1.module.weight, gradient norm: 0.021744754165410995\n",
      "bn1.module.bias, gradient norm: 0.014485446736216545\n",
      "conv2.bias, gradient norm: 6.495974957942963e-08\n",
      "conv2.lin.weight, gradient norm: 0.036122310906648636\n",
      "bn2.module.weight, gradient norm: 0.017376506701111794\n",
      "bn2.module.bias, gradient norm: 0.0096790362149477\n",
      "conv3.bias, gradient norm: 0.010063428431749344\n",
      "conv3.lin.weight, gradient norm: 0.019365079700946808\n",
      "Epoch: 327, Training Loss: 0.1656, Validation Loss: 0.2605, Train Acc: 0.9902, Val Acc: 0.9897, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.678438388275083e-09\n",
      "conv1.lin.weight, gradient norm: 0.18834613263607025\n",
      "bn1.module.weight, gradient norm: 0.026036981493234634\n",
      "bn1.module.bias, gradient norm: 0.02874329686164856\n",
      "conv2.bias, gradient norm: 7.639224719468984e-08\n",
      "conv2.lin.weight, gradient norm: 0.060001619160175323\n",
      "bn2.module.weight, gradient norm: 0.018911108374595642\n",
      "bn2.module.bias, gradient norm: 0.00703853415325284\n",
      "conv3.bias, gradient norm: 0.010572677478194237\n",
      "conv3.lin.weight, gradient norm: 0.019230790436267853\n",
      "Epoch: 328, Training Loss: 0.1670, Validation Loss: 0.2608, Train Acc: 0.9900, Val Acc: 0.9896, Test Acc: 0.9904\n",
      "conv1.bias, gradient norm: 1.5393725183443507e-09\n",
      "conv1.lin.weight, gradient norm: 0.1182367131114006\n",
      "bn1.module.weight, gradient norm: 0.04661620408296585\n",
      "bn1.module.bias, gradient norm: 0.02734501101076603\n",
      "conv2.bias, gradient norm: 5.328414687255645e-08\n",
      "conv2.lin.weight, gradient norm: 0.04042942076921463\n",
      "bn2.module.weight, gradient norm: 0.01736818440258503\n",
      "bn2.module.bias, gradient norm: 0.010357847437262535\n",
      "conv3.bias, gradient norm: 0.009527488611638546\n",
      "conv3.lin.weight, gradient norm: 0.020961135625839233\n",
      "Epoch: 329, Training Loss: 0.1648, Validation Loss: 0.2623, Train Acc: 0.9901, Val Acc: 0.9896, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 1.357035706028853e-09\n",
      "conv1.lin.weight, gradient norm: 0.07344548404216766\n",
      "bn1.module.weight, gradient norm: 0.031105415895581245\n",
      "bn1.module.bias, gradient norm: 0.02128690481185913\n",
      "conv2.bias, gradient norm: 5.814452919139512e-08\n",
      "conv2.lin.weight, gradient norm: 0.03987396880984306\n",
      "bn2.module.weight, gradient norm: 0.016971943899989128\n",
      "bn2.module.bias, gradient norm: 0.00856983382254839\n",
      "conv3.bias, gradient norm: 0.010656574741005898\n",
      "conv3.lin.weight, gradient norm: 0.018629804253578186\n",
      "Epoch: 330, Training Loss: 0.1650, Validation Loss: 0.2646, Train Acc: 0.9902, Val Acc: 0.9897, Test Acc: 0.9904\n",
      "conv1.bias, gradient norm: 1.1229103202836654e-09\n",
      "conv1.lin.weight, gradient norm: 0.11735345423221588\n",
      "bn1.module.weight, gradient norm: 0.03318050503730774\n",
      "bn1.module.bias, gradient norm: 0.020794371142983437\n",
      "conv2.bias, gradient norm: 4.8721723544531415e-08\n",
      "conv2.lin.weight, gradient norm: 0.04081638902425766\n",
      "bn2.module.weight, gradient norm: 0.014911712147295475\n",
      "bn2.module.bias, gradient norm: 0.007649551145732403\n",
      "conv3.bias, gradient norm: 0.011244870722293854\n",
      "conv3.lin.weight, gradient norm: 0.01942276395857334\n",
      "Epoch: 331, Training Loss: 0.1697, Validation Loss: 0.2657, Train Acc: 0.9901, Val Acc: 0.9897, Test Acc: 0.9904\n",
      "conv1.bias, gradient norm: 1.1904532914996935e-09\n",
      "conv1.lin.weight, gradient norm: 0.08837198466062546\n",
      "bn1.module.weight, gradient norm: 0.028373248875141144\n",
      "bn1.module.bias, gradient norm: 0.01435239240527153\n",
      "conv2.bias, gradient norm: 4.842317835596077e-08\n",
      "conv2.lin.weight, gradient norm: 0.040236447006464005\n",
      "bn2.module.weight, gradient norm: 0.01612161286175251\n",
      "bn2.module.bias, gradient norm: 0.008950642310082912\n",
      "conv3.bias, gradient norm: 0.010214600712060928\n",
      "conv3.lin.weight, gradient norm: 0.019276496022939682\n",
      "Epoch: 332, Training Loss: 0.1654, Validation Loss: 0.2669, Train Acc: 0.9901, Val Acc: 0.9897, Test Acc: 0.9904\n",
      "conv1.bias, gradient norm: 2.4826727340609978e-09\n",
      "conv1.lin.weight, gradient norm: 0.08773628622293472\n",
      "bn1.module.weight, gradient norm: 0.029044702649116516\n",
      "bn1.module.bias, gradient norm: 0.01737014576792717\n",
      "conv2.bias, gradient norm: 6.804276608818327e-08\n",
      "conv2.lin.weight, gradient norm: 0.03750644624233246\n",
      "bn2.module.weight, gradient norm: 0.015980368480086327\n",
      "bn2.module.bias, gradient norm: 0.007777764927595854\n",
      "conv3.bias, gradient norm: 0.010483545251190662\n",
      "conv3.lin.weight, gradient norm: 0.017711246386170387\n",
      "Epoch: 333, Training Loss: 0.1678, Validation Loss: 0.2672, Train Acc: 0.9901, Val Acc: 0.9897, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 1.1521543719084093e-09\n",
      "conv1.lin.weight, gradient norm: 0.07424861192703247\n",
      "bn1.module.weight, gradient norm: 0.018165597692131996\n",
      "bn1.module.bias, gradient norm: 0.013169186189770699\n",
      "conv2.bias, gradient norm: 6.345948833086368e-08\n",
      "conv2.lin.weight, gradient norm: 0.04238882660865784\n",
      "bn2.module.weight, gradient norm: 0.01676815375685692\n",
      "bn2.module.bias, gradient norm: 0.010049005039036274\n",
      "conv3.bias, gradient norm: 0.009865175932645798\n",
      "conv3.lin.weight, gradient norm: 0.02032758854329586\n",
      "Epoch: 334, Training Loss: 0.1651, Validation Loss: 0.2678, Train Acc: 0.9901, Val Acc: 0.9898, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.4917888035981264e-09\n",
      "conv1.lin.weight, gradient norm: 0.09241946786642075\n",
      "bn1.module.weight, gradient norm: 0.029322538524866104\n",
      "bn1.module.bias, gradient norm: 0.022104477509856224\n",
      "conv2.bias, gradient norm: 7.26737496847818e-08\n",
      "conv2.lin.weight, gradient norm: 0.04454692080616951\n",
      "bn2.module.weight, gradient norm: 0.017039695754647255\n",
      "bn2.module.bias, gradient norm: 0.008651472628116608\n",
      "conv3.bias, gradient norm: 0.010176999494433403\n",
      "conv3.lin.weight, gradient norm: 0.01933060958981514\n",
      "Epoch: 335, Training Loss: 0.1664, Validation Loss: 0.2688, Train Acc: 0.9901, Val Acc: 0.9897, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 1.2893145440173726e-09\n",
      "conv1.lin.weight, gradient norm: 0.11698097735643387\n",
      "bn1.module.weight, gradient norm: 0.031664133071899414\n",
      "bn1.module.bias, gradient norm: 0.01939166709780693\n",
      "conv2.bias, gradient norm: 6.635315941139197e-08\n",
      "conv2.lin.weight, gradient norm: 0.05168379470705986\n",
      "bn2.module.weight, gradient norm: 0.017504729330539703\n",
      "bn2.module.bias, gradient norm: 0.00962271448224783\n",
      "conv3.bias, gradient norm: 0.00960151944309473\n",
      "conv3.lin.weight, gradient norm: 0.020345697179436684\n",
      "Epoch: 336, Training Loss: 0.1659, Validation Loss: 0.2705, Train Acc: 0.9901, Val Acc: 0.9898, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.223904200209347e-09\n",
      "conv1.lin.weight, gradient norm: 0.08987988531589508\n",
      "bn1.module.weight, gradient norm: 0.020253164693713188\n",
      "bn1.module.bias, gradient norm: 0.020227842032909393\n",
      "conv2.bias, gradient norm: 5.6454229735436456e-08\n",
      "conv2.lin.weight, gradient norm: 0.03198830783367157\n",
      "bn2.module.weight, gradient norm: 0.015757735818624496\n",
      "bn2.module.bias, gradient norm: 0.008237870410084724\n",
      "conv3.bias, gradient norm: 0.010659964755177498\n",
      "conv3.lin.weight, gradient norm: 0.01857849583029747\n",
      "Epoch: 337, Training Loss: 0.1671, Validation Loss: 0.2719, Train Acc: 0.9901, Val Acc: 0.9897, Test Acc: 0.9904\n",
      "conv1.bias, gradient norm: 1.4085181909706534e-09\n",
      "conv1.lin.weight, gradient norm: 0.0940171480178833\n",
      "bn1.module.weight, gradient norm: 0.04246062412858009\n",
      "bn1.module.bias, gradient norm: 0.017140723764896393\n",
      "conv2.bias, gradient norm: 5.4925457959598134e-08\n",
      "conv2.lin.weight, gradient norm: 0.03187600523233414\n",
      "bn2.module.weight, gradient norm: 0.01618250273168087\n",
      "bn2.module.bias, gradient norm: 0.009047318249940872\n",
      "conv3.bias, gradient norm: 0.010371753945946693\n",
      "conv3.lin.weight, gradient norm: 0.018225014209747314\n",
      "Epoch: 338, Training Loss: 0.1640, Validation Loss: 0.2726, Train Acc: 0.9902, Val Acc: 0.9898, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 1.4337606657477409e-09\n",
      "conv1.lin.weight, gradient norm: 0.098396435379982\n",
      "bn1.module.weight, gradient norm: 0.03509041294455528\n",
      "bn1.module.bias, gradient norm: 0.0229373537003994\n",
      "conv2.bias, gradient norm: 6.445383604614108e-08\n",
      "conv2.lin.weight, gradient norm: 0.041063517332077026\n",
      "bn2.module.weight, gradient norm: 0.015414276160299778\n",
      "bn2.module.bias, gradient norm: 0.007707103621214628\n",
      "conv3.bias, gradient norm: 0.010636983439326286\n",
      "conv3.lin.weight, gradient norm: 0.017482493072748184\n",
      "Epoch: 339, Training Loss: 0.1679, Validation Loss: 0.2718, Train Acc: 0.9902, Val Acc: 0.9898, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.8997705630852124e-09\n",
      "conv1.lin.weight, gradient norm: 0.10786297917366028\n",
      "bn1.module.weight, gradient norm: 0.07337796688079834\n",
      "bn1.module.bias, gradient norm: 0.03944963961839676\n",
      "conv2.bias, gradient norm: 6.46478284238583e-08\n",
      "conv2.lin.weight, gradient norm: 0.05147960036993027\n",
      "bn2.module.weight, gradient norm: 0.015445390716195107\n",
      "bn2.module.bias, gradient norm: 0.006348174996674061\n",
      "conv3.bias, gradient norm: 0.01123138889670372\n",
      "conv3.lin.weight, gradient norm: 0.01944422535598278\n",
      "Epoch: 340, Training Loss: 0.1670, Validation Loss: 0.2701, Train Acc: 0.9902, Val Acc: 0.9897, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 2.080392746961479e-09\n",
      "conv1.lin.weight, gradient norm: 0.14388392865657806\n",
      "bn1.module.weight, gradient norm: 0.05620602145791054\n",
      "bn1.module.bias, gradient norm: 0.03678392991423607\n",
      "conv2.bias, gradient norm: 4.721125890227995e-08\n",
      "conv2.lin.weight, gradient norm: 0.04762977734208107\n",
      "bn2.module.weight, gradient norm: 0.01647239550948143\n",
      "bn2.module.bias, gradient norm: 0.009941548109054565\n",
      "conv3.bias, gradient norm: 0.009450065903365612\n",
      "conv3.lin.weight, gradient norm: 0.019304374232888222\n",
      "Epoch: 341, Training Loss: 0.1672, Validation Loss: 0.2691, Train Acc: 0.9903, Val Acc: 0.9899, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.666763282948125e-09\n",
      "conv1.lin.weight, gradient norm: 0.10144404321908951\n",
      "bn1.module.weight, gradient norm: 0.025096191093325615\n",
      "bn1.module.bias, gradient norm: 0.01787441596388817\n",
      "conv2.bias, gradient norm: 4.727250413338879e-08\n",
      "conv2.lin.weight, gradient norm: 0.04313063994050026\n",
      "bn2.module.weight, gradient norm: 0.01683632843196392\n",
      "bn2.module.bias, gradient norm: 0.009151102975010872\n",
      "conv3.bias, gradient norm: 0.01030417624861002\n",
      "conv3.lin.weight, gradient norm: 0.01926865428686142\n",
      "Epoch: 342, Training Loss: 0.1662, Validation Loss: 0.2686, Train Acc: 0.9905, Val Acc: 0.9901, Test Acc: 0.9908\n",
      "conv1.bias, gradient norm: 1.35051358984839e-09\n",
      "conv1.lin.weight, gradient norm: 0.0931212455034256\n",
      "bn1.module.weight, gradient norm: 0.03245978057384491\n",
      "bn1.module.bias, gradient norm: 0.023070141673088074\n",
      "conv2.bias, gradient norm: 7.540334934219572e-08\n",
      "conv2.lin.weight, gradient norm: 0.04520110413432121\n",
      "bn2.module.weight, gradient norm: 0.015705984085798264\n",
      "bn2.module.bias, gradient norm: 0.007948390208184719\n",
      "conv3.bias, gradient norm: 0.011481544002890587\n",
      "conv3.lin.weight, gradient norm: 0.022680720314383507\n",
      "Epoch: 343, Training Loss: 0.1640, Validation Loss: 0.2684, Train Acc: 0.9904, Val Acc: 0.9900, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.4508969581328302e-09\n",
      "conv1.lin.weight, gradient norm: 0.06693162024021149\n",
      "bn1.module.weight, gradient norm: 0.03344019502401352\n",
      "bn1.module.bias, gradient norm: 0.016514521092176437\n",
      "conv2.bias, gradient norm: 5.428600147183715e-08\n",
      "conv2.lin.weight, gradient norm: 0.04657425358891487\n",
      "bn2.module.weight, gradient norm: 0.017684781923890114\n",
      "bn2.module.bias, gradient norm: 0.008321620523929596\n",
      "conv3.bias, gradient norm: 0.010637493804097176\n",
      "conv3.lin.weight, gradient norm: 0.01929749734699726\n",
      "Epoch: 344, Training Loss: 0.1642, Validation Loss: 0.2691, Train Acc: 0.9903, Val Acc: 0.9900, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 1.4620556987310351e-09\n",
      "conv1.lin.weight, gradient norm: 0.09331044554710388\n",
      "bn1.module.weight, gradient norm: 0.07391130924224854\n",
      "bn1.module.bias, gradient norm: 0.03269059956073761\n",
      "conv2.bias, gradient norm: 7.046849503922203e-08\n",
      "conv2.lin.weight, gradient norm: 0.043374136090278625\n",
      "bn2.module.weight, gradient norm: 0.015906646847724915\n",
      "bn2.module.bias, gradient norm: 0.007852963171899319\n",
      "conv3.bias, gradient norm: 0.0109908701851964\n",
      "conv3.lin.weight, gradient norm: 0.018896471709012985\n",
      "Epoch: 345, Training Loss: 0.1656, Validation Loss: 0.2685, Train Acc: 0.9902, Val Acc: 0.9898, Test Acc: 0.9904\n",
      "conv1.bias, gradient norm: 2.9267737122751214e-09\n",
      "conv1.lin.weight, gradient norm: 0.12762947380542755\n",
      "bn1.module.weight, gradient norm: 0.0654330626130104\n",
      "bn1.module.bias, gradient norm: 0.0363556444644928\n",
      "conv2.bias, gradient norm: 5.6584195107234336e-08\n",
      "conv2.lin.weight, gradient norm: 0.05021810159087181\n",
      "bn2.module.weight, gradient norm: 0.01691589318215847\n",
      "bn2.module.bias, gradient norm: 0.011140720918774605\n",
      "conv3.bias, gradient norm: 0.00870747771114111\n",
      "conv3.lin.weight, gradient norm: 0.025704460218548775\n",
      "Epoch: 346, Training Loss: 0.1665, Validation Loss: 0.2685, Train Acc: 0.9903, Val Acc: 0.9899, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 2.5500026534786002e-09\n",
      "conv1.lin.weight, gradient norm: 0.12090407311916351\n",
      "bn1.module.weight, gradient norm: 0.028720874339342117\n",
      "bn1.module.bias, gradient norm: 0.02625352330505848\n",
      "conv2.bias, gradient norm: 5.850622741832012e-08\n",
      "conv2.lin.weight, gradient norm: 0.03873227909207344\n",
      "bn2.module.weight, gradient norm: 0.016368448734283447\n",
      "bn2.module.bias, gradient norm: 0.008974834345281124\n",
      "conv3.bias, gradient norm: 0.010033253580331802\n",
      "conv3.lin.weight, gradient norm: 0.018546653911471367\n",
      "Epoch: 347, Training Loss: 0.1676, Validation Loss: 0.2688, Train Acc: 0.9903, Val Acc: 0.9899, Test Acc: 0.9907\n",
      "conv1.bias, gradient norm: 1.171734154148396e-09\n",
      "conv1.lin.weight, gradient norm: 0.1300814151763916\n",
      "bn1.module.weight, gradient norm: 0.0495285801589489\n",
      "bn1.module.bias, gradient norm: 0.03883214294910431\n",
      "conv2.bias, gradient norm: 6.178627387498636e-08\n",
      "conv2.lin.weight, gradient norm: 0.05006391182541847\n",
      "bn2.module.weight, gradient norm: 0.01615176536142826\n",
      "bn2.module.bias, gradient norm: 0.007236181292682886\n",
      "conv3.bias, gradient norm: 0.010396248660981655\n",
      "conv3.lin.weight, gradient norm: 0.01931730844080448\n",
      "Epoch: 348, Training Loss: 0.1676, Validation Loss: 0.2688, Train Acc: 0.9901, Val Acc: 0.9897, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 1.7062082857677296e-09\n",
      "conv1.lin.weight, gradient norm: 0.10590532422065735\n",
      "bn1.module.weight, gradient norm: 0.04146973416209221\n",
      "bn1.module.bias, gradient norm: 0.02791159227490425\n",
      "conv2.bias, gradient norm: 4.484589410935769e-08\n",
      "conv2.lin.weight, gradient norm: 0.03744800388813019\n",
      "bn2.module.weight, gradient norm: 0.016655590385198593\n",
      "bn2.module.bias, gradient norm: 0.009460339322686195\n",
      "conv3.bias, gradient norm: 0.009893398731946945\n",
      "conv3.lin.weight, gradient norm: 0.019461890682578087\n",
      "Epoch: 349, Training Loss: 0.1668, Validation Loss: 0.2685, Train Acc: 0.9899, Val Acc: 0.9895, Test Acc: 0.9902\n",
      "conv1.bias, gradient norm: 1.2926106851551822e-09\n",
      "conv1.lin.weight, gradient norm: 0.1686527281999588\n",
      "bn1.module.weight, gradient norm: 0.04377462714910507\n",
      "bn1.module.bias, gradient norm: 0.03446671739220619\n",
      "conv2.bias, gradient norm: 3.1170625192089574e-08\n",
      "conv2.lin.weight, gradient norm: 0.06184329465031624\n",
      "bn2.module.weight, gradient norm: 0.018005652353167534\n",
      "bn2.module.bias, gradient norm: 0.012793299742043018\n",
      "conv3.bias, gradient norm: 0.008695059455931187\n",
      "conv3.lin.weight, gradient norm: 0.02558761276304722\n",
      "Epoch: 350, Training Loss: 0.1668, Validation Loss: 0.2698, Train Acc: 0.9901, Val Acc: 0.9895, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 2.4405377718750287e-09\n",
      "conv1.lin.weight, gradient norm: 0.10940798372030258\n",
      "bn1.module.weight, gradient norm: 0.029784221202135086\n",
      "bn1.module.bias, gradient norm: 0.020677246153354645\n",
      "conv2.bias, gradient norm: 6.762207505062179e-08\n",
      "conv2.lin.weight, gradient norm: 0.041306495666503906\n",
      "bn2.module.weight, gradient norm: 0.01699288748204708\n",
      "bn2.module.bias, gradient norm: 0.010023316368460655\n",
      "conv3.bias, gradient norm: 0.009373433887958527\n",
      "conv3.lin.weight, gradient norm: 0.021615589037537575\n",
      "Epoch: 351, Training Loss: 0.1676, Validation Loss: 0.2708, Train Acc: 0.9902, Val Acc: 0.9897, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.9129853257027207e-09\n",
      "conv1.lin.weight, gradient norm: 0.16569243371486664\n",
      "bn1.module.weight, gradient norm: 0.032745610922575\n",
      "bn1.module.bias, gradient norm: 0.032875582575798035\n",
      "conv2.bias, gradient norm: 6.667021068551549e-08\n",
      "conv2.lin.weight, gradient norm: 0.03838905692100525\n",
      "bn2.module.weight, gradient norm: 0.017225582152605057\n",
      "bn2.module.bias, gradient norm: 0.007329210638999939\n",
      "conv3.bias, gradient norm: 0.010837321169674397\n",
      "conv3.lin.weight, gradient norm: 0.018436811864376068\n",
      "Epoch: 352, Training Loss: 0.1670, Validation Loss: 0.2713, Train Acc: 0.9903, Val Acc: 0.9898, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.094313750726883e-09\n",
      "conv1.lin.weight, gradient norm: 0.08657271414995193\n",
      "bn1.module.weight, gradient norm: 0.042625583708286285\n",
      "bn1.module.bias, gradient norm: 0.02329137735068798\n",
      "conv2.bias, gradient norm: 7.101458976421782e-08\n",
      "conv2.lin.weight, gradient norm: 0.04113028198480606\n",
      "bn2.module.weight, gradient norm: 0.01682329922914505\n",
      "bn2.module.bias, gradient norm: 0.007965822704136372\n",
      "conv3.bias, gradient norm: 0.01078353263437748\n",
      "conv3.lin.weight, gradient norm: 0.0183516014367342\n",
      "Epoch: 353, Training Loss: 0.1660, Validation Loss: 0.2718, Train Acc: 0.9903, Val Acc: 0.9898, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 2.1250685655616053e-09\n",
      "conv1.lin.weight, gradient norm: 0.08870470523834229\n",
      "bn1.module.weight, gradient norm: 0.03202313929796219\n",
      "bn1.module.bias, gradient norm: 0.021422352641820908\n",
      "conv2.bias, gradient norm: 6.111449835088933e-08\n",
      "conv2.lin.weight, gradient norm: 0.041113704442977905\n",
      "bn2.module.weight, gradient norm: 0.015931617468595505\n",
      "bn2.module.bias, gradient norm: 0.00782142672687769\n",
      "conv3.bias, gradient norm: 0.010571361519396305\n",
      "conv3.lin.weight, gradient norm: 0.01760532148182392\n",
      "Epoch: 354, Training Loss: 0.1697, Validation Loss: 0.2716, Train Acc: 0.9903, Val Acc: 0.9898, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 1.4431879025167405e-09\n",
      "conv1.lin.weight, gradient norm: 0.07300379872322083\n",
      "bn1.module.weight, gradient norm: 0.04629536345601082\n",
      "bn1.module.bias, gradient norm: 0.020380031317472458\n",
      "conv2.bias, gradient norm: 6.562024879031014e-08\n",
      "conv2.lin.weight, gradient norm: 0.0328575000166893\n",
      "bn2.module.weight, gradient norm: 0.01644274964928627\n",
      "bn2.module.bias, gradient norm: 0.009318036958575249\n",
      "conv3.bias, gradient norm: 0.009928743354976177\n",
      "conv3.lin.weight, gradient norm: 0.018420176580548286\n",
      "Epoch: 355, Training Loss: 0.1651, Validation Loss: 0.2714, Train Acc: 0.9903, Val Acc: 0.9898, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 9.464410366533116e-10\n",
      "conv1.lin.weight, gradient norm: 0.08681871742010117\n",
      "bn1.module.weight, gradient norm: 0.02064734511077404\n",
      "bn1.module.bias, gradient norm: 0.015804458409547806\n",
      "conv2.bias, gradient norm: 5.2826909069381145e-08\n",
      "conv2.lin.weight, gradient norm: 0.030293375253677368\n",
      "bn2.module.weight, gradient norm: 0.016988283023238182\n",
      "bn2.module.bias, gradient norm: 0.008790633641183376\n",
      "conv3.bias, gradient norm: 0.009793979115784168\n",
      "conv3.lin.weight, gradient norm: 0.01929337903857231\n",
      "Epoch: 356, Training Loss: 0.1655, Validation Loss: 0.2714, Train Acc: 0.9903, Val Acc: 0.9899, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 1.6782916167912276e-09\n",
      "conv1.lin.weight, gradient norm: 0.11421816796064377\n",
      "bn1.module.weight, gradient norm: 0.04053402692079544\n",
      "bn1.module.bias, gradient norm: 0.0305885449051857\n",
      "conv2.bias, gradient norm: 5.6197620779130375e-08\n",
      "conv2.lin.weight, gradient norm: 0.03592851012945175\n",
      "bn2.module.weight, gradient norm: 0.01648857817053795\n",
      "bn2.module.bias, gradient norm: 0.00766746373847127\n",
      "conv3.bias, gradient norm: 0.01065028179436922\n",
      "conv3.lin.weight, gradient norm: 0.018695799633860588\n",
      "Epoch: 357, Training Loss: 0.1658, Validation Loss: 0.2713, Train Acc: 0.9903, Val Acc: 0.9899, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 1.1264230659335794e-09\n",
      "conv1.lin.weight, gradient norm: 0.08943010866641998\n",
      "bn1.module.weight, gradient norm: 0.02828708104789257\n",
      "bn1.module.bias, gradient norm: 0.0183495432138443\n",
      "conv2.bias, gradient norm: 7.513418864846244e-08\n",
      "conv2.lin.weight, gradient norm: 0.0376821905374527\n",
      "bn2.module.weight, gradient norm: 0.016731679439544678\n",
      "bn2.module.bias, gradient norm: 0.0075743477791547775\n",
      "conv3.bias, gradient norm: 0.010263804346323013\n",
      "conv3.lin.weight, gradient norm: 0.01809089072048664\n",
      "Epoch: 358, Training Loss: 0.1659, Validation Loss: 0.2714, Train Acc: 0.9903, Val Acc: 0.9899, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.4428245265207806e-09\n",
      "conv1.lin.weight, gradient norm: 0.11205722391605377\n",
      "bn1.module.weight, gradient norm: 0.041719768196344376\n",
      "bn1.module.bias, gradient norm: 0.02952032908797264\n",
      "conv2.bias, gradient norm: 5.072532971439614e-08\n",
      "conv2.lin.weight, gradient norm: 0.03632516786456108\n",
      "bn2.module.weight, gradient norm: 0.01646169275045395\n",
      "bn2.module.bias, gradient norm: 0.008609971962869167\n",
      "conv3.bias, gradient norm: 0.010431516915559769\n",
      "conv3.lin.weight, gradient norm: 0.018839260563254356\n",
      "Epoch: 359, Training Loss: 0.1646, Validation Loss: 0.2707, Train Acc: 0.9904, Val Acc: 0.9898, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.349205858147684e-09\n",
      "conv1.lin.weight, gradient norm: 0.09675070643424988\n",
      "bn1.module.weight, gradient norm: 0.026258986443281174\n",
      "bn1.module.bias, gradient norm: 0.019746895879507065\n",
      "conv2.bias, gradient norm: 4.780419260441704e-08\n",
      "conv2.lin.weight, gradient norm: 0.03613511472940445\n",
      "bn2.module.weight, gradient norm: 0.016607828438282013\n",
      "bn2.module.bias, gradient norm: 0.010171604342758656\n",
      "conv3.bias, gradient norm: 0.009318849071860313\n",
      "conv3.lin.weight, gradient norm: 0.019233521074056625\n",
      "Epoch: 360, Training Loss: 0.1648, Validation Loss: 0.2701, Train Acc: 0.9904, Val Acc: 0.9900, Test Acc: 0.9908\n",
      "conv1.bias, gradient norm: 8.111632476826003e-10\n",
      "conv1.lin.weight, gradient norm: 0.08619677275419235\n",
      "bn1.module.weight, gradient norm: 0.03643576428294182\n",
      "bn1.module.bias, gradient norm: 0.024691766127943993\n",
      "conv2.bias, gradient norm: 5.520050549989719e-08\n",
      "conv2.lin.weight, gradient norm: 0.03598141670227051\n",
      "bn2.module.weight, gradient norm: 0.016375325620174408\n",
      "bn2.module.bias, gradient norm: 0.008511164225637913\n",
      "conv3.bias, gradient norm: 0.010463636368513107\n",
      "conv3.lin.weight, gradient norm: 0.019121505320072174\n",
      "Epoch: 361, Training Loss: 0.1654, Validation Loss: 0.2696, Train Acc: 0.9904, Val Acc: 0.9900, Test Acc: 0.9908\n",
      "conv1.bias, gradient norm: 1.8498335085936901e-09\n",
      "conv1.lin.weight, gradient norm: 0.11798664182424545\n",
      "bn1.module.weight, gradient norm: 0.0457192063331604\n",
      "bn1.module.bias, gradient norm: 0.029820619150996208\n",
      "conv2.bias, gradient norm: 6.827180243362818e-08\n",
      "conv2.lin.weight, gradient norm: 0.02985508181154728\n",
      "bn2.module.weight, gradient norm: 0.016958244144916534\n",
      "bn2.module.bias, gradient norm: 0.008148157969117165\n",
      "conv3.bias, gradient norm: 0.010508952662348747\n",
      "conv3.lin.weight, gradient norm: 0.01963212713599205\n",
      "Epoch: 362, Training Loss: 0.1653, Validation Loss: 0.2692, Train Acc: 0.9903, Val Acc: 0.9899, Test Acc: 0.9907\n",
      "conv1.bias, gradient norm: 1.9940504802917758e-09\n",
      "conv1.lin.weight, gradient norm: 0.09800100326538086\n",
      "bn1.module.weight, gradient norm: 0.03053472191095352\n",
      "bn1.module.bias, gradient norm: 0.01989799737930298\n",
      "conv2.bias, gradient norm: 6.44684448047883e-08\n",
      "conv2.lin.weight, gradient norm: 0.03933041915297508\n",
      "bn2.module.weight, gradient norm: 0.0167162474244833\n",
      "bn2.module.bias, gradient norm: 0.010221128351986408\n",
      "conv3.bias, gradient norm: 0.009778057225048542\n",
      "conv3.lin.weight, gradient norm: 0.019103974103927612\n",
      "Epoch: 363, Training Loss: 0.1654, Validation Loss: 0.2690, Train Acc: 0.9903, Val Acc: 0.9898, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.461089027543494e-09\n",
      "conv1.lin.weight, gradient norm: 0.08474481850862503\n",
      "bn1.module.weight, gradient norm: 0.025927875190973282\n",
      "bn1.module.bias, gradient norm: 0.01785890758037567\n",
      "conv2.bias, gradient norm: 6.091458715218323e-08\n",
      "conv2.lin.weight, gradient norm: 0.04119478166103363\n",
      "bn2.module.weight, gradient norm: 0.01620822586119175\n",
      "bn2.module.bias, gradient norm: 0.009894595481455326\n",
      "conv3.bias, gradient norm: 0.010183892212808132\n",
      "conv3.lin.weight, gradient norm: 0.018725909292697906\n",
      "Epoch: 364, Training Loss: 0.1652, Validation Loss: 0.2694, Train Acc: 0.9904, Val Acc: 0.9900, Test Acc: 0.9908\n",
      "conv1.bias, gradient norm: 1.4100159928531752e-09\n",
      "conv1.lin.weight, gradient norm: 0.09030008316040039\n",
      "bn1.module.weight, gradient norm: 0.037797801196575165\n",
      "bn1.module.bias, gradient norm: 0.023141643032431602\n",
      "conv2.bias, gradient norm: 4.9451305272896207e-08\n",
      "conv2.lin.weight, gradient norm: 0.03306999057531357\n",
      "bn2.module.weight, gradient norm: 0.01595699042081833\n",
      "bn2.module.bias, gradient norm: 0.009172752499580383\n",
      "conv3.bias, gradient norm: 0.00980114284902811\n",
      "conv3.lin.weight, gradient norm: 0.01878116838634014\n",
      "Epoch: 365, Training Loss: 0.1672, Validation Loss: 0.2700, Train Acc: 0.9904, Val Acc: 0.9900, Test Acc: 0.9908\n",
      "conv1.bias, gradient norm: 1.4412423476883873e-09\n",
      "conv1.lin.weight, gradient norm: 0.1009707972407341\n",
      "bn1.module.weight, gradient norm: 0.03096146322786808\n",
      "bn1.module.bias, gradient norm: 0.024003643542528152\n",
      "conv2.bias, gradient norm: 6.569383970145282e-08\n",
      "conv2.lin.weight, gradient norm: 0.045302148908376694\n",
      "bn2.module.weight, gradient norm: 0.01646796055138111\n",
      "bn2.module.bias, gradient norm: 0.006108582951128483\n",
      "conv3.bias, gradient norm: 0.011636565439403057\n",
      "conv3.lin.weight, gradient norm: 0.02348375879228115\n",
      "Epoch: 366, Training Loss: 0.1641, Validation Loss: 0.2704, Train Acc: 0.9903, Val Acc: 0.9900, Test Acc: 0.9907\n",
      "conv1.bias, gradient norm: 1.6448784556644114e-09\n",
      "conv1.lin.weight, gradient norm: 0.10786641389131546\n",
      "bn1.module.weight, gradient norm: 0.02199338935315609\n",
      "bn1.module.bias, gradient norm: 0.01314707100391388\n",
      "conv2.bias, gradient norm: 6.918097739117002e-08\n",
      "conv2.lin.weight, gradient norm: 0.034592777490615845\n",
      "bn2.module.weight, gradient norm: 0.016462594270706177\n",
      "bn2.module.bias, gradient norm: 0.00811754446476698\n",
      "conv3.bias, gradient norm: 0.010699865408241749\n",
      "conv3.lin.weight, gradient norm: 0.018319467082619667\n",
      "Epoch: 367, Training Loss: 0.1649, Validation Loss: 0.2705, Train Acc: 0.9902, Val Acc: 0.9899, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 2.085368766557849e-09\n",
      "conv1.lin.weight, gradient norm: 0.11085961014032364\n",
      "bn1.module.weight, gradient norm: 0.03274928033351898\n",
      "bn1.module.bias, gradient norm: 0.01939048431813717\n",
      "conv2.bias, gradient norm: 5.049691509384502e-08\n",
      "conv2.lin.weight, gradient norm: 0.03670733794569969\n",
      "bn2.module.weight, gradient norm: 0.016323542222380638\n",
      "bn2.module.bias, gradient norm: 0.007815693505108356\n",
      "conv3.bias, gradient norm: 0.010839451104402542\n",
      "conv3.lin.weight, gradient norm: 0.018228819593787193\n",
      "Epoch: 368, Training Loss: 0.1666, Validation Loss: 0.2700, Train Acc: 0.9902, Val Acc: 0.9898, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.6908228150924742e-09\n",
      "conv1.lin.weight, gradient norm: 0.08293189108371735\n",
      "bn1.module.weight, gradient norm: 0.055085744708776474\n",
      "bn1.module.bias, gradient norm: 0.022910574451088905\n",
      "conv2.bias, gradient norm: 7.040201666086432e-08\n",
      "conv2.lin.weight, gradient norm: 0.03616200387477875\n",
      "bn2.module.weight, gradient norm: 0.017542708665132523\n",
      "bn2.module.bias, gradient norm: 0.008911618031561375\n",
      "conv3.bias, gradient norm: 0.01028540451079607\n",
      "conv3.lin.weight, gradient norm: 0.019506802782416344\n",
      "Epoch: 369, Training Loss: 0.1642, Validation Loss: 0.2695, Train Acc: 0.9902, Val Acc: 0.9898, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 2.350669214834511e-09\n",
      "conv1.lin.weight, gradient norm: 0.09713252633810043\n",
      "bn1.module.weight, gradient norm: 0.04728204011917114\n",
      "bn1.module.bias, gradient norm: 0.02275112457573414\n",
      "conv2.bias, gradient norm: 7.093706244631903e-08\n",
      "conv2.lin.weight, gradient norm: 0.03503759950399399\n",
      "bn2.module.weight, gradient norm: 0.01737770438194275\n",
      "bn2.module.bias, gradient norm: 0.00854378193616867\n",
      "conv3.bias, gradient norm: 0.009909539483487606\n",
      "conv3.lin.weight, gradient norm: 0.019547225907444954\n",
      "Epoch: 370, Training Loss: 0.1645, Validation Loss: 0.2690, Train Acc: 0.9903, Val Acc: 0.9898, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.335990429396361e-09\n",
      "conv1.lin.weight, gradient norm: 0.12671536207199097\n",
      "bn1.module.weight, gradient norm: 0.026068147271871567\n",
      "bn1.module.bias, gradient norm: 0.021375516429543495\n",
      "conv2.bias, gradient norm: 6.308818711886488e-08\n",
      "conv2.lin.weight, gradient norm: 0.04359643906354904\n",
      "bn2.module.weight, gradient norm: 0.016560738906264305\n",
      "bn2.module.bias, gradient norm: 0.007721362169831991\n",
      "conv3.bias, gradient norm: 0.011063747107982635\n",
      "conv3.lin.weight, gradient norm: 0.02031520940363407\n",
      "Epoch: 371, Training Loss: 0.1648, Validation Loss: 0.2689, Train Acc: 0.9903, Val Acc: 0.9898, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.4997432185026582e-09\n",
      "conv1.lin.weight, gradient norm: 0.15344318747520447\n",
      "bn1.module.weight, gradient norm: 0.030145103111863136\n",
      "bn1.module.bias, gradient norm: 0.028216643258929253\n",
      "conv2.bias, gradient norm: 7.031417936786966e-08\n",
      "conv2.lin.weight, gradient norm: 0.048892054706811905\n",
      "bn2.module.weight, gradient norm: 0.017771651968359947\n",
      "bn2.module.bias, gradient norm: 0.006399084348231554\n",
      "conv3.bias, gradient norm: 0.010897922329604626\n",
      "conv3.lin.weight, gradient norm: 0.019924253225326538\n",
      "Epoch: 372, Training Loss: 0.1670, Validation Loss: 0.2688, Train Acc: 0.9902, Val Acc: 0.9897, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 1.4846353035835591e-09\n",
      "conv1.lin.weight, gradient norm: 0.08429108560085297\n",
      "bn1.module.weight, gradient norm: 0.021593144163489342\n",
      "bn1.module.bias, gradient norm: 0.017464959993958473\n",
      "conv2.bias, gradient norm: 6.009884145896649e-08\n",
      "conv2.lin.weight, gradient norm: 0.029320072382688522\n",
      "bn2.module.weight, gradient norm: 0.016659071668982506\n",
      "bn2.module.bias, gradient norm: 0.008754879236221313\n",
      "conv3.bias, gradient norm: 0.010159354656934738\n",
      "conv3.lin.weight, gradient norm: 0.0191084872931242\n",
      "Epoch: 373, Training Loss: 0.1638, Validation Loss: 0.2691, Train Acc: 0.9901, Val Acc: 0.9897, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 2.4019044531087275e-09\n",
      "conv1.lin.weight, gradient norm: 0.11705340445041656\n",
      "bn1.module.weight, gradient norm: 0.04529779031872749\n",
      "bn1.module.bias, gradient norm: 0.028591783717274666\n",
      "conv2.bias, gradient norm: 6.927641038600996e-08\n",
      "conv2.lin.weight, gradient norm: 0.040031615644693375\n",
      "bn2.module.weight, gradient norm: 0.01637304574251175\n",
      "bn2.module.bias, gradient norm: 0.008924000896513462\n",
      "conv3.bias, gradient norm: 0.009826742112636566\n",
      "conv3.lin.weight, gradient norm: 0.020143475383520126\n",
      "Epoch: 374, Training Loss: 0.1659, Validation Loss: 0.2693, Train Acc: 0.9902, Val Acc: 0.9897, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 1.6761146914845426e-09\n",
      "conv1.lin.weight, gradient norm: 0.10158270597457886\n",
      "bn1.module.weight, gradient norm: 0.02863250859081745\n",
      "bn1.module.bias, gradient norm: 0.01966710574924946\n",
      "conv2.bias, gradient norm: 5.374316813799851e-08\n",
      "conv2.lin.weight, gradient norm: 0.04655294865369797\n",
      "bn2.module.weight, gradient norm: 0.01663530245423317\n",
      "bn2.module.bias, gradient norm: 0.009588301181793213\n",
      "conv3.bias, gradient norm: 0.01017215009778738\n",
      "conv3.lin.weight, gradient norm: 0.019343214109539986\n",
      "Epoch: 375, Training Loss: 0.1653, Validation Loss: 0.2695, Train Acc: 0.9902, Val Acc: 0.9897, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 2.13713846619612e-09\n",
      "conv1.lin.weight, gradient norm: 0.11302340775728226\n",
      "bn1.module.weight, gradient norm: 0.025082165375351906\n",
      "bn1.module.bias, gradient norm: 0.01623762585222721\n",
      "conv2.bias, gradient norm: 4.650263818462008e-08\n",
      "conv2.lin.weight, gradient norm: 0.03848649561405182\n",
      "bn2.module.weight, gradient norm: 0.015610734932124615\n",
      "bn2.module.bias, gradient norm: 0.008659515529870987\n",
      "conv3.bias, gradient norm: 0.011248216964304447\n",
      "conv3.lin.weight, gradient norm: 0.01942513696849346\n",
      "Epoch: 376, Training Loss: 0.1644, Validation Loss: 0.2697, Train Acc: 0.9902, Val Acc: 0.9898, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.8363263132314955e-09\n",
      "conv1.lin.weight, gradient norm: 0.1385892778635025\n",
      "bn1.module.weight, gradient norm: 0.038949523121118546\n",
      "bn1.module.bias, gradient norm: 0.02767564356327057\n",
      "conv2.bias, gradient norm: 4.8934470697759025e-08\n",
      "conv2.lin.weight, gradient norm: 0.029991164803504944\n",
      "bn2.module.weight, gradient norm: 0.01561383344233036\n",
      "bn2.module.bias, gradient norm: 0.007913773879408836\n",
      "conv3.bias, gradient norm: 0.010827425867319107\n",
      "conv3.lin.weight, gradient norm: 0.017912810668349266\n",
      "Epoch: 377, Training Loss: 0.1665, Validation Loss: 0.2695, Train Acc: 0.9902, Val Acc: 0.9897, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 2.525402553743561e-09\n",
      "conv1.lin.weight, gradient norm: 0.07919941842556\n",
      "bn1.module.weight, gradient norm: 0.018359554931521416\n",
      "bn1.module.bias, gradient norm: 0.016163241118192673\n",
      "conv2.bias, gradient norm: 7.614843866576848e-08\n",
      "conv2.lin.weight, gradient norm: 0.029713839292526245\n",
      "bn2.module.weight, gradient norm: 0.015859665349125862\n",
      "bn2.module.bias, gradient norm: 0.008952743373811245\n",
      "conv3.bias, gradient norm: 0.01038700807839632\n",
      "conv3.lin.weight, gradient norm: 0.01822378672659397\n",
      "Epoch: 378, Training Loss: 0.1655, Validation Loss: 0.2694, Train Acc: 0.9901, Val Acc: 0.9896, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 1.1667254939951022e-09\n",
      "conv1.lin.weight, gradient norm: 0.09411225467920303\n",
      "bn1.module.weight, gradient norm: 0.017694691196084023\n",
      "bn1.module.bias, gradient norm: 0.014163284562528133\n",
      "conv2.bias, gradient norm: 5.4650268310751926e-08\n",
      "conv2.lin.weight, gradient norm: 0.03230041265487671\n",
      "bn2.module.weight, gradient norm: 0.016609318554401398\n",
      "bn2.module.bias, gradient norm: 0.009894336573779583\n",
      "conv3.bias, gradient norm: 0.009784983471035957\n",
      "conv3.lin.weight, gradient norm: 0.01908273436129093\n",
      "Epoch: 379, Training Loss: 0.1654, Validation Loss: 0.2696, Train Acc: 0.9900, Val Acc: 0.9895, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 1.4671567294399779e-09\n",
      "conv1.lin.weight, gradient norm: 0.1142459288239479\n",
      "bn1.module.weight, gradient norm: 0.03369257599115372\n",
      "bn1.module.bias, gradient norm: 0.02180914394557476\n",
      "conv2.bias, gradient norm: 5.424866600378664e-08\n",
      "conv2.lin.weight, gradient norm: 0.05437760427594185\n",
      "bn2.module.weight, gradient norm: 0.016839556396007538\n",
      "bn2.module.bias, gradient norm: 0.011444423347711563\n",
      "conv3.bias, gradient norm: 0.00894968118518591\n",
      "conv3.lin.weight, gradient norm: 0.023370003327727318\n",
      "Epoch: 380, Training Loss: 0.1661, Validation Loss: 0.2705, Train Acc: 0.9901, Val Acc: 0.9896, Test Acc: 0.9904\n",
      "conv1.bias, gradient norm: 2.4197044368179377e-09\n",
      "conv1.lin.weight, gradient norm: 0.11364936083555222\n",
      "bn1.module.weight, gradient norm: 0.04432859644293785\n",
      "bn1.module.bias, gradient norm: 0.02713145688176155\n",
      "conv2.bias, gradient norm: 6.245473116450739e-08\n",
      "conv2.lin.weight, gradient norm: 0.037660278379917145\n",
      "bn2.module.weight, gradient norm: 0.017642349004745483\n",
      "bn2.module.bias, gradient norm: 0.010223934426903725\n",
      "conv3.bias, gradient norm: 0.009137539193034172\n",
      "conv3.lin.weight, gradient norm: 0.020097237080335617\n",
      "Epoch: 381, Training Loss: 0.1664, Validation Loss: 0.2713, Train Acc: 0.9901, Val Acc: 0.9897, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 2.2383572773065907e-09\n",
      "conv1.lin.weight, gradient norm: 0.11517491191625595\n",
      "bn1.module.weight, gradient norm: 0.02719644084572792\n",
      "bn1.module.bias, gradient norm: 0.024186154827475548\n",
      "conv2.bias, gradient norm: 7.762807285871531e-08\n",
      "conv2.lin.weight, gradient norm: 0.04045196250081062\n",
      "bn2.module.weight, gradient norm: 0.017586246132850647\n",
      "bn2.module.bias, gradient norm: 0.0074992794543504715\n",
      "conv3.bias, gradient norm: 0.010633434168994427\n",
      "conv3.lin.weight, gradient norm: 0.018645599484443665\n",
      "Epoch: 382, Training Loss: 0.1641, Validation Loss: 0.2721, Train Acc: 0.9902, Val Acc: 0.9897, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.580567787762277e-09\n",
      "conv1.lin.weight, gradient norm: 0.09970641881227493\n",
      "bn1.module.weight, gradient norm: 0.04613142088055611\n",
      "bn1.module.bias, gradient norm: 0.02316403202712536\n",
      "conv2.bias, gradient norm: 8.246190930094599e-08\n",
      "conv2.lin.weight, gradient norm: 0.03922545164823532\n",
      "bn2.module.weight, gradient norm: 0.016623875126242638\n",
      "bn2.module.bias, gradient norm: 0.007674344815313816\n",
      "conv3.bias, gradient norm: 0.010444165207445621\n",
      "conv3.lin.weight, gradient norm: 0.018303580582141876\n",
      "Epoch: 383, Training Loss: 0.1661, Validation Loss: 0.2726, Train Acc: 0.9903, Val Acc: 0.9898, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.5384790108541324e-09\n",
      "conv1.lin.weight, gradient norm: 0.09578300267457962\n",
      "bn1.module.weight, gradient norm: 0.03617796301841736\n",
      "bn1.module.bias, gradient norm: 0.024884073063731194\n",
      "conv2.bias, gradient norm: 7.165228055328043e-08\n",
      "conv2.lin.weight, gradient norm: 0.04069489985704422\n",
      "bn2.module.weight, gradient norm: 0.016231007874011993\n",
      "bn2.module.bias, gradient norm: 0.007277345284819603\n",
      "conv3.bias, gradient norm: 0.01136687584221363\n",
      "conv3.lin.weight, gradient norm: 0.019559305161237717\n",
      "Epoch: 384, Training Loss: 0.1655, Validation Loss: 0.2724, Train Acc: 0.9903, Val Acc: 0.9898, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.2581201636052697e-09\n",
      "conv1.lin.weight, gradient norm: 0.13838732242584229\n",
      "bn1.module.weight, gradient norm: 0.04620198905467987\n",
      "bn1.module.bias, gradient norm: 0.03097051940858364\n",
      "conv2.bias, gradient norm: 7.473972374327786e-08\n",
      "conv2.lin.weight, gradient norm: 0.050570566207170486\n",
      "bn2.module.weight, gradient norm: 0.016326261684298515\n",
      "bn2.module.bias, gradient norm: 0.006680259946733713\n",
      "conv3.bias, gradient norm: 0.011929626576602459\n",
      "conv3.lin.weight, gradient norm: 0.02288419008255005\n",
      "Epoch: 385, Training Loss: 0.1654, Validation Loss: 0.2711, Train Acc: 0.9902, Val Acc: 0.9897, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.7411179165094381e-09\n",
      "conv1.lin.weight, gradient norm: 0.0906149223446846\n",
      "bn1.module.weight, gradient norm: 0.024916883558034897\n",
      "bn1.module.bias, gradient norm: 0.018352149054408073\n",
      "conv2.bias, gradient norm: 5.245773238016227e-08\n",
      "conv2.lin.weight, gradient norm: 0.03928022086620331\n",
      "bn2.module.weight, gradient norm: 0.01648000255227089\n",
      "bn2.module.bias, gradient norm: 0.007535511162132025\n",
      "conv3.bias, gradient norm: 0.010837242007255554\n",
      "conv3.lin.weight, gradient norm: 0.02049288898706436\n",
      "Epoch: 386, Training Loss: 0.1654, Validation Loss: 0.2696, Train Acc: 0.9901, Val Acc: 0.9895, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 1.621677792051912e-09\n",
      "conv1.lin.weight, gradient norm: 0.11415436118841171\n",
      "bn1.module.weight, gradient norm: 0.04028385877609253\n",
      "bn1.module.bias, gradient norm: 0.028542859479784966\n",
      "conv2.bias, gradient norm: 6.420777509674735e-08\n",
      "conv2.lin.weight, gradient norm: 0.04118594899773598\n",
      "bn2.module.weight, gradient norm: 0.018217233940958977\n",
      "bn2.module.bias, gradient norm: 0.0107477568089962\n",
      "conv3.bias, gradient norm: 0.009428704157471657\n",
      "conv3.lin.weight, gradient norm: 0.021934326738119125\n",
      "Epoch: 387, Training Loss: 0.1658, Validation Loss: 0.2684, Train Acc: 0.9901, Val Acc: 0.9895, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.8413995883648226e-09\n",
      "conv1.lin.weight, gradient norm: 0.1346503049135208\n",
      "bn1.module.weight, gradient norm: 0.04877633601427078\n",
      "bn1.module.bias, gradient norm: 0.030123038217425346\n",
      "conv2.bias, gradient norm: 4.820887511414185e-08\n",
      "conv2.lin.weight, gradient norm: 0.035106852650642395\n",
      "bn2.module.weight, gradient norm: 0.016473324969410896\n",
      "bn2.module.bias, gradient norm: 0.009776453487575054\n",
      "conv3.bias, gradient norm: 0.009868118911981583\n",
      "conv3.lin.weight, gradient norm: 0.019043676555156708\n",
      "Epoch: 388, Training Loss: 0.1641, Validation Loss: 0.2678, Train Acc: 0.9902, Val Acc: 0.9896, Test Acc: 0.9907\n",
      "conv1.bias, gradient norm: 2.0417627588642517e-09\n",
      "conv1.lin.weight, gradient norm: 0.12815473973751068\n",
      "bn1.module.weight, gradient norm: 0.04485389590263367\n",
      "bn1.module.bias, gradient norm: 0.029965823516249657\n",
      "conv2.bias, gradient norm: 7.444502614362136e-08\n",
      "conv2.lin.weight, gradient norm: 0.03354877978563309\n",
      "bn2.module.weight, gradient norm: 0.017346946522593498\n",
      "bn2.module.bias, gradient norm: 0.009470255114138126\n",
      "conv3.bias, gradient norm: 0.009708916768431664\n",
      "conv3.lin.weight, gradient norm: 0.020302269607782364\n",
      "Epoch: 389, Training Loss: 0.1650, Validation Loss: 0.2682, Train Acc: 0.9903, Val Acc: 0.9897, Test Acc: 0.9908\n",
      "conv1.bias, gradient norm: 1.4555099348001477e-09\n",
      "conv1.lin.weight, gradient norm: 0.1495286226272583\n",
      "bn1.module.weight, gradient norm: 0.03229751065373421\n",
      "bn1.module.bias, gradient norm: 0.036196157336235046\n",
      "conv2.bias, gradient norm: 5.358583621273283e-08\n",
      "conv2.lin.weight, gradient norm: 0.04250233992934227\n",
      "bn2.module.weight, gradient norm: 0.01632634550333023\n",
      "bn2.module.bias, gradient norm: 0.007782199885696173\n",
      "conv3.bias, gradient norm: 0.010943888686597347\n",
      "conv3.lin.weight, gradient norm: 0.018679708242416382\n",
      "Epoch: 390, Training Loss: 0.1647, Validation Loss: 0.2688, Train Acc: 0.9903, Val Acc: 0.9897, Test Acc: 0.9908\n",
      "conv1.bias, gradient norm: 1.877732858091008e-09\n",
      "conv1.lin.weight, gradient norm: 0.13235905766487122\n",
      "bn1.module.weight, gradient norm: 0.036743633449077606\n",
      "bn1.module.bias, gradient norm: 0.030355967581272125\n",
      "conv2.bias, gradient norm: 5.698006688703572e-08\n",
      "conv2.lin.weight, gradient norm: 0.05276135355234146\n",
      "bn2.module.weight, gradient norm: 0.016140511259436607\n",
      "bn2.module.bias, gradient norm: 0.0077217151410877705\n",
      "conv3.bias, gradient norm: 0.01096652913838625\n",
      "conv3.lin.weight, gradient norm: 0.02040022425353527\n",
      "Epoch: 391, Training Loss: 0.1651, Validation Loss: 0.2685, Train Acc: 0.9902, Val Acc: 0.9897, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.7659924633761648e-09\n",
      "conv1.lin.weight, gradient norm: 0.12209460884332657\n",
      "bn1.module.weight, gradient norm: 0.03864870220422745\n",
      "bn1.module.bias, gradient norm: 0.031298696994781494\n",
      "conv2.bias, gradient norm: 4.6741728709776e-08\n",
      "conv2.lin.weight, gradient norm: 0.04058174788951874\n",
      "bn2.module.weight, gradient norm: 0.016246674582362175\n",
      "bn2.module.bias, gradient norm: 0.00962724257260561\n",
      "conv3.bias, gradient norm: 0.010223534889519215\n",
      "conv3.lin.weight, gradient norm: 0.01833505555987358\n",
      "Epoch: 392, Training Loss: 0.1676, Validation Loss: 0.2676, Train Acc: 0.9901, Val Acc: 0.9896, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 1.615348632633129e-09\n",
      "conv1.lin.weight, gradient norm: 0.10134567320346832\n",
      "bn1.module.weight, gradient norm: 0.02429337054491043\n",
      "bn1.module.bias, gradient norm: 0.021035214886069298\n",
      "conv2.bias, gradient norm: 7.580344885127488e-08\n",
      "conv2.lin.weight, gradient norm: 0.04677464812994003\n",
      "bn2.module.weight, gradient norm: 0.01650431752204895\n",
      "bn2.module.bias, gradient norm: 0.011346850544214249\n",
      "conv3.bias, gradient norm: 0.009719964116811752\n",
      "conv3.lin.weight, gradient norm: 0.021595513448119164\n",
      "Epoch: 393, Training Loss: 0.1652, Validation Loss: 0.2674, Train Acc: 0.9901, Val Acc: 0.9895, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 1.3539147580843292e-09\n",
      "conv1.lin.weight, gradient norm: 0.12877300381660461\n",
      "bn1.module.weight, gradient norm: 0.04266856610774994\n",
      "bn1.module.bias, gradient norm: 0.026337960734963417\n",
      "conv2.bias, gradient norm: 3.8209240216247053e-08\n",
      "conv2.lin.weight, gradient norm: 0.041349925100803375\n",
      "bn2.module.weight, gradient norm: 0.016568206250667572\n",
      "bn2.module.bias, gradient norm: 0.009421478025615215\n",
      "conv3.bias, gradient norm: 0.009951054118573666\n",
      "conv3.lin.weight, gradient norm: 0.019872907549142838\n",
      "Epoch: 394, Training Loss: 0.1653, Validation Loss: 0.2676, Train Acc: 0.9902, Val Acc: 0.9897, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 2.513066643672346e-09\n",
      "conv1.lin.weight, gradient norm: 0.1356654316186905\n",
      "bn1.module.weight, gradient norm: 0.05279292166233063\n",
      "bn1.module.bias, gradient norm: 0.02894413471221924\n",
      "conv2.bias, gradient norm: 6.625535320381459e-08\n",
      "conv2.lin.weight, gradient norm: 0.03806639835238457\n",
      "bn2.module.weight, gradient norm: 0.017621342092752457\n",
      "bn2.module.bias, gradient norm: 0.00870824046432972\n",
      "conv3.bias, gradient norm: 0.010066082701086998\n",
      "conv3.lin.weight, gradient norm: 0.01958691142499447\n",
      "Epoch: 395, Training Loss: 0.1641, Validation Loss: 0.2681, Train Acc: 0.9903, Val Acc: 0.9898, Test Acc: 0.9907\n",
      "conv1.bias, gradient norm: 2.895424344728781e-09\n",
      "conv1.lin.weight, gradient norm: 0.18383590877056122\n",
      "bn1.module.weight, gradient norm: 0.04614419490098953\n",
      "bn1.module.bias, gradient norm: 0.040648430585861206\n",
      "conv2.bias, gradient norm: 7.184815586924742e-08\n",
      "conv2.lin.weight, gradient norm: 0.06296220421791077\n",
      "bn2.module.weight, gradient norm: 0.017015982419252396\n",
      "bn2.module.bias, gradient norm: 0.005857342388480902\n",
      "conv3.bias, gradient norm: 0.01208674069494009\n",
      "conv3.lin.weight, gradient norm: 0.025451431050896645\n",
      "Epoch: 396, Training Loss: 0.1643, Validation Loss: 0.2683, Train Acc: 0.9902, Val Acc: 0.9897, Test Acc: 0.9908\n",
      "conv1.bias, gradient norm: 2.3120865222381326e-09\n",
      "conv1.lin.weight, gradient norm: 0.1280081570148468\n",
      "bn1.module.weight, gradient norm: 0.035704322159290314\n",
      "bn1.module.bias, gradient norm: 0.0262403953820467\n",
      "conv2.bias, gradient norm: 4.974422651571331e-08\n",
      "conv2.lin.weight, gradient norm: 0.03816111385822296\n",
      "bn2.module.weight, gradient norm: 0.016906553879380226\n",
      "bn2.module.bias, gradient norm: 0.0078031206503510475\n",
      "conv3.bias, gradient norm: 0.010506483726203442\n",
      "conv3.lin.weight, gradient norm: 0.019225073978304863\n",
      "Epoch: 397, Training Loss: 0.1660, Validation Loss: 0.2685, Train Acc: 0.9902, Val Acc: 0.9896, Test Acc: 0.9908\n",
      "conv1.bias, gradient norm: 1.6375795164336182e-09\n",
      "conv1.lin.weight, gradient norm: 0.10741481184959412\n",
      "bn1.module.weight, gradient norm: 0.038279179483652115\n",
      "bn1.module.bias, gradient norm: 0.024700185284018517\n",
      "conv2.bias, gradient norm: 5.690661453172652e-08\n",
      "conv2.lin.weight, gradient norm: 0.03341541811823845\n",
      "bn2.module.weight, gradient norm: 0.016237815842032433\n",
      "bn2.module.bias, gradient norm: 0.009109687991440296\n",
      "conv3.bias, gradient norm: 0.010355675593018532\n",
      "conv3.lin.weight, gradient norm: 0.018259137868881226\n",
      "Epoch: 398, Training Loss: 0.1650, Validation Loss: 0.2686, Train Acc: 0.9901, Val Acc: 0.9895, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.730638188313094e-09\n",
      "conv1.lin.weight, gradient norm: 0.15873834490776062\n",
      "bn1.module.weight, gradient norm: 0.04621351882815361\n",
      "bn1.module.bias, gradient norm: 0.035788144916296005\n",
      "conv2.bias, gradient norm: 4.257762142856336e-08\n",
      "conv2.lin.weight, gradient norm: 0.03715880587697029\n",
      "bn2.module.weight, gradient norm: 0.016255173832178116\n",
      "bn2.module.bias, gradient norm: 0.009129173122346401\n",
      "conv3.bias, gradient norm: 0.00986851379275322\n",
      "conv3.lin.weight, gradient norm: 0.018536649644374847\n",
      "Epoch: 399, Training Loss: 0.1653, Validation Loss: 0.2683, Train Acc: 0.9901, Val Acc: 0.9895, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 2.372832375030498e-09\n",
      "conv1.lin.weight, gradient norm: 0.12660226225852966\n",
      "bn1.module.weight, gradient norm: 0.0315493606030941\n",
      "bn1.module.bias, gradient norm: 0.024459846317768097\n",
      "conv2.bias, gradient norm: 5.332139352276499e-08\n",
      "conv2.lin.weight, gradient norm: 0.0377688929438591\n",
      "bn2.module.weight, gradient norm: 0.017220966517925262\n",
      "bn2.module.bias, gradient norm: 0.008734607137739658\n",
      "conv3.bias, gradient norm: 0.0099576311185956\n",
      "conv3.lin.weight, gradient norm: 0.020294923335313797\n",
      "Epoch: 400, Training Loss: 0.1634, Validation Loss: 0.2678, Train Acc: 0.9902, Val Acc: 0.9895, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 2.5899267175333307e-09\n",
      "conv1.lin.weight, gradient norm: 0.10750454664230347\n",
      "bn1.module.weight, gradient norm: 0.02356126718223095\n",
      "bn1.module.bias, gradient norm: 0.02215583436191082\n",
      "conv2.bias, gradient norm: 6.06178502948751e-08\n",
      "conv2.lin.weight, gradient norm: 0.029439516365528107\n",
      "bn2.module.weight, gradient norm: 0.015824493020772934\n",
      "bn2.module.bias, gradient norm: 0.008379701524972916\n",
      "conv3.bias, gradient norm: 0.010402855463325977\n",
      "conv3.lin.weight, gradient norm: 0.01778268814086914\n",
      "Epoch: 401, Training Loss: 0.1675, Validation Loss: 0.2677, Train Acc: 0.9902, Val Acc: 0.9896, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.4849015350648642e-09\n",
      "conv1.lin.weight, gradient norm: 0.10241372138261795\n",
      "bn1.module.weight, gradient norm: 0.030045734718441963\n",
      "bn1.module.bias, gradient norm: 0.019155066460371017\n",
      "conv2.bias, gradient norm: 8.15844671819832e-08\n",
      "conv2.lin.weight, gradient norm: 0.03745642304420471\n",
      "bn2.module.weight, gradient norm: 0.015967639163136482\n",
      "bn2.module.bias, gradient norm: 0.006721071433275938\n",
      "conv3.bias, gradient norm: 0.011105583049356937\n",
      "conv3.lin.weight, gradient norm: 0.018378503620624542\n",
      "Epoch: 402, Training Loss: 0.1678, Validation Loss: 0.2672, Train Acc: 0.9902, Val Acc: 0.9897, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 2.4171762369462613e-09\n",
      "conv1.lin.weight, gradient norm: 0.11877566576004028\n",
      "bn1.module.weight, gradient norm: 0.04357872158288956\n",
      "bn1.module.bias, gradient norm: 0.02869061753153801\n",
      "conv2.bias, gradient norm: 7.601243368071664e-08\n",
      "conv2.lin.weight, gradient norm: 0.03872545436024666\n",
      "bn2.module.weight, gradient norm: 0.016735970973968506\n",
      "bn2.module.bias, gradient norm: 0.0069227060303092\n",
      "conv3.bias, gradient norm: 0.01098579727113247\n",
      "conv3.lin.weight, gradient norm: 0.020107749849557877\n",
      "Epoch: 403, Training Loss: 0.1658, Validation Loss: 0.2667, Train Acc: 0.9901, Val Acc: 0.9896, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.6479813069736338e-09\n",
      "conv1.lin.weight, gradient norm: 0.11483422666788101\n",
      "bn1.module.weight, gradient norm: 0.039658673107624054\n",
      "bn1.module.bias, gradient norm: 0.030412668362259865\n",
      "conv2.bias, gradient norm: 5.0306507404229706e-08\n",
      "conv2.lin.weight, gradient norm: 0.041330836713314056\n",
      "bn2.module.weight, gradient norm: 0.017200110480189323\n",
      "bn2.module.bias, gradient norm: 0.008672419004142284\n",
      "conv3.bias, gradient norm: 0.010107327252626419\n",
      "conv3.lin.weight, gradient norm: 0.020125051960349083\n",
      "Epoch: 404, Training Loss: 0.1662, Validation Loss: 0.2662, Train Acc: 0.9901, Val Acc: 0.9896, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 2.0862476191041424e-09\n",
      "conv1.lin.weight, gradient norm: 0.12142059206962585\n",
      "bn1.module.weight, gradient norm: 0.03888826444745064\n",
      "bn1.module.bias, gradient norm: 0.028049778193235397\n",
      "conv2.bias, gradient norm: 6.189191026351182e-08\n",
      "conv2.lin.weight, gradient norm: 0.04055185616016388\n",
      "bn2.module.weight, gradient norm: 0.01723122037947178\n",
      "bn2.module.bias, gradient norm: 0.009493334218859673\n",
      "conv3.bias, gradient norm: 0.009899701923131943\n",
      "conv3.lin.weight, gradient norm: 0.02096940577030182\n",
      "Epoch: 405, Training Loss: 0.1640, Validation Loss: 0.2663, Train Acc: 0.9901, Val Acc: 0.9896, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.90106574926574e-09\n",
      "conv1.lin.weight, gradient norm: 0.09414002299308777\n",
      "bn1.module.weight, gradient norm: 0.04896567016839981\n",
      "bn1.module.bias, gradient norm: 0.023615634068846703\n",
      "conv2.bias, gradient norm: 8.596094858148717e-08\n",
      "conv2.lin.weight, gradient norm: 0.035064101219177246\n",
      "bn2.module.weight, gradient norm: 0.017311807721853256\n",
      "bn2.module.bias, gradient norm: 0.010700840502977371\n",
      "conv3.bias, gradient norm: 0.009793720208108425\n",
      "conv3.lin.weight, gradient norm: 0.020031040534377098\n",
      "Epoch: 406, Training Loss: 0.1638, Validation Loss: 0.2667, Train Acc: 0.9902, Val Acc: 0.9896, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.5135784847686296e-09\n",
      "conv1.lin.weight, gradient norm: 0.09570398926734924\n",
      "bn1.module.weight, gradient norm: 0.02071388065814972\n",
      "bn1.module.bias, gradient norm: 0.019407738000154495\n",
      "conv2.bias, gradient norm: 4.941826858839704e-08\n",
      "conv2.lin.weight, gradient norm: 0.03607165440917015\n",
      "bn2.module.weight, gradient norm: 0.017087744548916817\n",
      "bn2.module.bias, gradient norm: 0.00891256146132946\n",
      "conv3.bias, gradient norm: 0.010555616579949856\n",
      "conv3.lin.weight, gradient norm: 0.018801985308527946\n",
      "Epoch: 407, Training Loss: 0.1633, Validation Loss: 0.2671, Train Acc: 0.9902, Val Acc: 0.9897, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.625521273140862e-09\n",
      "conv1.lin.weight, gradient norm: 0.10733603686094284\n",
      "bn1.module.weight, gradient norm: 0.02263149432837963\n",
      "bn1.module.bias, gradient norm: 0.02118533104658127\n",
      "conv2.bias, gradient norm: 6.829800014429566e-08\n",
      "conv2.lin.weight, gradient norm: 0.0388672836124897\n",
      "bn2.module.weight, gradient norm: 0.016973543912172318\n",
      "bn2.module.bias, gradient norm: 0.007725378032773733\n",
      "conv3.bias, gradient norm: 0.010798437520861626\n",
      "conv3.lin.weight, gradient norm: 0.019791914150118828\n",
      "Epoch: 408, Training Loss: 0.1673, Validation Loss: 0.2673, Train Acc: 0.9902, Val Acc: 0.9897, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 2.1307036135453927e-09\n",
      "conv1.lin.weight, gradient norm: 0.09314368665218353\n",
      "bn1.module.weight, gradient norm: 0.02287752740085125\n",
      "bn1.module.bias, gradient norm: 0.011994208209216595\n",
      "conv2.bias, gradient norm: 5.670609226626766e-08\n",
      "conv2.lin.weight, gradient norm: 0.038912370800971985\n",
      "bn2.module.weight, gradient norm: 0.01663949340581894\n",
      "bn2.module.bias, gradient norm: 0.008666330017149448\n",
      "conv3.bias, gradient norm: 0.01027660258114338\n",
      "conv3.lin.weight, gradient norm: 0.01899583637714386\n",
      "Epoch: 409, Training Loss: 0.1654, Validation Loss: 0.2673, Train Acc: 0.9902, Val Acc: 0.9896, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.0828046237421063e-09\n",
      "conv1.lin.weight, gradient norm: 0.1116374135017395\n",
      "bn1.module.weight, gradient norm: 0.01704050414264202\n",
      "bn1.module.bias, gradient norm: 0.013309474103152752\n",
      "conv2.bias, gradient norm: 5.433790306597075e-08\n",
      "conv2.lin.weight, gradient norm: 0.04342803731560707\n",
      "bn2.module.weight, gradient norm: 0.016326021403074265\n",
      "bn2.module.bias, gradient norm: 0.008821722120046616\n",
      "conv3.bias, gradient norm: 0.01032470166683197\n",
      "conv3.lin.weight, gradient norm: 0.019296061247587204\n",
      "Epoch: 410, Training Loss: 0.1651, Validation Loss: 0.2677, Train Acc: 0.9901, Val Acc: 0.9896, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.8672012824794137e-09\n",
      "conv1.lin.weight, gradient norm: 0.09267841279506683\n",
      "bn1.module.weight, gradient norm: 0.031322311609983444\n",
      "bn1.module.bias, gradient norm: 0.02297075092792511\n",
      "conv2.bias, gradient norm: 7.245495226015919e-08\n",
      "conv2.lin.weight, gradient norm: 0.029770085588097572\n",
      "bn2.module.weight, gradient norm: 0.017092563211917877\n",
      "bn2.module.bias, gradient norm: 0.00835967157036066\n",
      "conv3.bias, gradient norm: 0.009989389218389988\n",
      "conv3.lin.weight, gradient norm: 0.018488062545657158\n",
      "Epoch: 411, Training Loss: 0.1660, Validation Loss: 0.2679, Train Acc: 0.9901, Val Acc: 0.9896, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.2199387056099908e-09\n",
      "conv1.lin.weight, gradient norm: 0.10256704688072205\n",
      "bn1.module.weight, gradient norm: 0.034586094319820404\n",
      "bn1.module.bias, gradient norm: 0.022458422929048538\n",
      "conv2.bias, gradient norm: 5.2430856101182144e-08\n",
      "conv2.lin.weight, gradient norm: 0.037777550518512726\n",
      "bn2.module.weight, gradient norm: 0.017082413658499718\n",
      "bn2.module.bias, gradient norm: 0.009445038624107838\n",
      "conv3.bias, gradient norm: 0.00986699853092432\n",
      "conv3.lin.weight, gradient norm: 0.019140927121043205\n",
      "Epoch: 412, Training Loss: 0.1647, Validation Loss: 0.2681, Train Acc: 0.9901, Val Acc: 0.9895, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 1.1068845839901087e-09\n",
      "conv1.lin.weight, gradient norm: 0.08644749969244003\n",
      "bn1.module.weight, gradient norm: 0.05544544383883476\n",
      "bn1.module.bias, gradient norm: 0.024066340178251266\n",
      "conv2.bias, gradient norm: 6.237993233071393e-08\n",
      "conv2.lin.weight, gradient norm: 0.03863169625401497\n",
      "bn2.module.weight, gradient norm: 0.016967248171567917\n",
      "bn2.module.bias, gradient norm: 0.01048351265490055\n",
      "conv3.bias, gradient norm: 0.00934570748358965\n",
      "conv3.lin.weight, gradient norm: 0.02009303867816925\n",
      "Epoch: 413, Training Loss: 0.1641, Validation Loss: 0.2687, Train Acc: 0.9901, Val Acc: 0.9896, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 2.0501067510281246e-09\n",
      "conv1.lin.weight, gradient norm: 0.10397125035524368\n",
      "bn1.module.weight, gradient norm: 0.030984727665781975\n",
      "bn1.module.bias, gradient norm: 0.021400365978479385\n",
      "conv2.bias, gradient norm: 6.409570119103591e-08\n",
      "conv2.lin.weight, gradient norm: 0.03401323780417442\n",
      "bn2.module.weight, gradient norm: 0.01701337657868862\n",
      "bn2.module.bias, gradient norm: 0.009422605857253075\n",
      "conv3.bias, gradient norm: 0.009727882221341133\n",
      "conv3.lin.weight, gradient norm: 0.01908308081328869\n",
      "Epoch: 414, Training Loss: 0.1653, Validation Loss: 0.2692, Train Acc: 0.9901, Val Acc: 0.9896, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.569900875963981e-09\n",
      "conv1.lin.weight, gradient norm: 0.09843269735574722\n",
      "bn1.module.weight, gradient norm: 0.01813972182571888\n",
      "bn1.module.bias, gradient norm: 0.013920980505645275\n",
      "conv2.bias, gradient norm: 7.265071388928845e-08\n",
      "conv2.lin.weight, gradient norm: 0.03741512447595596\n",
      "bn2.module.weight, gradient norm: 0.01648828014731407\n",
      "bn2.module.bias, gradient norm: 0.007435349281877279\n",
      "conv3.bias, gradient norm: 0.010652477853000164\n",
      "conv3.lin.weight, gradient norm: 0.01931976154446602\n",
      "Epoch: 415, Training Loss: 0.1649, Validation Loss: 0.2698, Train Acc: 0.9901, Val Acc: 0.9897, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.727055609634931e-09\n",
      "conv1.lin.weight, gradient norm: 0.09836780279874802\n",
      "bn1.module.weight, gradient norm: 0.024616865441203117\n",
      "bn1.module.bias, gradient norm: 0.017228243872523308\n",
      "conv2.bias, gradient norm: 5.0605862611519115e-08\n",
      "conv2.lin.weight, gradient norm: 0.03607464209198952\n",
      "bn2.module.weight, gradient norm: 0.016551423817873\n",
      "bn2.module.bias, gradient norm: 0.008495808579027653\n",
      "conv3.bias, gradient norm: 0.010522738099098206\n",
      "conv3.lin.weight, gradient norm: 0.01925978809595108\n",
      "Epoch: 416, Training Loss: 0.1640, Validation Loss: 0.2705, Train Acc: 0.9902, Val Acc: 0.9898, Test Acc: 0.9907\n",
      "conv1.bias, gradient norm: 1.9061212608306732e-09\n",
      "conv1.lin.weight, gradient norm: 0.09296334534883499\n",
      "bn1.module.weight, gradient norm: 0.03471292182803154\n",
      "bn1.module.bias, gradient norm: 0.0254657045006752\n",
      "conv2.bias, gradient norm: 6.472315305927623e-08\n",
      "conv2.lin.weight, gradient norm: 0.0397428497672081\n",
      "bn2.module.weight, gradient norm: 0.016308411955833435\n",
      "bn2.module.bias, gradient norm: 0.0073035527020692825\n",
      "conv3.bias, gradient norm: 0.01098471786826849\n",
      "conv3.lin.weight, gradient norm: 0.019574914127588272\n",
      "Epoch: 417, Training Loss: 0.1642, Validation Loss: 0.2706, Train Acc: 0.9902, Val Acc: 0.9897, Test Acc: 0.9907\n",
      "conv1.bias, gradient norm: 2.9422186909044967e-09\n",
      "conv1.lin.weight, gradient norm: 0.113115593791008\n",
      "bn1.module.weight, gradient norm: 0.03971379995346069\n",
      "bn1.module.bias, gradient norm: 0.027245741337537766\n",
      "conv2.bias, gradient norm: 6.235993765812964e-08\n",
      "conv2.lin.weight, gradient norm: 0.03730557858943939\n",
      "bn2.module.weight, gradient norm: 0.01573169231414795\n",
      "bn2.module.bias, gradient norm: 0.0076364497654139996\n",
      "conv3.bias, gradient norm: 0.011247607879340649\n",
      "conv3.lin.weight, gradient norm: 0.021429797634482384\n",
      "Epoch: 418, Training Loss: 0.1650, Validation Loss: 0.2704, Train Acc: 0.9902, Val Acc: 0.9897, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.0495821989309206e-09\n",
      "conv1.lin.weight, gradient norm: 0.09677694737911224\n",
      "bn1.module.weight, gradient norm: 0.027303609997034073\n",
      "bn1.module.bias, gradient norm: 0.018055105581879616\n",
      "conv2.bias, gradient norm: 6.077827663375501e-08\n",
      "conv2.lin.weight, gradient norm: 0.04271373152732849\n",
      "bn2.module.weight, gradient norm: 0.016061028465628624\n",
      "bn2.module.bias, gradient norm: 0.008388872258365154\n",
      "conv3.bias, gradient norm: 0.011019956320524216\n",
      "conv3.lin.weight, gradient norm: 0.018726233392953873\n",
      "Epoch: 419, Training Loss: 0.1634, Validation Loss: 0.2698, Train Acc: 0.9901, Val Acc: 0.9897, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 2.857200920303171e-09\n",
      "conv1.lin.weight, gradient norm: 0.11540867388248444\n",
      "bn1.module.weight, gradient norm: 0.021786151453852654\n",
      "bn1.module.bias, gradient norm: 0.01889999769628048\n",
      "conv2.bias, gradient norm: 5.3232742658337884e-08\n",
      "conv2.lin.weight, gradient norm: 0.034400466829538345\n",
      "bn2.module.weight, gradient norm: 0.016974184662103653\n",
      "bn2.module.bias, gradient norm: 0.00740932347252965\n",
      "conv3.bias, gradient norm: 0.010830105282366276\n",
      "conv3.lin.weight, gradient norm: 0.018661851063370705\n",
      "Epoch: 420, Training Loss: 0.1649, Validation Loss: 0.2688, Train Acc: 0.9901, Val Acc: 0.9897, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 2.501236773255755e-09\n",
      "conv1.lin.weight, gradient norm: 0.07173029333353043\n",
      "bn1.module.weight, gradient norm: 0.020460793748497963\n",
      "bn1.module.bias, gradient norm: 0.01659397780895233\n",
      "conv2.bias, gradient norm: 5.389865975757857e-08\n",
      "conv2.lin.weight, gradient norm: 0.031354617327451706\n",
      "bn2.module.weight, gradient norm: 0.015798235312104225\n",
      "bn2.module.bias, gradient norm: 0.00908718816936016\n",
      "conv3.bias, gradient norm: 0.010333000682294369\n",
      "conv3.lin.weight, gradient norm: 0.018048737198114395\n",
      "Epoch: 421, Training Loss: 0.1640, Validation Loss: 0.2679, Train Acc: 0.9901, Val Acc: 0.9897, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 1.379662384337621e-09\n",
      "conv1.lin.weight, gradient norm: 0.08284187316894531\n",
      "bn1.module.weight, gradient norm: 0.0291296374052763\n",
      "bn1.module.bias, gradient norm: 0.01825210638344288\n",
      "conv2.bias, gradient norm: 6.649220551935286e-08\n",
      "conv2.lin.weight, gradient norm: 0.033482808619737625\n",
      "bn2.module.weight, gradient norm: 0.01665942743420601\n",
      "bn2.module.bias, gradient norm: 0.00788956880569458\n",
      "conv3.bias, gradient norm: 0.010231437161564827\n",
      "conv3.lin.weight, gradient norm: 0.018205750733613968\n",
      "Epoch: 422, Training Loss: 0.1644, Validation Loss: 0.2670, Train Acc: 0.9901, Val Acc: 0.9897, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 2.3793409464900606e-09\n",
      "conv1.lin.weight, gradient norm: 0.10658816248178482\n",
      "bn1.module.weight, gradient norm: 0.02781660482287407\n",
      "bn1.module.bias, gradient norm: 0.019916528835892677\n",
      "conv2.bias, gradient norm: 3.810321658193061e-08\n",
      "conv2.lin.weight, gradient norm: 0.03730206564068794\n",
      "bn2.module.weight, gradient norm: 0.01737484335899353\n",
      "bn2.module.bias, gradient norm: 0.009110522456467152\n",
      "conv3.bias, gradient norm: 0.009589826688170433\n",
      "conv3.lin.weight, gradient norm: 0.019282396882772446\n",
      "Epoch: 423, Training Loss: 0.1639, Validation Loss: 0.2664, Train Acc: 0.9902, Val Acc: 0.9897, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 2.7867708141116054e-09\n",
      "conv1.lin.weight, gradient norm: 0.09183526039123535\n",
      "bn1.module.weight, gradient norm: 0.019871408119797707\n",
      "bn1.module.bias, gradient norm: 0.018216021358966827\n",
      "conv2.bias, gradient norm: 6.88943941895559e-08\n",
      "conv2.lin.weight, gradient norm: 0.02981617860496044\n",
      "bn2.module.weight, gradient norm: 0.016833176836371422\n",
      "bn2.module.bias, gradient norm: 0.009025185368955135\n",
      "conv3.bias, gradient norm: 0.009873390197753906\n",
      "conv3.lin.weight, gradient norm: 0.018821366131305695\n",
      "Epoch: 424, Training Loss: 0.1646, Validation Loss: 0.2663, Train Acc: 0.9902, Val Acc: 0.9897, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 1.8326722361905468e-09\n",
      "conv1.lin.weight, gradient norm: 0.10026785731315613\n",
      "bn1.module.weight, gradient norm: 0.025652414187788963\n",
      "bn1.module.bias, gradient norm: 0.01656152680516243\n",
      "conv2.bias, gradient norm: 8.122987082970212e-08\n",
      "conv2.lin.weight, gradient norm: 0.031007634475827217\n",
      "bn2.module.weight, gradient norm: 0.017594506964087486\n",
      "bn2.module.bias, gradient norm: 0.009250558912754059\n",
      "conv3.bias, gradient norm: 0.00965733453631401\n",
      "conv3.lin.weight, gradient norm: 0.01965908147394657\n",
      "Epoch: 425, Training Loss: 0.1653, Validation Loss: 0.2663, Train Acc: 0.9902, Val Acc: 0.9898, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.643988056798662e-09\n",
      "conv1.lin.weight, gradient norm: 0.135982483625412\n",
      "bn1.module.weight, gradient norm: 0.05004136636853218\n",
      "bn1.module.bias, gradient norm: 0.033236462622880936\n",
      "conv2.bias, gradient norm: 8.255742756091422e-08\n",
      "conv2.lin.weight, gradient norm: 0.04928860440850258\n",
      "bn2.module.weight, gradient norm: 0.01630515418946743\n",
      "bn2.module.bias, gradient norm: 0.008213748224079609\n",
      "conv3.bias, gradient norm: 0.011346574872732162\n",
      "conv3.lin.weight, gradient norm: 0.020829912275075912\n",
      "Epoch: 426, Training Loss: 0.1648, Validation Loss: 0.2664, Train Acc: 0.9902, Val Acc: 0.9897, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 2.170998714134953e-09\n",
      "conv1.lin.weight, gradient norm: 0.10517758131027222\n",
      "bn1.module.weight, gradient norm: 0.04630995914340019\n",
      "bn1.module.bias, gradient norm: 0.022844886407256126\n",
      "conv2.bias, gradient norm: 6.75084663726011e-08\n",
      "conv2.lin.weight, gradient norm: 0.04060571268200874\n",
      "bn2.module.weight, gradient norm: 0.017048966139554977\n",
      "bn2.module.bias, gradient norm: 0.010400108061730862\n",
      "conv3.bias, gradient norm: 0.00970613956451416\n",
      "conv3.lin.weight, gradient norm: 0.020312730222940445\n",
      "Epoch: 427, Training Loss: 0.1636, Validation Loss: 0.2671, Train Acc: 0.9902, Val Acc: 0.9897, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 2.0957102719876275e-09\n",
      "conv1.lin.weight, gradient norm: 0.09543833881616592\n",
      "bn1.module.weight, gradient norm: 0.027332521975040436\n",
      "bn1.module.bias, gradient norm: 0.02049981988966465\n",
      "conv2.bias, gradient norm: 5.26322168070692e-08\n",
      "conv2.lin.weight, gradient norm: 0.03211143985390663\n",
      "bn2.module.weight, gradient norm: 0.01663205400109291\n",
      "bn2.module.bias, gradient norm: 0.008869902230799198\n",
      "conv3.bias, gradient norm: 0.01017808634787798\n",
      "conv3.lin.weight, gradient norm: 0.0186280757188797\n",
      "Epoch: 428, Training Loss: 0.1646, Validation Loss: 0.2677, Train Acc: 0.9902, Val Acc: 0.9897, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.6351854315033165e-09\n",
      "conv1.lin.weight, gradient norm: 0.09546968340873718\n",
      "bn1.module.weight, gradient norm: 0.026315974071621895\n",
      "bn1.module.bias, gradient norm: 0.017121849581599236\n",
      "conv2.bias, gradient norm: 6.041641142928711e-08\n",
      "conv2.lin.weight, gradient norm: 0.03451588749885559\n",
      "bn2.module.weight, gradient norm: 0.016903357580304146\n",
      "bn2.module.bias, gradient norm: 0.007620010059326887\n",
      "conv3.bias, gradient norm: 0.010745796374976635\n",
      "conv3.lin.weight, gradient norm: 0.018626227974891663\n",
      "Epoch: 429, Training Loss: 0.1658, Validation Loss: 0.2681, Train Acc: 0.9902, Val Acc: 0.9897, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 3.1708278225295317e-09\n",
      "conv1.lin.weight, gradient norm: 0.12094118446111679\n",
      "bn1.module.weight, gradient norm: 0.04380489140748978\n",
      "bn1.module.bias, gradient norm: 0.029764434322714806\n",
      "conv2.bias, gradient norm: 6.050775169796907e-08\n",
      "conv2.lin.weight, gradient norm: 0.04067147150635719\n",
      "bn2.module.weight, gradient norm: 0.01561757642775774\n",
      "bn2.module.bias, gradient norm: 0.009830414317548275\n",
      "conv3.bias, gradient norm: 0.00988418236374855\n",
      "conv3.lin.weight, gradient norm: 0.019384531304240227\n",
      "Epoch: 430, Training Loss: 0.1666, Validation Loss: 0.2684, Train Acc: 0.9902, Val Acc: 0.9898, Test Acc: 0.9907\n",
      "conv1.bias, gradient norm: 1.6650739675938553e-09\n",
      "conv1.lin.weight, gradient norm: 0.10609514266252518\n",
      "bn1.module.weight, gradient norm: 0.025725167244672775\n",
      "bn1.module.bias, gradient norm: 0.025672299787402153\n",
      "conv2.bias, gradient norm: 5.7007977005696375e-08\n",
      "conv2.lin.weight, gradient norm: 0.037756312638521194\n",
      "bn2.module.weight, gradient norm: 0.015964020043611526\n",
      "bn2.module.bias, gradient norm: 0.007779420353472233\n",
      "conv3.bias, gradient norm: 0.010373473167419434\n",
      "conv3.lin.weight, gradient norm: 0.01813262142241001\n",
      "Epoch: 431, Training Loss: 0.1654, Validation Loss: 0.2680, Train Acc: 0.9903, Val Acc: 0.9898, Test Acc: 0.9907\n",
      "conv1.bias, gradient norm: 1.3862179182311252e-09\n",
      "conv1.lin.weight, gradient norm: 0.13322792947292328\n",
      "bn1.module.weight, gradient norm: 0.03966176509857178\n",
      "bn1.module.bias, gradient norm: 0.03147175535559654\n",
      "conv2.bias, gradient norm: 5.741529918168453e-08\n",
      "conv2.lin.weight, gradient norm: 0.03757309541106224\n",
      "bn2.module.weight, gradient norm: 0.01637127250432968\n",
      "bn2.module.bias, gradient norm: 0.0075005716644227505\n",
      "conv3.bias, gradient norm: 0.011078394949436188\n",
      "conv3.lin.weight, gradient norm: 0.018610218539834023\n",
      "Epoch: 432, Training Loss: 0.1646, Validation Loss: 0.2672, Train Acc: 0.9902, Val Acc: 0.9898, Test Acc: 0.9907\n",
      "conv1.bias, gradient norm: 2.5924780100439193e-09\n",
      "conv1.lin.weight, gradient norm: 0.10690126568078995\n",
      "bn1.module.weight, gradient norm: 0.01626325584948063\n",
      "bn1.module.bias, gradient norm: 0.01166155282407999\n",
      "conv2.bias, gradient norm: 7.437071758431557e-08\n",
      "conv2.lin.weight, gradient norm: 0.03490930795669556\n",
      "bn2.module.weight, gradient norm: 0.01695801317691803\n",
      "bn2.module.bias, gradient norm: 0.009257540106773376\n",
      "conv3.bias, gradient norm: 0.010021036490797997\n",
      "conv3.lin.weight, gradient norm: 0.019246267154812813\n",
      "Epoch: 433, Training Loss: 0.1653, Validation Loss: 0.2665, Train Acc: 0.9902, Val Acc: 0.9898, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.307771113623346e-09\n",
      "conv1.lin.weight, gradient norm: 0.08117181807756424\n",
      "bn1.module.weight, gradient norm: 0.025294460356235504\n",
      "bn1.module.bias, gradient norm: 0.013883810490369797\n",
      "conv2.bias, gradient norm: 6.938939378642317e-08\n",
      "conv2.lin.weight, gradient norm: 0.030452899634838104\n",
      "bn2.module.weight, gradient norm: 0.01687171682715416\n",
      "bn2.module.bias, gradient norm: 0.007543839048594236\n",
      "conv3.bias, gradient norm: 0.010632532648742199\n",
      "conv3.lin.weight, gradient norm: 0.018160276114940643\n",
      "Epoch: 434, Training Loss: 0.1649, Validation Loss: 0.2658, Train Acc: 0.9903, Val Acc: 0.9898, Test Acc: 0.9907\n",
      "conv1.bias, gradient norm: 2.7938380497971593e-09\n",
      "conv1.lin.weight, gradient norm: 0.1396225094795227\n",
      "bn1.module.weight, gradient norm: 0.05454816669225693\n",
      "bn1.module.bias, gradient norm: 0.04269174113869667\n",
      "conv2.bias, gradient norm: 7.679600599885816e-08\n",
      "conv2.lin.weight, gradient norm: 0.03920798748731613\n",
      "bn2.module.weight, gradient norm: 0.016300581395626068\n",
      "bn2.module.bias, gradient norm: 0.00783569272607565\n",
      "conv3.bias, gradient norm: 0.010205728933215141\n",
      "conv3.lin.weight, gradient norm: 0.017964093014597893\n",
      "Epoch: 435, Training Loss: 0.1665, Validation Loss: 0.2650, Train Acc: 0.9902, Val Acc: 0.9898, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.0573446562744948e-09\n",
      "conv1.lin.weight, gradient norm: 0.10627222061157227\n",
      "bn1.module.weight, gradient norm: 0.030521240085363388\n",
      "bn1.module.bias, gradient norm: 0.023959428071975708\n",
      "conv2.bias, gradient norm: 7.152137015964399e-08\n",
      "conv2.lin.weight, gradient norm: 0.035920944064855576\n",
      "bn2.module.weight, gradient norm: 0.017143962904810905\n",
      "bn2.module.bias, gradient norm: 0.00817075464874506\n",
      "conv3.bias, gradient norm: 0.010342164896428585\n",
      "conv3.lin.weight, gradient norm: 0.018850170075893402\n",
      "Epoch: 436, Training Loss: 0.1648, Validation Loss: 0.2643, Train Acc: 0.9902, Val Acc: 0.9897, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.9844199616869673e-09\n",
      "conv1.lin.weight, gradient norm: 0.10539164394140244\n",
      "bn1.module.weight, gradient norm: 0.024928970262408257\n",
      "bn1.module.bias, gradient norm: 0.020900718867778778\n",
      "conv2.bias, gradient norm: 7.678752922402055e-08\n",
      "conv2.lin.weight, gradient norm: 0.0358491949737072\n",
      "bn2.module.weight, gradient norm: 0.017055070027709007\n",
      "bn2.module.bias, gradient norm: 0.008359803818166256\n",
      "conv3.bias, gradient norm: 0.01033771876245737\n",
      "conv3.lin.weight, gradient norm: 0.019264161586761475\n",
      "Epoch: 437, Training Loss: 0.1635, Validation Loss: 0.2640, Train Acc: 0.9901, Val Acc: 0.9897, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 2.4490183214709305e-09\n",
      "conv1.lin.weight, gradient norm: 0.08968937397003174\n",
      "bn1.module.weight, gradient norm: 0.04917372763156891\n",
      "bn1.module.bias, gradient norm: 0.02477172017097473\n",
      "conv2.bias, gradient norm: 4.4685897648832906e-08\n",
      "conv2.lin.weight, gradient norm: 0.039723485708236694\n",
      "bn2.module.weight, gradient norm: 0.017534835264086723\n",
      "bn2.module.bias, gradient norm: 0.008668973110616207\n",
      "conv3.bias, gradient norm: 0.009792756289243698\n",
      "conv3.lin.weight, gradient norm: 0.019798945635557175\n",
      "Epoch: 438, Training Loss: 0.1663, Validation Loss: 0.2639, Train Acc: 0.9902, Val Acc: 0.9899, Test Acc: 0.9907\n",
      "conv1.bias, gradient norm: 1.753403866544545e-09\n",
      "conv1.lin.weight, gradient norm: 0.09709969162940979\n",
      "bn1.module.weight, gradient norm: 0.053628019988536835\n",
      "bn1.module.bias, gradient norm: 0.029150158166885376\n",
      "conv2.bias, gradient norm: 7.141071733940407e-08\n",
      "conv2.lin.weight, gradient norm: 0.037507690489292145\n",
      "bn2.module.weight, gradient norm: 0.01611555926501751\n",
      "bn2.module.bias, gradient norm: 0.008208846673369408\n",
      "conv3.bias, gradient norm: 0.010142350569367409\n",
      "conv3.lin.weight, gradient norm: 0.01778809167444706\n",
      "Epoch: 439, Training Loss: 0.1677, Validation Loss: 0.2642, Train Acc: 0.9902, Val Acc: 0.9899, Test Acc: 0.9907\n",
      "conv1.bias, gradient norm: 1.6551694459465693e-09\n",
      "conv1.lin.weight, gradient norm: 0.11499655246734619\n",
      "bn1.module.weight, gradient norm: 0.025057610124349594\n",
      "bn1.module.bias, gradient norm: 0.0168902687728405\n",
      "conv2.bias, gradient norm: 5.3756153306494525e-08\n",
      "conv2.lin.weight, gradient norm: 0.04399355128407478\n",
      "bn2.module.weight, gradient norm: 0.01630813628435135\n",
      "bn2.module.bias, gradient norm: 0.00752472598105669\n",
      "conv3.bias, gradient norm: 0.01107785478234291\n",
      "conv3.lin.weight, gradient norm: 0.021075787022709846\n",
      "Epoch: 440, Training Loss: 0.1642, Validation Loss: 0.2646, Train Acc: 0.9902, Val Acc: 0.9898, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.3565905065959782e-09\n",
      "conv1.lin.weight, gradient norm: 0.0814211368560791\n",
      "bn1.module.weight, gradient norm: 0.019280176609754562\n",
      "bn1.module.bias, gradient norm: 0.013619605451822281\n",
      "conv2.bias, gradient norm: 6.419257658762945e-08\n",
      "conv2.lin.weight, gradient norm: 0.035401780158281326\n",
      "bn2.module.weight, gradient norm: 0.0172741562128067\n",
      "bn2.module.bias, gradient norm: 0.008584504015743732\n",
      "conv3.bias, gradient norm: 0.009775498881936073\n",
      "conv3.lin.weight, gradient norm: 0.01947670243680477\n",
      "Epoch: 441, Training Loss: 0.1645, Validation Loss: 0.2651, Train Acc: 0.9901, Val Acc: 0.9898, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.119240256031162e-09\n",
      "conv1.lin.weight, gradient norm: 0.10639995336532593\n",
      "bn1.module.weight, gradient norm: 0.03489525243639946\n",
      "bn1.module.bias, gradient norm: 0.017395446076989174\n",
      "conv2.bias, gradient norm: 4.8021426835020975e-08\n",
      "conv2.lin.weight, gradient norm: 0.039220333099365234\n",
      "bn2.module.weight, gradient norm: 0.01603538915514946\n",
      "bn2.module.bias, gradient norm: 0.009748461656272411\n",
      "conv3.bias, gradient norm: 0.00976056233048439\n",
      "conv3.lin.weight, gradient norm: 0.018933920189738274\n",
      "Epoch: 442, Training Loss: 0.1653, Validation Loss: 0.2655, Train Acc: 0.9901, Val Acc: 0.9897, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 2.8132634000144208e-09\n",
      "conv1.lin.weight, gradient norm: 0.09155073016881943\n",
      "bn1.module.weight, gradient norm: 0.035476818680763245\n",
      "bn1.module.bias, gradient norm: 0.022197389975190163\n",
      "conv2.bias, gradient norm: 6.563594467934308e-08\n",
      "conv2.lin.weight, gradient norm: 0.04005473479628563\n",
      "bn2.module.weight, gradient norm: 0.016181226819753647\n",
      "bn2.module.bias, gradient norm: 0.009131139144301414\n",
      "conv3.bias, gradient norm: 0.009702530689537525\n",
      "conv3.lin.weight, gradient norm: 0.01838979683816433\n",
      "Epoch: 443, Training Loss: 0.1648, Validation Loss: 0.2657, Train Acc: 0.9901, Val Acc: 0.9897, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 3.071350507255488e-09\n",
      "conv1.lin.weight, gradient norm: 0.12596650421619415\n",
      "bn1.module.weight, gradient norm: 0.052531320601701736\n",
      "bn1.module.bias, gradient norm: 0.031314462423324585\n",
      "conv2.bias, gradient norm: 6.324695789317047e-08\n",
      "conv2.lin.weight, gradient norm: 0.03733089193701744\n",
      "bn2.module.weight, gradient norm: 0.01590142585337162\n",
      "bn2.module.bias, gradient norm: 0.009642944671213627\n",
      "conv3.bias, gradient norm: 0.009655649773776531\n",
      "conv3.lin.weight, gradient norm: 0.019674045965075493\n",
      "Epoch: 444, Training Loss: 0.1674, Validation Loss: 0.2663, Train Acc: 0.9902, Val Acc: 0.9897, Test Acc: 0.9907\n",
      "conv1.bias, gradient norm: 2.1870001365442704e-09\n",
      "conv1.lin.weight, gradient norm: 0.08273136615753174\n",
      "bn1.module.weight, gradient norm: 0.025292837992310524\n",
      "bn1.module.bias, gradient norm: 0.0217884574085474\n",
      "conv2.bias, gradient norm: 4.210667370330157e-08\n",
      "conv2.lin.weight, gradient norm: 0.03570018336176872\n",
      "bn2.module.weight, gradient norm: 0.016284862533211708\n",
      "bn2.module.bias, gradient norm: 0.0077678547240793705\n",
      "conv3.bias, gradient norm: 0.010686459951102734\n",
      "conv3.lin.weight, gradient norm: 0.01942434348165989\n",
      "Epoch: 445, Training Loss: 0.1647, Validation Loss: 0.2665, Train Acc: 0.9902, Val Acc: 0.9897, Test Acc: 0.9907\n",
      "conv1.bias, gradient norm: 2.824813272184201e-09\n",
      "conv1.lin.weight, gradient norm: 0.08338186144828796\n",
      "bn1.module.weight, gradient norm: 0.05286930873990059\n",
      "bn1.module.bias, gradient norm: 0.03248609974980354\n",
      "conv2.bias, gradient norm: 5.11528455149346e-08\n",
      "conv2.lin.weight, gradient norm: 0.029681604355573654\n",
      "bn2.module.weight, gradient norm: 0.016419222578406334\n",
      "bn2.module.bias, gradient norm: 0.008218636736273766\n",
      "conv3.bias, gradient norm: 0.010320385918021202\n",
      "conv3.lin.weight, gradient norm: 0.018206363543868065\n",
      "Epoch: 446, Training Loss: 0.1647, Validation Loss: 0.2666, Train Acc: 0.9902, Val Acc: 0.9898, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 2.573070201350447e-09\n",
      "conv1.lin.weight, gradient norm: 0.16226118803024292\n",
      "bn1.module.weight, gradient norm: 0.035655226558446884\n",
      "bn1.module.bias, gradient norm: 0.03444063663482666\n",
      "conv2.bias, gradient norm: 6.953192155378929e-08\n",
      "conv2.lin.weight, gradient norm: 0.05026780441403389\n",
      "bn2.module.weight, gradient norm: 0.016331562772393227\n",
      "bn2.module.bias, gradient norm: 0.006404780317097902\n",
      "conv3.bias, gradient norm: 0.011396272107958794\n",
      "conv3.lin.weight, gradient norm: 0.02172716334462166\n",
      "Epoch: 447, Training Loss: 0.1636, Validation Loss: 0.2668, Train Acc: 0.9902, Val Acc: 0.9896, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 1.5809477060813037e-09\n",
      "conv1.lin.weight, gradient norm: 0.10320296883583069\n",
      "bn1.module.weight, gradient norm: 0.028253896161913872\n",
      "bn1.module.bias, gradient norm: 0.019012534990906715\n",
      "conv2.bias, gradient norm: 6.720058820519625e-08\n",
      "conv2.lin.weight, gradient norm: 0.03468421474099159\n",
      "bn2.module.weight, gradient norm: 0.016508331522345543\n",
      "bn2.module.bias, gradient norm: 0.008173293434083462\n",
      "conv3.bias, gradient norm: 0.010737644508481026\n",
      "conv3.lin.weight, gradient norm: 0.019159164279699326\n",
      "Epoch: 448, Training Loss: 0.1634, Validation Loss: 0.2671, Train Acc: 0.9900, Val Acc: 0.9895, Test Acc: 0.9904\n",
      "conv1.bias, gradient norm: 1.4572681950042465e-09\n",
      "conv1.lin.weight, gradient norm: 0.1182946190237999\n",
      "bn1.module.weight, gradient norm: 0.04932595416903496\n",
      "bn1.module.bias, gradient norm: 0.026363538578152657\n",
      "conv2.bias, gradient norm: 5.88602375728442e-08\n",
      "conv2.lin.weight, gradient norm: 0.051503803580999374\n",
      "bn2.module.weight, gradient norm: 0.017001699656248093\n",
      "bn2.module.bias, gradient norm: 0.010297410190105438\n",
      "conv3.bias, gradient norm: 0.009544739499688148\n",
      "conv3.lin.weight, gradient norm: 0.01964006945490837\n",
      "Epoch: 449, Training Loss: 0.1648, Validation Loss: 0.2672, Train Acc: 0.9900, Val Acc: 0.9895, Test Acc: 0.9904\n",
      "conv1.bias, gradient norm: 1.756088940929601e-09\n",
      "conv1.lin.weight, gradient norm: 0.13640166819095612\n",
      "bn1.module.weight, gradient norm: 0.052427150309085846\n",
      "bn1.module.bias, gradient norm: 0.029681025072932243\n",
      "conv2.bias, gradient norm: 6.48646505396755e-08\n",
      "conv2.lin.weight, gradient norm: 0.0407017283141613\n",
      "bn2.module.weight, gradient norm: 0.016392888501286507\n",
      "bn2.module.bias, gradient norm: 0.009869947098195553\n",
      "conv3.bias, gradient norm: 0.01002667285501957\n",
      "conv3.lin.weight, gradient norm: 0.020626820623874664\n",
      "Epoch: 450, Training Loss: 0.1646, Validation Loss: 0.2675, Train Acc: 0.9900, Val Acc: 0.9894, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 1.1771152941264518e-09\n",
      "conv1.lin.weight, gradient norm: 0.12126332521438599\n",
      "bn1.module.weight, gradient norm: 0.03005031682550907\n",
      "bn1.module.bias, gradient norm: 0.02698075957596302\n",
      "conv2.bias, gradient norm: 6.941048269482053e-08\n",
      "conv2.lin.weight, gradient norm: 0.029020993039011955\n",
      "bn2.module.weight, gradient norm: 0.016699841246008873\n",
      "bn2.module.bias, gradient norm: 0.007725840900093317\n",
      "conv3.bias, gradient norm: 0.010563699528574944\n",
      "conv3.lin.weight, gradient norm: 0.018307575955986977\n",
      "Epoch: 451, Training Loss: 0.1629, Validation Loss: 0.2675, Train Acc: 0.9900, Val Acc: 0.9895, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 2.335400983710656e-09\n",
      "conv1.lin.weight, gradient norm: 0.11044412106275558\n",
      "bn1.module.weight, gradient norm: 0.025351326912641525\n",
      "bn1.module.bias, gradient norm: 0.021700678393244743\n",
      "conv2.bias, gradient norm: 8.232769488358826e-08\n",
      "conv2.lin.weight, gradient norm: 0.03875947743654251\n",
      "bn2.module.weight, gradient norm: 0.016705544665455818\n",
      "bn2.module.bias, gradient norm: 0.008611327968537807\n",
      "conv3.bias, gradient norm: 0.010317797772586346\n",
      "conv3.lin.weight, gradient norm: 0.01964116096496582\n",
      "Epoch: 452, Training Loss: 0.1645, Validation Loss: 0.2675, Train Acc: 0.9901, Val Acc: 0.9895, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 2.2628907636601525e-09\n",
      "conv1.lin.weight, gradient norm: 0.09787961840629578\n",
      "bn1.module.weight, gradient norm: 0.017888281494379044\n",
      "bn1.module.bias, gradient norm: 0.01553667988628149\n",
      "conv2.bias, gradient norm: 5.684811910100507e-08\n",
      "conv2.lin.weight, gradient norm: 0.031687796115875244\n",
      "bn2.module.weight, gradient norm: 0.016954369843006134\n",
      "bn2.module.bias, gradient norm: 0.00866814237087965\n",
      "conv3.bias, gradient norm: 0.010193463414907455\n",
      "conv3.lin.weight, gradient norm: 0.02005251683294773\n",
      "Epoch: 453, Training Loss: 0.1647, Validation Loss: 0.2674, Train Acc: 0.9901, Val Acc: 0.9896, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 2.868785653475925e-09\n",
      "conv1.lin.weight, gradient norm: 0.1091599315404892\n",
      "bn1.module.weight, gradient norm: 0.025614416226744652\n",
      "bn1.module.bias, gradient norm: 0.02015640027821064\n",
      "conv2.bias, gradient norm: 4.8641549454941924e-08\n",
      "conv2.lin.weight, gradient norm: 0.02619555965065956\n",
      "bn2.module.weight, gradient norm: 0.016408488154411316\n",
      "bn2.module.bias, gradient norm: 0.007375279441475868\n",
      "conv3.bias, gradient norm: 0.010997246950864792\n",
      "conv3.lin.weight, gradient norm: 0.018407359719276428\n",
      "Epoch: 454, Training Loss: 0.1626, Validation Loss: 0.2668, Train Acc: 0.9901, Val Acc: 0.9896, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.7297967502827305e-09\n",
      "conv1.lin.weight, gradient norm: 0.09433841705322266\n",
      "bn1.module.weight, gradient norm: 0.022114528343081474\n",
      "bn1.module.bias, gradient norm: 0.014252142049372196\n",
      "conv2.bias, gradient norm: 6.379474370987737e-08\n",
      "conv2.lin.weight, gradient norm: 0.03163586184382439\n",
      "bn2.module.weight, gradient norm: 0.017050735652446747\n",
      "bn2.module.bias, gradient norm: 0.008200196549296379\n",
      "conv3.bias, gradient norm: 0.010256074368953705\n",
      "conv3.lin.weight, gradient norm: 0.01899733580648899\n",
      "Epoch: 455, Training Loss: 0.1623, Validation Loss: 0.2662, Train Acc: 0.9901, Val Acc: 0.9896, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 2.458207859490358e-09\n",
      "conv1.lin.weight, gradient norm: 0.09743888676166534\n",
      "bn1.module.weight, gradient norm: 0.031212972477078438\n",
      "bn1.module.bias, gradient norm: 0.019738230854272842\n",
      "conv2.bias, gradient norm: 6.465914736963896e-08\n",
      "conv2.lin.weight, gradient norm: 0.03936329111456871\n",
      "bn2.module.weight, gradient norm: 0.015734950080513954\n",
      "bn2.module.bias, gradient norm: 0.007121475413441658\n",
      "conv3.bias, gradient norm: 0.011120193637907505\n",
      "conv3.lin.weight, gradient norm: 0.019817175343632698\n",
      "Epoch: 456, Training Loss: 0.1642, Validation Loss: 0.2655, Train Acc: 0.9901, Val Acc: 0.9896, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 2.150454703198079e-09\n",
      "conv1.lin.weight, gradient norm: 0.10357543081045151\n",
      "bn1.module.weight, gradient norm: 0.03933074697852135\n",
      "bn1.module.bias, gradient norm: 0.026022400707006454\n",
      "conv2.bias, gradient norm: 3.2960958407102225e-08\n",
      "conv2.lin.weight, gradient norm: 0.030866488814353943\n",
      "bn2.module.weight, gradient norm: 0.016012534499168396\n",
      "bn2.module.bias, gradient norm: 0.0074702175334095955\n",
      "conv3.bias, gradient norm: 0.01071961596608162\n",
      "conv3.lin.weight, gradient norm: 0.0176384337246418\n",
      "Epoch: 457, Training Loss: 0.1652, Validation Loss: 0.2648, Train Acc: 0.9901, Val Acc: 0.9896, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.7308905420065912e-09\n",
      "conv1.lin.weight, gradient norm: 0.0995447039604187\n",
      "bn1.module.weight, gradient norm: 0.031508397310972214\n",
      "bn1.module.bias, gradient norm: 0.016742106527090073\n",
      "conv2.bias, gradient norm: 5.18001073146479e-08\n",
      "conv2.lin.weight, gradient norm: 0.027782319113612175\n",
      "bn2.module.weight, gradient norm: 0.017164068296551704\n",
      "bn2.module.bias, gradient norm: 0.008358566090464592\n",
      "conv3.bias, gradient norm: 0.01039584819227457\n",
      "conv3.lin.weight, gradient norm: 0.018634282052516937\n",
      "Epoch: 458, Training Loss: 0.1627, Validation Loss: 0.2642, Train Acc: 0.9901, Val Acc: 0.9896, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 2.465597059853053e-09\n",
      "conv1.lin.weight, gradient norm: 0.13089865446090698\n",
      "bn1.module.weight, gradient norm: 0.05423438921570778\n",
      "bn1.module.bias, gradient norm: 0.024329787120223045\n",
      "conv2.bias, gradient norm: 7.837267901322775e-08\n",
      "conv2.lin.weight, gradient norm: 0.0386556014418602\n",
      "bn2.module.weight, gradient norm: 0.016906702890992165\n",
      "bn2.module.bias, gradient norm: 0.009661592543125153\n",
      "conv3.bias, gradient norm: 0.010388716123998165\n",
      "conv3.lin.weight, gradient norm: 0.019861329346895218\n",
      "Epoch: 459, Training Loss: 0.1633, Validation Loss: 0.2638, Train Acc: 0.9901, Val Acc: 0.9896, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 2.297178447463466e-09\n",
      "conv1.lin.weight, gradient norm: 0.09607435762882233\n",
      "bn1.module.weight, gradient norm: 0.02595381811261177\n",
      "bn1.module.bias, gradient norm: 0.016127655282616615\n",
      "conv2.bias, gradient norm: 4.7048644091773895e-08\n",
      "conv2.lin.weight, gradient norm: 0.03430687636137009\n",
      "bn2.module.weight, gradient norm: 0.016121016815304756\n",
      "bn2.module.bias, gradient norm: 0.008673899807035923\n",
      "conv3.bias, gradient norm: 0.010471895337104797\n",
      "conv3.lin.weight, gradient norm: 0.018036480993032455\n",
      "Epoch: 460, Training Loss: 0.1656, Validation Loss: 0.2635, Train Acc: 0.9901, Val Acc: 0.9896, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.6524756007996189e-09\n",
      "conv1.lin.weight, gradient norm: 0.1110977977514267\n",
      "bn1.module.weight, gradient norm: 0.027467824518680573\n",
      "bn1.module.bias, gradient norm: 0.022346775978803635\n",
      "conv2.bias, gradient norm: 4.7342549436280024e-08\n",
      "conv2.lin.weight, gradient norm: 0.028391346335411072\n",
      "bn2.module.weight, gradient norm: 0.016794614493846893\n",
      "bn2.module.bias, gradient norm: 0.008986417204141617\n",
      "conv3.bias, gradient norm: 0.010468743741512299\n",
      "conv3.lin.weight, gradient norm: 0.01876831240952015\n",
      "Epoch: 461, Training Loss: 0.1635, Validation Loss: 0.2633, Train Acc: 0.9902, Val Acc: 0.9897, Test Acc: 0.9907\n",
      "conv1.bias, gradient norm: 2.8737361379427284e-09\n",
      "conv1.lin.weight, gradient norm: 0.10648200660943985\n",
      "bn1.module.weight, gradient norm: 0.03961166739463806\n",
      "bn1.module.bias, gradient norm: 0.022366847842931747\n",
      "conv2.bias, gradient norm: 5.828549021202889e-08\n",
      "conv2.lin.weight, gradient norm: 0.0339481420814991\n",
      "bn2.module.weight, gradient norm: 0.01592014543712139\n",
      "bn2.module.bias, gradient norm: 0.009019371122121811\n",
      "conv3.bias, gradient norm: 0.01045200414955616\n",
      "conv3.lin.weight, gradient norm: 0.019563650712370872\n",
      "Epoch: 462, Training Loss: 0.1636, Validation Loss: 0.2635, Train Acc: 0.9902, Val Acc: 0.9897, Test Acc: 0.9907\n",
      "conv1.bias, gradient norm: 2.64790633863754e-09\n",
      "conv1.lin.weight, gradient norm: 0.14572186768054962\n",
      "bn1.module.weight, gradient norm: 0.04446665942668915\n",
      "bn1.module.bias, gradient norm: 0.025210609659552574\n",
      "conv2.bias, gradient norm: 4.1255066918211014e-08\n",
      "conv2.lin.weight, gradient norm: 0.04254870489239693\n",
      "bn2.module.weight, gradient norm: 0.016721148043870926\n",
      "bn2.module.bias, gradient norm: 0.007673327345401049\n",
      "conv3.bias, gradient norm: 0.01099785789847374\n",
      "conv3.lin.weight, gradient norm: 0.021287642419338226\n",
      "Epoch: 463, Training Loss: 0.1646, Validation Loss: 0.2639, Train Acc: 0.9902, Val Acc: 0.9897, Test Acc: 0.9907\n",
      "conv1.bias, gradient norm: 1.7719944400695908e-09\n",
      "conv1.lin.weight, gradient norm: 0.09678773581981659\n",
      "bn1.module.weight, gradient norm: 0.02776642143726349\n",
      "bn1.module.bias, gradient norm: 0.016665024682879448\n",
      "conv2.bias, gradient norm: 4.568526534853845e-08\n",
      "conv2.lin.weight, gradient norm: 0.03351226821541786\n",
      "bn2.module.weight, gradient norm: 0.017032768577337265\n",
      "bn2.module.bias, gradient norm: 0.009046973660588264\n",
      "conv3.bias, gradient norm: 0.009807845577597618\n",
      "conv3.lin.weight, gradient norm: 0.019086118787527084\n",
      "Epoch: 464, Training Loss: 0.1641, Validation Loss: 0.2643, Train Acc: 0.9902, Val Acc: 0.9897, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 2.2461388304861885e-09\n",
      "conv1.lin.weight, gradient norm: 0.10574007779359818\n",
      "bn1.module.weight, gradient norm: 0.028170054778456688\n",
      "bn1.module.bias, gradient norm: 0.0188802108168602\n",
      "conv2.bias, gradient norm: 6.387319473333264e-08\n",
      "conv2.lin.weight, gradient norm: 0.0352921299636364\n",
      "bn2.module.weight, gradient norm: 0.01629921980202198\n",
      "bn2.module.bias, gradient norm: 0.008272609673440456\n",
      "conv3.bias, gradient norm: 0.010174412280321121\n",
      "conv3.lin.weight, gradient norm: 0.01839161105453968\n",
      "Epoch: 465, Training Loss: 0.1648, Validation Loss: 0.2649, Train Acc: 0.9901, Val Acc: 0.9897, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.922578096724692e-09\n",
      "conv1.lin.weight, gradient norm: 0.14608962833881378\n",
      "bn1.module.weight, gradient norm: 0.04343539476394653\n",
      "bn1.module.bias, gradient norm: 0.027828216552734375\n",
      "conv2.bias, gradient norm: 7.970618298713816e-08\n",
      "conv2.lin.weight, gradient norm: 0.03683729097247124\n",
      "bn2.module.weight, gradient norm: 0.01805463619530201\n",
      "bn2.module.bias, gradient norm: 0.008307669311761856\n",
      "conv3.bias, gradient norm: 0.010245352983474731\n",
      "conv3.lin.weight, gradient norm: 0.019994474947452545\n",
      "Epoch: 466, Training Loss: 0.1634, Validation Loss: 0.2656, Train Acc: 0.9901, Val Acc: 0.9896, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 2.150707167913879e-09\n",
      "conv1.lin.weight, gradient norm: 0.10545907914638519\n",
      "bn1.module.weight, gradient norm: 0.035572588443756104\n",
      "bn1.module.bias, gradient norm: 0.020619399845600128\n",
      "conv2.bias, gradient norm: 6.095628890534499e-08\n",
      "conv2.lin.weight, gradient norm: 0.03507828339934349\n",
      "bn2.module.weight, gradient norm: 0.016667017713189125\n",
      "bn2.module.bias, gradient norm: 0.008186249993741512\n",
      "conv3.bias, gradient norm: 0.010654340498149395\n",
      "conv3.lin.weight, gradient norm: 0.019108355045318604\n",
      "Epoch: 467, Training Loss: 0.1642, Validation Loss: 0.2664, Train Acc: 0.9901, Val Acc: 0.9896, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.282721706630241e-09\n",
      "conv1.lin.weight, gradient norm: 0.12051394581794739\n",
      "bn1.module.weight, gradient norm: 0.037723660469055176\n",
      "bn1.module.bias, gradient norm: 0.031485747545957565\n",
      "conv2.bias, gradient norm: 6.458648016405277e-08\n",
      "conv2.lin.weight, gradient norm: 0.03216267377138138\n",
      "bn2.module.weight, gradient norm: 0.016337525099515915\n",
      "bn2.module.bias, gradient norm: 0.009017891250550747\n",
      "conv3.bias, gradient norm: 0.010046667419373989\n",
      "conv3.lin.weight, gradient norm: 0.01852707751095295\n",
      "Epoch: 468, Training Loss: 0.1649, Validation Loss: 0.2670, Train Acc: 0.9901, Val Acc: 0.9896, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.0118356152943875e-09\n",
      "conv1.lin.weight, gradient norm: 0.09696611016988754\n",
      "bn1.module.weight, gradient norm: 0.046129073947668076\n",
      "bn1.module.bias, gradient norm: 0.02538607083261013\n",
      "conv2.bias, gradient norm: 3.881084964518777e-08\n",
      "conv2.lin.weight, gradient norm: 0.03100050427019596\n",
      "bn2.module.weight, gradient norm: 0.015441711992025375\n",
      "bn2.module.bias, gradient norm: 0.007808829192072153\n",
      "conv3.bias, gradient norm: 0.011009987443685532\n",
      "conv3.lin.weight, gradient norm: 0.01800348050892353\n",
      "Epoch: 469, Training Loss: 0.1650, Validation Loss: 0.2675, Train Acc: 0.9901, Val Acc: 0.9896, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.97866589779494e-09\n",
      "conv1.lin.weight, gradient norm: 0.0867617055773735\n",
      "bn1.module.weight, gradient norm: 0.03282196447253227\n",
      "bn1.module.bias, gradient norm: 0.023500744253396988\n",
      "conv2.bias, gradient norm: 5.7843305256710664e-08\n",
      "conv2.lin.weight, gradient norm: 0.02900541014969349\n",
      "bn2.module.weight, gradient norm: 0.01699795387685299\n",
      "bn2.module.bias, gradient norm: 0.008498944342136383\n",
      "conv3.bias, gradient norm: 0.010385994799435139\n",
      "conv3.lin.weight, gradient norm: 0.01887834258377552\n",
      "Epoch: 470, Training Loss: 0.1638, Validation Loss: 0.2676, Train Acc: 0.9901, Val Acc: 0.9896, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 1.869022048239799e-09\n",
      "conv1.lin.weight, gradient norm: 0.10082750022411346\n",
      "bn1.module.weight, gradient norm: 0.04193591699004173\n",
      "bn1.module.bias, gradient norm: 0.030158551409840584\n",
      "conv2.bias, gradient norm: 4.072956016898388e-08\n",
      "conv2.lin.weight, gradient norm: 0.03208459913730621\n",
      "bn2.module.weight, gradient norm: 0.01597767509520054\n",
      "bn2.module.bias, gradient norm: 0.008695767261087894\n",
      "conv3.bias, gradient norm: 0.010029063560068607\n",
      "conv3.lin.weight, gradient norm: 0.01820245198905468\n",
      "Epoch: 471, Training Loss: 0.1645, Validation Loss: 0.2678, Train Acc: 0.9901, Val Acc: 0.9897, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 2.398609089127035e-09\n",
      "conv1.lin.weight, gradient norm: 0.08228336274623871\n",
      "bn1.module.weight, gradient norm: 0.019445549696683884\n",
      "bn1.module.bias, gradient norm: 0.014296689070761204\n",
      "conv2.bias, gradient norm: 5.209686548823811e-08\n",
      "conv2.lin.weight, gradient norm: 0.03171548619866371\n",
      "bn2.module.weight, gradient norm: 0.016018634662032127\n",
      "bn2.module.bias, gradient norm: 0.009503188543021679\n",
      "conv3.bias, gradient norm: 0.010283732786774635\n",
      "conv3.lin.weight, gradient norm: 0.01848934404551983\n",
      "Epoch: 472, Training Loss: 0.1652, Validation Loss: 0.2680, Train Acc: 0.9901, Val Acc: 0.9896, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 2.0952310997301993e-09\n",
      "conv1.lin.weight, gradient norm: 0.09471515566110611\n",
      "bn1.module.weight, gradient norm: 0.03563011810183525\n",
      "bn1.module.bias, gradient norm: 0.019208870828151703\n",
      "conv2.bias, gradient norm: 6.787841755340196e-08\n",
      "conv2.lin.weight, gradient norm: 0.035252202302217484\n",
      "bn2.module.weight, gradient norm: 0.017047226428985596\n",
      "bn2.module.bias, gradient norm: 0.009615066461265087\n",
      "conv3.bias, gradient norm: 0.010018939152359962\n",
      "conv3.lin.weight, gradient norm: 0.01936459168791771\n",
      "Epoch: 473, Training Loss: 0.1629, Validation Loss: 0.2683, Train Acc: 0.9901, Val Acc: 0.9896, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.7555160658488944e-09\n",
      "conv1.lin.weight, gradient norm: 0.11920934915542603\n",
      "bn1.module.weight, gradient norm: 0.029024969786405563\n",
      "bn1.module.bias, gradient norm: 0.021294234320521355\n",
      "conv2.bias, gradient norm: 5.463343200062809e-08\n",
      "conv2.lin.weight, gradient norm: 0.03789208084344864\n",
      "bn2.module.weight, gradient norm: 0.01703708805143833\n",
      "bn2.module.bias, gradient norm: 0.008709250018000603\n",
      "conv3.bias, gradient norm: 0.010473094880580902\n",
      "conv3.lin.weight, gradient norm: 0.02000327967107296\n",
      "Epoch: 474, Training Loss: 0.1615, Validation Loss: 0.2684, Train Acc: 0.9901, Val Acc: 0.9897, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 2.6186386392623717e-09\n",
      "conv1.lin.weight, gradient norm: 0.12826184928417206\n",
      "bn1.module.weight, gradient norm: 0.029146356508135796\n",
      "bn1.module.bias, gradient norm: 0.02489793673157692\n",
      "conv2.bias, gradient norm: 4.750429738464845e-08\n",
      "conv2.lin.weight, gradient norm: 0.0327625647187233\n",
      "bn2.module.weight, gradient norm: 0.015918690711259842\n",
      "bn2.module.bias, gradient norm: 0.007579000201076269\n",
      "conv3.bias, gradient norm: 0.010728750377893448\n",
      "conv3.lin.weight, gradient norm: 0.018047450110316277\n",
      "Epoch: 475, Training Loss: 0.1647, Validation Loss: 0.2684, Train Acc: 0.9901, Val Acc: 0.9897, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 2.411505217736476e-09\n",
      "conv1.lin.weight, gradient norm: 0.09599336236715317\n",
      "bn1.module.weight, gradient norm: 0.027623025700449944\n",
      "bn1.module.bias, gradient norm: 0.017611224204301834\n",
      "conv2.bias, gradient norm: 5.010472747812855e-08\n",
      "conv2.lin.weight, gradient norm: 0.03266456723213196\n",
      "bn2.module.weight, gradient norm: 0.016029339283704758\n",
      "bn2.module.bias, gradient norm: 0.008328655734658241\n",
      "conv3.bias, gradient norm: 0.010502312332391739\n",
      "conv3.lin.weight, gradient norm: 0.018910782411694527\n",
      "Epoch: 476, Training Loss: 0.1640, Validation Loss: 0.2684, Train Acc: 0.9900, Val Acc: 0.9896, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 2.5540061177053985e-09\n",
      "conv1.lin.weight, gradient norm: 0.08582700788974762\n",
      "bn1.module.weight, gradient norm: 0.03174566477537155\n",
      "bn1.module.bias, gradient norm: 0.017444200813770294\n",
      "conv2.bias, gradient norm: 4.957830412877229e-08\n",
      "conv2.lin.weight, gradient norm: 0.03379293531179428\n",
      "bn2.module.weight, gradient norm: 0.016357267275452614\n",
      "bn2.module.bias, gradient norm: 0.009629687294363976\n",
      "conv3.bias, gradient norm: 0.010322125628590584\n",
      "conv3.lin.weight, gradient norm: 0.01844811998307705\n",
      "Epoch: 477, Training Loss: 0.1655, Validation Loss: 0.2684, Train Acc: 0.9900, Val Acc: 0.9897, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 2.1982604625492286e-09\n",
      "conv1.lin.weight, gradient norm: 0.09922494739294052\n",
      "bn1.module.weight, gradient norm: 0.016961058601737022\n",
      "bn1.module.bias, gradient norm: 0.013487551361322403\n",
      "conv2.bias, gradient norm: 6.398184382305772e-08\n",
      "conv2.lin.weight, gradient norm: 0.03260665386915207\n",
      "bn2.module.weight, gradient norm: 0.016463862732052803\n",
      "bn2.module.bias, gradient norm: 0.00851009227335453\n",
      "conv3.bias, gradient norm: 0.01044104341417551\n",
      "conv3.lin.weight, gradient norm: 0.0181417278945446\n",
      "Epoch: 478, Training Loss: 0.1671, Validation Loss: 0.2685, Train Acc: 0.9900, Val Acc: 0.9897, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 2.165761792127796e-09\n",
      "conv1.lin.weight, gradient norm: 0.10494576394557953\n",
      "bn1.module.weight, gradient norm: 0.017416581511497498\n",
      "bn1.module.bias, gradient norm: 0.013073239475488663\n",
      "conv2.bias, gradient norm: 4.147196719372914e-08\n",
      "conv2.lin.weight, gradient norm: 0.03226814419031143\n",
      "bn2.module.weight, gradient norm: 0.016105985268950462\n",
      "bn2.module.bias, gradient norm: 0.008047881536185741\n",
      "conv3.bias, gradient norm: 0.010569022037088871\n",
      "conv3.lin.weight, gradient norm: 0.018661120906472206\n",
      "Epoch: 479, Training Loss: 0.1629, Validation Loss: 0.2684, Train Acc: 0.9900, Val Acc: 0.9896, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 1.600992782790911e-09\n",
      "conv1.lin.weight, gradient norm: 0.0834340825676918\n",
      "bn1.module.weight, gradient norm: 0.029287222772836685\n",
      "bn1.module.bias, gradient norm: 0.015573479235172272\n",
      "conv2.bias, gradient norm: 5.5279016919485e-08\n",
      "conv2.lin.weight, gradient norm: 0.046008944511413574\n",
      "bn2.module.weight, gradient norm: 0.01652558520436287\n",
      "bn2.module.bias, gradient norm: 0.009121455252170563\n",
      "conv3.bias, gradient norm: 0.010243604891002178\n",
      "conv3.lin.weight, gradient norm: 0.018567247316241264\n",
      "Epoch: 480, Training Loss: 0.1633, Validation Loss: 0.2684, Train Acc: 0.9900, Val Acc: 0.9896, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 1.5256843566291423e-09\n",
      "conv1.lin.weight, gradient norm: 0.10624397546052933\n",
      "bn1.module.weight, gradient norm: 0.04429909586906433\n",
      "bn1.module.bias, gradient norm: 0.020764386281371117\n",
      "conv2.bias, gradient norm: 5.2056655874821445e-08\n",
      "conv2.lin.weight, gradient norm: 0.04396591708064079\n",
      "bn2.module.weight, gradient norm: 0.01700652576982975\n",
      "bn2.module.bias, gradient norm: 0.009928473271429539\n",
      "conv3.bias, gradient norm: 0.009752437472343445\n",
      "conv3.lin.weight, gradient norm: 0.019688211381435394\n",
      "Epoch: 481, Training Loss: 0.1623, Validation Loss: 0.2685, Train Acc: 0.9900, Val Acc: 0.9897, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 2.03981320723301e-09\n",
      "conv1.lin.weight, gradient norm: 0.09910263866186142\n",
      "bn1.module.weight, gradient norm: 0.035352788865566254\n",
      "bn1.module.bias, gradient norm: 0.01707121916115284\n",
      "conv2.bias, gradient norm: 7.028709347878248e-08\n",
      "conv2.lin.weight, gradient norm: 0.039766017347574234\n",
      "bn2.module.weight, gradient norm: 0.016401205211877823\n",
      "bn2.module.bias, gradient norm: 0.009176078252494335\n",
      "conv3.bias, gradient norm: 0.010227358900010586\n",
      "conv3.lin.weight, gradient norm: 0.019011054188013077\n",
      "Epoch: 482, Training Loss: 0.1635, Validation Loss: 0.2685, Train Acc: 0.9901, Val Acc: 0.9897, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.4241188228680812e-09\n",
      "conv1.lin.weight, gradient norm: 0.08596658706665039\n",
      "bn1.module.weight, gradient norm: 0.021540125831961632\n",
      "bn1.module.bias, gradient norm: 0.01238048356026411\n",
      "conv2.bias, gradient norm: 6.875061586697484e-08\n",
      "conv2.lin.weight, gradient norm: 0.03693101927638054\n",
      "bn2.module.weight, gradient norm: 0.015936290845274925\n",
      "bn2.module.bias, gradient norm: 0.009120315313339233\n",
      "conv3.bias, gradient norm: 0.01002136617898941\n",
      "conv3.lin.weight, gradient norm: 0.019362058490514755\n",
      "Epoch: 483, Training Loss: 0.1658, Validation Loss: 0.2685, Train Acc: 0.9901, Val Acc: 0.9897, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.4407268711380539e-09\n",
      "conv1.lin.weight, gradient norm: 0.09967076778411865\n",
      "bn1.module.weight, gradient norm: 0.02306104078888893\n",
      "bn1.module.bias, gradient norm: 0.011511034332215786\n",
      "conv2.bias, gradient norm: 6.501647220602536e-08\n",
      "conv2.lin.weight, gradient norm: 0.03393298387527466\n",
      "bn2.module.weight, gradient norm: 0.01631072163581848\n",
      "bn2.module.bias, gradient norm: 0.009269685484468937\n",
      "conv3.bias, gradient norm: 0.009842085652053356\n",
      "conv3.lin.weight, gradient norm: 0.01872093789279461\n",
      "Epoch: 484, Training Loss: 0.1649, Validation Loss: 0.2686, Train Acc: 0.9902, Val Acc: 0.9898, Test Acc: 0.9907\n",
      "conv1.bias, gradient norm: 4.1032484077163645e-09\n",
      "conv1.lin.weight, gradient norm: 0.1000443547964096\n",
      "bn1.module.weight, gradient norm: 0.020400142297148705\n",
      "bn1.module.bias, gradient norm: 0.018827296793460846\n",
      "conv2.bias, gradient norm: 5.799259383820754e-08\n",
      "conv2.lin.weight, gradient norm: 0.028376422822475433\n",
      "bn2.module.weight, gradient norm: 0.016670992597937584\n",
      "bn2.module.bias, gradient norm: 0.008528143167495728\n",
      "conv3.bias, gradient norm: 0.010047723539173603\n",
      "conv3.lin.weight, gradient norm: 0.018801461905241013\n",
      "Epoch: 485, Training Loss: 0.1654, Validation Loss: 0.2685, Train Acc: 0.9902, Val Acc: 0.9898, Test Acc: 0.9907\n",
      "conv1.bias, gradient norm: 2.5407369541596836e-09\n",
      "conv1.lin.weight, gradient norm: 0.18666985630989075\n",
      "bn1.module.weight, gradient norm: 0.03870580717921257\n",
      "bn1.module.bias, gradient norm: 0.029648778960108757\n",
      "conv2.bias, gradient norm: 6.613127823129616e-08\n",
      "conv2.lin.weight, gradient norm: 0.05827352777123451\n",
      "bn2.module.weight, gradient norm: 0.01617896743118763\n",
      "bn2.module.bias, gradient norm: 0.006869842763990164\n",
      "conv3.bias, gradient norm: 0.011470437049865723\n",
      "conv3.lin.weight, gradient norm: 0.023813538253307343\n",
      "Epoch: 486, Training Loss: 0.1643, Validation Loss: 0.2684, Train Acc: 0.9902, Val Acc: 0.9898, Test Acc: 0.9907\n",
      "conv1.bias, gradient norm: 2.1681896278380464e-09\n",
      "conv1.lin.weight, gradient norm: 0.09567777067422867\n",
      "bn1.module.weight, gradient norm: 0.033298857510089874\n",
      "bn1.module.bias, gradient norm: 0.02142733708024025\n",
      "conv2.bias, gradient norm: 4.724167723679784e-08\n",
      "conv2.lin.weight, gradient norm: 0.03185275197029114\n",
      "bn2.module.weight, gradient norm: 0.01663142815232277\n",
      "bn2.module.bias, gradient norm: 0.007955433800816536\n",
      "conv3.bias, gradient norm: 0.01041080616414547\n",
      "conv3.lin.weight, gradient norm: 0.018831226974725723\n",
      "Epoch: 487, Training Loss: 0.1651, Validation Loss: 0.2684, Train Acc: 0.9901, Val Acc: 0.9897, Test Acc: 0.9907\n",
      "conv1.bias, gradient norm: 2.088618167306322e-09\n",
      "conv1.lin.weight, gradient norm: 0.09814435243606567\n",
      "bn1.module.weight, gradient norm: 0.029635082930326462\n",
      "bn1.module.bias, gradient norm: 0.014945777133107185\n",
      "conv2.bias, gradient norm: 5.537895475526966e-08\n",
      "conv2.lin.weight, gradient norm: 0.045910779386758804\n",
      "bn2.module.weight, gradient norm: 0.016519170254468918\n",
      "bn2.module.bias, gradient norm: 0.009450973942875862\n",
      "conv3.bias, gradient norm: 0.009888432919979095\n",
      "conv3.lin.weight, gradient norm: 0.019130535423755646\n",
      "Epoch: 488, Training Loss: 0.1660, Validation Loss: 0.2684, Train Acc: 0.9901, Val Acc: 0.9897, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.578884134545433e-09\n",
      "conv1.lin.weight, gradient norm: 0.0996597483754158\n",
      "bn1.module.weight, gradient norm: 0.029961176216602325\n",
      "bn1.module.bias, gradient norm: 0.019295519217848778\n",
      "conv2.bias, gradient norm: 6.437588240260084e-08\n",
      "conv2.lin.weight, gradient norm: 0.03235172852873802\n",
      "bn2.module.weight, gradient norm: 0.017080938443541527\n",
      "bn2.module.bias, gradient norm: 0.008785403333604336\n",
      "conv3.bias, gradient norm: 0.010209784843027592\n",
      "conv3.lin.weight, gradient norm: 0.01944255828857422\n",
      "Epoch: 489, Training Loss: 0.1643, Validation Loss: 0.2685, Train Acc: 0.9901, Val Acc: 0.9897, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 8.726677158676921e-10\n",
      "conv1.lin.weight, gradient norm: 0.10891041904687881\n",
      "bn1.module.weight, gradient norm: 0.03133773431181908\n",
      "bn1.module.bias, gradient norm: 0.020035594701766968\n",
      "conv2.bias, gradient norm: 5.5606079740755376e-08\n",
      "conv2.lin.weight, gradient norm: 0.03316003829240799\n",
      "bn2.module.weight, gradient norm: 0.017198042944073677\n",
      "bn2.module.bias, gradient norm: 0.008549828082323074\n",
      "conv3.bias, gradient norm: 0.010173286311328411\n",
      "conv3.lin.weight, gradient norm: 0.019152270630002022\n",
      "Epoch: 490, Training Loss: 0.1628, Validation Loss: 0.2684, Train Acc: 0.9901, Val Acc: 0.9896, Test Acc: 0.9907\n",
      "conv1.bias, gradient norm: 1.8033222692892537e-09\n",
      "conv1.lin.weight, gradient norm: 0.0919436663389206\n",
      "bn1.module.weight, gradient norm: 0.0522938035428524\n",
      "bn1.module.bias, gradient norm: 0.031809788197278976\n",
      "conv2.bias, gradient norm: 4.4160707091123186e-08\n",
      "conv2.lin.weight, gradient norm: 0.028793882578611374\n",
      "bn2.module.weight, gradient norm: 0.01641417294740677\n",
      "bn2.module.bias, gradient norm: 0.008677761070430279\n",
      "conv3.bias, gradient norm: 0.01041664369404316\n",
      "conv3.lin.weight, gradient norm: 0.01950952038168907\n",
      "Epoch: 491, Training Loss: 0.1634, Validation Loss: 0.2683, Train Acc: 0.9901, Val Acc: 0.9897, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.6399992475157887e-09\n",
      "conv1.lin.weight, gradient norm: 0.11803943663835526\n",
      "bn1.module.weight, gradient norm: 0.03460705280303955\n",
      "bn1.module.bias, gradient norm: 0.018385697156190872\n",
      "conv2.bias, gradient norm: 5.937124214483447e-08\n",
      "conv2.lin.weight, gradient norm: 0.027921386063098907\n",
      "bn2.module.weight, gradient norm: 0.01686892658472061\n",
      "bn2.module.bias, gradient norm: 0.008428467437624931\n",
      "conv3.bias, gradient norm: 0.01050972193479538\n",
      "conv3.lin.weight, gradient norm: 0.019922910258173943\n",
      "Epoch: 492, Training Loss: 0.1609, Validation Loss: 0.2682, Train Acc: 0.9902, Val Acc: 0.9897, Test Acc: 0.9907\n",
      "conv1.bias, gradient norm: 2.2788735343226563e-09\n",
      "conv1.lin.weight, gradient norm: 0.08712850511074066\n",
      "bn1.module.weight, gradient norm: 0.04321642592549324\n",
      "bn1.module.bias, gradient norm: 0.02471252717077732\n",
      "conv2.bias, gradient norm: 6.471058355828063e-08\n",
      "conv2.lin.weight, gradient norm: 0.032315388321876526\n",
      "bn2.module.weight, gradient norm: 0.016524488106369972\n",
      "bn2.module.bias, gradient norm: 0.007060646545141935\n",
      "conv3.bias, gradient norm: 0.011186577379703522\n",
      "conv3.lin.weight, gradient norm: 0.020465146750211716\n",
      "Epoch: 493, Training Loss: 0.1631, Validation Loss: 0.2680, Train Acc: 0.9902, Val Acc: 0.9898, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 2.310746705092015e-09\n",
      "conv1.lin.weight, gradient norm: 0.11942948400974274\n",
      "bn1.module.weight, gradient norm: 0.04568913206458092\n",
      "bn1.module.bias, gradient norm: 0.02908145636320114\n",
      "conv2.bias, gradient norm: 4.573626100068395e-08\n",
      "conv2.lin.weight, gradient norm: 0.03570041060447693\n",
      "bn2.module.weight, gradient norm: 0.016298340633511543\n",
      "bn2.module.bias, gradient norm: 0.007105438970029354\n",
      "conv3.bias, gradient norm: 0.01140649151057005\n",
      "conv3.lin.weight, gradient norm: 0.01799101196229458\n",
      "Epoch: 494, Training Loss: 0.1630, Validation Loss: 0.2678, Train Acc: 0.9902, Val Acc: 0.9898, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 2.008711197376556e-09\n",
      "conv1.lin.weight, gradient norm: 0.0949079841375351\n",
      "bn1.module.weight, gradient norm: 0.017895903438329697\n",
      "bn1.module.bias, gradient norm: 0.013665391132235527\n",
      "conv2.bias, gradient norm: 6.749288417040589e-08\n",
      "conv2.lin.weight, gradient norm: 0.03586842492222786\n",
      "bn2.module.weight, gradient norm: 0.016947204247117043\n",
      "bn2.module.bias, gradient norm: 0.00752192223444581\n",
      "conv3.bias, gradient norm: 0.01092367060482502\n",
      "conv3.lin.weight, gradient norm: 0.022575940936803818\n",
      "Epoch: 495, Training Loss: 0.1623, Validation Loss: 0.2674, Train Acc: 0.9902, Val Acc: 0.9898, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 2.080965177952976e-09\n",
      "conv1.lin.weight, gradient norm: 0.11584315448999405\n",
      "bn1.module.weight, gradient norm: 0.04090585932135582\n",
      "bn1.module.bias, gradient norm: 0.023767685517668724\n",
      "conv2.bias, gradient norm: 6.955131226504818e-08\n",
      "conv2.lin.weight, gradient norm: 0.03888387978076935\n",
      "bn2.module.weight, gradient norm: 0.015743616968393326\n",
      "bn2.module.bias, gradient norm: 0.007441561669111252\n",
      "conv3.bias, gradient norm: 0.010990464128553867\n",
      "conv3.lin.weight, gradient norm: 0.0184671301394701\n",
      "Epoch: 496, Training Loss: 0.1650, Validation Loss: 0.2671, Train Acc: 0.9901, Val Acc: 0.9897, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.7313263045437566e-09\n",
      "conv1.lin.weight, gradient norm: 0.09411926567554474\n",
      "bn1.module.weight, gradient norm: 0.03954395651817322\n",
      "bn1.module.bias, gradient norm: 0.020498842000961304\n",
      "conv2.bias, gradient norm: 6.070062852359115e-08\n",
      "conv2.lin.weight, gradient norm: 0.031185565516352654\n",
      "bn2.module.weight, gradient norm: 0.015482152812182903\n",
      "bn2.module.bias, gradient norm: 0.008877191692590714\n",
      "conv3.bias, gradient norm: 0.010581877082586288\n",
      "conv3.lin.weight, gradient norm: 0.017857862636446953\n",
      "Epoch: 497, Training Loss: 0.1649, Validation Loss: 0.2669, Train Acc: 0.9901, Val Acc: 0.9897, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 1.2863072829105704e-09\n",
      "conv1.lin.weight, gradient norm: 0.11348774284124374\n",
      "bn1.module.weight, gradient norm: 0.05273313447833061\n",
      "bn1.module.bias, gradient norm: 0.024364715442061424\n",
      "conv2.bias, gradient norm: 5.0798849571265237e-08\n",
      "conv2.lin.weight, gradient norm: 0.03927115350961685\n",
      "bn2.module.weight, gradient norm: 0.016707060858607292\n",
      "bn2.module.bias, gradient norm: 0.00903219822794199\n",
      "conv3.bias, gradient norm: 0.010187542997300625\n",
      "conv3.lin.weight, gradient norm: 0.018619049340486526\n",
      "Epoch: 498, Training Loss: 0.1622, Validation Loss: 0.2667, Train Acc: 0.9901, Val Acc: 0.9897, Test Acc: 0.9905\n",
      "conv1.bias, gradient norm: 1.5071773828978507e-09\n",
      "conv1.lin.weight, gradient norm: 0.09530482441186905\n",
      "bn1.module.weight, gradient norm: 0.029863325878977776\n",
      "bn1.module.bias, gradient norm: 0.021147271618247032\n",
      "conv2.bias, gradient norm: 4.300628475562007e-08\n",
      "conv2.lin.weight, gradient norm: 0.035468894988298416\n",
      "bn2.module.weight, gradient norm: 0.016572806984186172\n",
      "bn2.module.bias, gradient norm: 0.008930965326726437\n",
      "conv3.bias, gradient norm: 0.01013085339218378\n",
      "conv3.lin.weight, gradient norm: 0.01939372532069683\n",
      "Epoch: 499, Training Loss: 0.1649, Validation Loss: 0.2664, Train Acc: 0.9901, Val Acc: 0.9897, Test Acc: 0.9906\n",
      "conv1.bias, gradient norm: 1.643013725072251e-09\n",
      "conv1.lin.weight, gradient norm: 0.09023217856884003\n",
      "bn1.module.weight, gradient norm: 0.01887703128159046\n",
      "bn1.module.bias, gradient norm: 0.015120962634682655\n",
      "conv2.bias, gradient norm: 4.258223640363212e-08\n",
      "conv2.lin.weight, gradient norm: 0.03118344396352768\n",
      "bn2.module.weight, gradient norm: 0.016118047758936882\n",
      "bn2.module.bias, gradient norm: 0.008973786607384682\n",
      "conv3.bias, gradient norm: 0.010427557863295078\n",
      "conv3.lin.weight, gradient norm: 0.017960084602236748\n",
      "Epoch: 500, Training Loss: 0.1647, Validation Loss: 0.2661, Train Acc: 0.9901, Val Acc: 0.9897, Test Acc: 0.9906\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "training_accuracies = []\n",
    "validation_accuracies = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train()\n",
    "    training_losses.append(train_loss)\n",
    "    scheduler.step()\n",
    "    \n",
    "    train_acc, val_acc, test_acc, val_loss = test()\n",
    "    training_accuracies.append(train_acc)\n",
    "    validation_accuracies.append(val_acc)\n",
    "    validation_losses.append(val_loss)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, '\n",
    "          f'Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(training_losses, label='Training Loss')\n",
    "plt.plot(validation_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(training_accuracies, label='Training Accuracy')\n",
    "plt.plot(validation_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
